{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91d80ffe",
   "metadata": {},
   "source": [
    "# ðŸ’³ Fraud Detection - Model Training\n",
    "\n",
    "This notebook trains multiple machine learning models for fraud detection using the preprocessed transaction data.\n",
    "\n",
    "**Models Trained:**\n",
    "1. **Logistic Regression** - Baseline linear model\n",
    "2. **Decision Tree** - Interpretable tree-based model  \n",
    "3. **Random Forest** - Ensemble of decision trees\n",
    "4. **XGBoost** - Gradient boosting (best for imbalanced data)\n",
    "5. **LightGBM** - Fast gradient boosting\n",
    "6. **Gradient Boosting** - Scikit-learn ensemble method\n",
    "\n",
    "**Class Imbalance Handling:**\n",
    "- SMOTE (Synthetic Minority Over-sampling) with 0.3 ratio\n",
    "- Class weights balancing\n",
    "- Stratified train-validation split\n",
    "\n",
    "**Output:** Trained models saved as PKL files for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94417b",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43817b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Try to import XGBoost and LightGBM\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    xgboost_available = True\n",
    "except ImportError:\n",
    "    xgboost_available = False\n",
    "    print(\"âš  XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    lightgbm_available = True\n",
    "except ImportError:\n",
    "    lightgbm_available = False\n",
    "    print(\"âš  LightGBM not available. Install with: pip install lightgbm\")\n",
    "\n",
    "# Imbalance handling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Metrics for quick evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")\n",
    "if xgboost_available:\n",
    "    print(\"âœ“ XGBoost available\")\n",
    "if lightgbm_available:\n",
    "    print(\"âœ“ LightGBM available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b622767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed and scaled data from EDA notebook\n",
    "print(\"Loading preprocessed data from EDA...\\n\")\n",
    "\n",
    "# Load the scaled dataset (recommended from EDA)\n",
    "train_data = pd.read_csv('train_transaction_scaled.csv')\n",
    "\n",
    "print(f\"âœ“ Data loaded: {train_data.shape}\")\n",
    "print(f\"Columns: {train_data.shape[1]}\")\n",
    "print(f\"Rows: {train_data.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b6a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "if 'isFraud' in train_data.columns:\n",
    "    X = train_data.drop(columns=['isFraud'])\n",
    "    y = train_data['isFraud']\n",
    "else:\n",
    "    raise ValueError(\"Target variable 'isFraud' not found in dataset\")\n",
    "\n",
    "# Remove ID columns if present\n",
    "id_cols = ['TransactionID', 'TransactionDT']\n",
    "X = X.drop(columns=[col for col in id_cols if col in X.columns], errors='ignore')\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Overview:\")\n",
    "print(f\"   Features: {X.shape[1]}\")\n",
    "print(f\"   Samples: {X.shape[0]:,}\")\n",
    "print(f\"\\nðŸŽ¯ Target Distribution:\")\n",
    "print(f\"   Not Fraud: {(y == 0).sum():,} ({(y == 0).sum()/len(y)*100:.2f}%)\")\n",
    "print(f\"   Fraud: {(y == 1).sum():,} ({(y == 1).sum()/len(y)*100:.2f}%)\")\n",
    "print(f\"   Imbalance Ratio: 1:{(y == 0).sum() // (y == 1).sum()}\")\n",
    "\n",
    "# Stratified train-validation split (80-20)\n",
    "print(f\"\\nðŸ“‚ Creating stratified train-validation split (80-20)...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Maintain fraud ratio\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Train set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"âœ“ Validation set: {X_val.shape[0]:,} samples\")\n",
    "print(f\"\\nTrain fraud rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Validation fraud rate: {y_val.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e83684",
   "metadata": {},
   "source": [
    "## 2. Handle Class Imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a5547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'='*80}\")\n",
    "print(\"APPLYING SMOTE TO BALANCE CLASSES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"Before SMOTE:\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Fraud cases: {y_train.sum():,} ({(y_train.sum()/len(y_train))*100:.2f}%)\")\n",
    "print(f\"Not Fraud cases: {(y_train == 0).sum():,} ({((y_train == 0).sum()/len(y_train))*100:.2f}%)\")\n",
    "\n",
    "# Apply SMOTE with optimal sampling strategy for fraud detection\n",
    "# Using 0.3 ratio - fraud becomes 30% of majority (better than 50% for high imbalance)\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.3, k_neighbors=5)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nAfter SMOTE:\")\n",
    "print(f\"Training samples: {len(X_train_smote):,}\")\n",
    "print(f\"Fraud cases: {y_train_smote.sum():,} ({(y_train_smote.sum()/len(y_train_smote))*100:.2f}%)\")\n",
    "print(f\"Not Fraud cases: {(y_train_smote == 0).sum():,} ({((y_train_smote == 0).sum()/len(y_train_smote))*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nâœ“ SMOTE applied successfully\")\n",
    "print(\"Note: Using 0.3 ratio for better generalization on highly imbalanced data\")\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before SMOTE\n",
    "axes[0].bar(['Not Fraud', 'Fraud'], [(y_train == 0).sum(), y_train.sum()], \n",
    "            color=['green', 'red'], edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Before SMOTE', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate([(y_train == 0).sum(), y_train.sum()]):\n",
    "    axes[0].text(i, v, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# After SMOTE\n",
    "axes[1].bar(['Not Fraud', 'Fraud'], [(y_train_smote == 0).sum(), y_train_smote.sum()], \n",
    "            color=['green', 'red'], edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('After SMOTE (30% Ratio)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate([(y_train_smote == 0).sum(), y_train_smote.sum()]):\n",
    "    axes[1].text(i, v, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a73ad",
   "metadata": {},
   "source": [
    "## 3. Model Training Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9243013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, model_name, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Train a model and return comprehensive evaluation metrics including confusion matrix\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING: {model_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"âœ“ Model trained in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Get probability scores if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        y_pred_proba = model.decision_function(X_val)\n",
    "    else:\n",
    "        y_pred_proba = None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Validation Results:\")\n",
    "    print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"   Precision: {precision:.4f} (of predicted frauds, how many are correct)\")\n",
    "    print(f\"   Recall:    {recall:.4f} (of actual frauds, how many we caught)\")\n",
    "    print(f\"   F1-Score:  {f1:.4f} (harmonic mean of precision & recall)\")\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "            print(f\"   ROC-AUC:   {roc_auc:.4f}\")\n",
    "        except:\n",
    "            roc_auc = None\n",
    "    else:\n",
    "        roc_auc = None\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Confusion Matrix:\")\n",
    "    print(f\"   TN: {cm[0,0]:6,}  |  FP: {cm[0,1]:6,}\")\n",
    "    print(f\"   FN: {cm[1,0]:6,}  |  TP: {cm[1,1]:6,}\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'training_time': training_time,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94a6fe",
   "metadata": {},
   "source": [
    "## 4. Model 1: Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419aa512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression - Good baseline for binary classification\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',  # Handle imbalance\n",
    "    solver='lbfgs',  # Good for large datasets\n",
    "    C=1.0  # Regularization strength\n",
    ")\n",
    "\n",
    "lr_results = train_and_evaluate_model(\n",
    "    lr_model, \"Logistic Regression\",\n",
    "    X_train_smote, y_train_smote, X_val, y_val\n",
    ")\n",
    "\n",
    "# Save model\n",
    "with open('model_logistic_regression.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "print(\"\\nâœ“ Model saved: model_logistic_regression.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0c983",
   "metadata": {},
   "source": [
    "## 5. Model 2: Decision Tree (Interpretable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44253fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree - Interpretable model with pruning to prevent overfitting\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    max_depth=15,  # Increased from 10 for better performance\n",
    "    min_samples_split=50,  # Reduced for more splits\n",
    "    min_samples_leaf=25,  # Minimum samples per leaf\n",
    "    class_weight='balanced',\n",
    "    criterion='gini'  # or 'entropy'\n",
    ")\n",
    "\n",
    "dt_results = train_and_evaluate_model(\n",
    "    dt_model, \"Decision Tree\",\n",
    "    X_train_smote, y_train_smote, X_val, y_val\n",
    ")\n",
    "\n",
    "print(f\"\\nTree Statistics:\")\n",
    "print(f\"   Tree depth: {dt_model.get_depth()}\")\n",
    "print(f\"   Number of leaves: {dt_model.get_n_leaves()}\")\n",
    "\n",
    "# Save model\n",
    "with open('model_decision_tree.pkl', 'wb') as f:\n",
    "    pickle.dump(dt_model, f)\n",
    "print(\"\\nâœ“ Model saved: model_decision_tree.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98926f9e",
   "metadata": {},
   "source": [
    "## 6. Model 3: Random Forest (Ensemble - Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest - Best balance of performance and interpretability\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,  # More trees for better performance\n",
    "    random_state=42,\n",
    "    max_depth=20,  # Deeper trees\n",
    "    min_samples_split=20,  # More aggressive splitting\n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',  # Good default for classification\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    bootstrap=True\n",
    ")\n",
    "\n",
    "rf_results = train_and_evaluate_model(\n",
    "    rf_model, \"Random Forest\",\n",
    "    X_train_smote, y_train_smote, X_val, y_val\n",
    ")\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"   Number of trees: {rf_model.n_estimators}\")\n",
    "print(f\"   Max features per split: sqrt({X_train.shape[1]}) = {int(np.sqrt(X_train.shape[1]))}\")\n",
    "\n",
    "# Save model\n",
    "with open('model_random_forest.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "print(\"\\nâœ“ Model saved: model_random_forest.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a88e8",
   "metadata": {},
   "source": [
    "## 7. Model 4: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe50e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if xgboost_available:\n",
    "    # XGBoost - Excellent for imbalanced classification\n",
    "    # Calculate scale_pos_weight for imbalance\n",
    "    scale_pos_weight = (y_train_smote == 0).sum() / (y_train_smote == 1).sum()\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,  # Row sampling\n",
    "        colsample_bytree=0.8,  # Column sampling\n",
    "        gamma=0,  # Minimum loss reduction\n",
    "        min_child_weight=3,\n",
    "        scale_pos_weight=scale_pos_weight,  # Handle imbalance\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    xgb_results = train_and_evaluate_model(\n",
    "        xgb_model, \"XGBoost\",\n",
    "        X_train_smote, y_train_smote, X_val, y_val\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel Statistics:\")\n",
    "    print(f\"   Number of boosting rounds: {xgb_model.n_estimators}\")\n",
    "    print(f\"   Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    # Save model\n",
    "    xgb_model.save_model('model_xgboost.json')\n",
    "    print(\"\\nâœ“ Model saved: model_xgboost.json\")\n",
    "else:\n",
    "    print(\"âš  XGBoost not available. Skipping...\")\n",
    "    xgb_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lightgbm_available:\n",
    "    # LightGBM - Fast and efficient gradient boosting\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,  # Should be < 2^max_depth\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_samples=20,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    lgb_results = train_and_evaluate_model(\n",
    "        lgb_model, \"LightGBM\",\n",
    "        X_train_smote, y_train_smote, X_val, y_val\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel Statistics:\")\n",
    "    print(f\"   Number of boosting rounds: {lgb_model.n_estimators}\")\n",
    "    print(f\"   Number of leaves: {lgb_model.num_leaves}\")\n",
    "    \n",
    "    # Save model\n",
    "    with open('model_lightgbm.pkl', 'wb') as f:\n",
    "        pickle.dump(lgb_model, f)\n",
    "    print(\"\\nâœ“ Model saved: model_lightgbm.pkl\")\n",
    "else:\n",
    "    print(\"âš  LightGBM not available. Skipping...\")\n",
    "    lgb_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70223c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting - Powerful ensemble method\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,  # Stochastic gradient boosting\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    validation_fraction=0.1,  # For early stopping monitoring\n",
    "    n_iter_no_change=10  # Early stopping\n",
    ")\n",
    "\n",
    "gb_results = train_and_evaluate_model(\n",
    "    gb_model, \"Gradient Boosting\",\n",
    "    X_train_smote, y_train_smote, X_val, y_val\n",
    ")\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"   Number of boosting stages: {gb_model.n_estimators}\")\n",
    "print(f\"   Effective estimators used: {gb_model.n_estimators_}\")\n",
    "\n",
    "# Save model\n",
    "with open('model_gradient_boosting.pkl', 'wb') as f:\n",
    "    pickle.dump(gb_model, f)\n",
    "print(\"\\nâœ“ Model saved: model_gradient_boosting.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6bacd2",
   "metadata": {},
   "source": [
    "## 9. Model 6: Gradient Boosting (Scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab843358",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… MODEL TRAINING COMPLETE!\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"ðŸ“Š Trained {len(all_results)} models:\")\n",
    "for i, r in enumerate(all_results, 1):\n",
    "    print(f\"   {i}. {r['model_name']}\")\n",
    "    print(f\"      â€¢ F1-Score: {r['f1']:.4f}\")\n",
    "    print(f\"      â€¢ Recall: {r['recall']:.4f}\")\n",
    "    print(f\"      â€¢ Training Time: {r['training_time']:.2f}s\")\n",
    "\n",
    "# Find best model by F1-score\n",
    "best_model = max(all_results, key=lambda x: x['f1'])\n",
    "print(f\"\\nðŸ† Best Model (by F1-Score): {best_model['model_name']}\")\n",
    "print(f\"   F1-Score: {best_model['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Generated Files:\")\n",
    "print(f\"   Models:\")\n",
    "print(f\"   â€¢ model_logistic_regression.pkl\")\n",
    "print(f\"   â€¢ model_decision_tree.pkl\")\n",
    "print(f\"   â€¢ model_random_forest.pkl\")\n",
    "if xgboost_available:\n",
    "    print(f\"   â€¢ model_xgboost.json\")\n",
    "if lightgbm_available:\n",
    "    print(f\"   â€¢ model_lightgbm.pkl\")\n",
    "print(f\"   â€¢ model_gradient_boosting.pkl\")\n",
    "\n",
    "print(f\"\\n   Results:\")\n",
    "print(f\"   â€¢ training_results_summary.csv\")\n",
    "print(f\"   â€¢ all_model_results.pkl\")\n",
    "print(f\"   â€¢ train_val_split.pkl\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Next Steps:\")\n",
    "print(f\"   1. Open Model_Evaluation.ipynb for detailed model comparison\")\n",
    "print(f\"   2. Analyze confusion matrices, ROC curves, and feature importance\")\n",
    "print(f\"   3. Select the best model based on business requirements\")\n",
    "print(f\"   4. Apply the best model to test_transaction.csv\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Proceed to Model_Evaluation.ipynb for comprehensive evaluation!\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8a362",
   "metadata": {},
   "source": [
    "## 11. Training Complete - Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06537d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = [lr_results, dt_results, rf_results]\n",
    "\n",
    "if xgboost_available and xgb_results:\n",
    "    all_results.append(xgb_results)\n",
    "\n",
    "if lightgbm_available and lgb_results:\n",
    "    all_results.append(lgb_results)\n",
    "\n",
    "all_results.append(gb_results)\n",
    "\n",
    "# Create summary DataFrame\n",
    "results_summary = pd.DataFrame([{\n",
    "    'Model': r['model_name'],\n",
    "    'Training Time (s)': r['training_time'],\n",
    "    'Accuracy': r['accuracy'],\n",
    "    'Precision': r['precision'],\n",
    "    'Recall': r['recall'],\n",
    "    'F1-Score': r['f1'],\n",
    "    'ROC-AUC': r['roc_auc'] if r['roc_auc'] else 'N/A'\n",
    "} for r in all_results])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRAINING SUMMARY - ALL MODELS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(results_summary.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "results_summary.to_csv('training_results_summary.csv', index=False)\n",
    "print(f\"\\nâœ“ Training summary saved: training_results_summary.csv\")\n",
    "\n",
    "# Save all results for evaluation notebook\n",
    "with open('all_model_results.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "print(f\"âœ“ All model results saved: all_model_results.pkl\")\n",
    "\n",
    "# Save train/val split for evaluation\n",
    "with open('train_val_split.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train': X_train,\n",
    "        'X_val': X_val,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'X_train_smote': X_train_smote,\n",
    "        'y_train_smote': y_train_smote\n",
    "    }, f)\n",
    "print(f\"âœ“ Train/val split saved: train_val_split.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfda8dc",
   "metadata": {},
   "source": [
    "## 10. Save All Models and Results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
