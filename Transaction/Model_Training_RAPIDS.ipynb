{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a35ea876",
   "metadata": {},
   "source": [
    "# üí≥ Fraud Detection - Model Training (RAPIDS GPU-Accelerated)\n",
    "\n",
    "This notebook trains multiple machine learning models for fraud detection using **RAPIDS cuML** for GPU acceleration where available.\n",
    "\n",
    "**Models Trained:**\n",
    "1. **Logistic Regression** - GPU-accelerated baseline (cuML) or CPU fallback\n",
    "2. **Decision Tree** - Interpretable tree-based model (CPU - sklearn)\n",
    "3. **Random Forest** - GPU-accelerated ensemble (cuML) or CPU fallback\n",
    "4. **XGBoost** - GPU gradient boosting (`tree_method='gpu_hist'`)\n",
    "5. **LightGBM** - Fast gradient boosting (CPU - no GPU support in sklearn API)\n",
    "6. **Gradient Boosting** - Scikit-learn ensemble method (CPU)\n",
    "\n",
    "**‚ö° GPU Acceleration:**\n",
    "- **RAPIDS cuML** for Logistic Regression and Random Forest (10-100x faster)\n",
    "- **XGBoost GPU** for gradient boosting (`gpu_hist`)\n",
    "- **CPU fallback** for models without GPU support (Decision Tree, LightGBM, Gradient Boosting)\n",
    "\n",
    "**Class Imbalance Handling:**\n",
    "- SMOTE (Synthetic Minority Over-sampling) with 0.3 ratio\n",
    "- Class weights balancing\n",
    "- Stratified train-validation split\n",
    "\n",
    "**Requirements:**\n",
    "- NVIDIA GPU with CUDA support (optional - auto-fallback to CPU)\n",
    "- RAPIDS cuML: `conda install -c rapidsai -c conda-forge -c nvidia rapids=23.10 python=3.10 cudatoolkit=11.8`\n",
    "\n",
    "**Output:** Trained models saved as PKL files for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d2300",
   "metadata": {},
   "source": [
    "## 1. Setup and Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eccac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"=\"*80)\n",
    "print(\"GPU AVAILABILITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    import cudf\n",
    "    import cuml\n",
    "    import cupy as cp\n",
    "    from cuml.linear_model import LogisticRegression as cuLogisticRegression\n",
    "    from cuml.ensemble import RandomForestClassifier as cuRandomForestClassifier\n",
    "    \n",
    "    rapids_available = True\n",
    "    print(\"‚úì RAPIDS cuML available\")\n",
    "    print(f\"‚úì cuDF version: {cudf.__version__}\")\n",
    "    print(f\"‚úì cuML version: {cuml.__version__}\")\n",
    "    \n",
    "    # Check GPU\n",
    "    gpu_count = cp.cuda.runtime.getDeviceCount()\n",
    "    print(f\"‚úì GPUs available: {gpu_count}\")\n",
    "    \n",
    "    if gpu_count > 0:\n",
    "        gpu_name = cp.cuda.runtime.getDeviceProperties(0)['name'].decode()\n",
    "        gpu_mem = cp.cuda.runtime.getDeviceProperties(0)['totalGlobalMem'] / 1e9\n",
    "        print(f\"‚úì GPU 0: {gpu_name}\")\n",
    "        print(f\"‚úì GPU Memory: {gpu_mem:.1f} GB\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    rapids_available = False\n",
    "    print(\"‚ùå RAPIDS not available\")\n",
    "    print(\"\\nüì¶ Installation required:\")\n",
    "    print(\"conda create -n rapids-env -c rapidsai -c conda-forge -c nvidia rapids=23.10 python=3.10 cudatoolkit=11.8\")\n",
    "    print(\"\\nFalling back to CPU training...\")\n",
    "\n",
    "# Standard ML libraries (fallback and non-GPU models)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# XGBoost with GPU support\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    xgboost_available = True\n",
    "    print(\"‚úì XGBoost available\")\n",
    "except ImportError:\n",
    "    xgboost_available = False\n",
    "    print(\"‚ö† XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "# LightGBM\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    lightgbm_available = True\n",
    "    print(\"‚úì LightGBM available\")\n",
    "except ImportError:\n",
    "    lightgbm_available = False\n",
    "    print(\"‚ö† LightGBM not available. Install with: pip install lightgbm\")\n",
    "\n",
    "# SMOTE for imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"\\n‚úì All libraries imported successfully\")\n",
    "print(f\"\\nüöÄ GPU Acceleration: {'ENABLED' if rapids_available else 'DISABLED (CPU mode)'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a01cf1",
   "metadata": {},
   "source": [
    "## 2. Load Data (GPU-Accelerated with cuDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462ff122",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading preprocessed data from EDA...\\n\")\n",
    "\n",
    "if rapids_available:\n",
    "    # Load with cuDF for GPU acceleration\n",
    "    print(\"üìä Loading data on GPU with cuDF...\")\n",
    "    train_data_gpu = cudf.read_csv('train_transaction_scaled.csv')\n",
    "    \n",
    "    print(f\"‚úì Data loaded on GPU: {train_data_gpu.shape}\")\n",
    "    print(f\"‚úì GPU memory usage: {train_data_gpu.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n",
    "    \n",
    "    # Convert to pandas for compatibility with some operations\n",
    "    train_data = train_data_gpu.to_pandas()\n",
    "    \n",
    "else:\n",
    "    # CPU fallback\n",
    "    print(\"üìä Loading data with pandas (CPU)...\")\n",
    "    train_data = pd.read_csv('train_transaction_scaled.csv')\n",
    "\n",
    "print(f\"\\n‚úì Dataset shape: {train_data.shape}\")\n",
    "print(f\"  Columns: {train_data.shape[1]}\")\n",
    "print(f\"  Rows: {train_data.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051581a5",
   "metadata": {},
   "source": [
    "## 3. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c1f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "if 'isFraud' in train_data.columns:\n",
    "    X = train_data.drop(columns=['isFraud'])\n",
    "    y = train_data['isFraud']\n",
    "else:\n",
    "    raise ValueError(\"Target variable 'isFraud' not found in dataset\")\n",
    "\n",
    "# Remove ID columns if present\n",
    "id_cols = ['TransactionID', 'TransactionDT']\n",
    "X = X.drop(columns=[col for col in id_cols if col in X.columns], errors='ignore')\n",
    "\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"   Features: {X.shape[1]}\")\n",
    "print(f\"   Samples: {X.shape[0]:,}\")\n",
    "print(f\"\\nüéØ Target Distribution:\")\n",
    "print(f\"   Not Fraud: {(y == 0).sum():,} ({(y == 0).sum()/len(y)*100:.2f}%)\")\n",
    "print(f\"   Fraud: {(y == 1).sum():,} ({(y == 1).sum()/len(y)*100:.2f}%)\")\n",
    "print(f\"   Imbalance Ratio: 1:{(y == 0).sum() // (y == 1).sum()}\")\n",
    "\n",
    "# Stratified train-validation split (80-20)\n",
    "print(f\"\\nüìÇ Creating stratified train-validation split (80-20)...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"‚úì Validation set: {X_val.shape[0]:,} samples\")\n",
    "print(f\"\\nTrain fraud rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Validation fraud rate: {y_val.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f9f06",
   "metadata": {},
   "source": [
    "## 4. Handle Class Imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7eebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'='*80}\")\n",
    "print(\"APPLYING SMOTE TO BALANCE CLASSES\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"Before SMOTE:\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Fraud cases: {y_train.sum():,} ({(y_train.sum()/len(y_train))*100:.2f}%)\")\n",
    "print(f\"Not Fraud cases: {(y_train == 0).sum():,} ({((y_train == 0).sum()/len(y_train))*100:.2f}%)\")\n",
    "\n",
    "# Apply SMOTE with optimal sampling strategy for fraud detection\n",
    "# Using 0.3 ratio - fraud becomes 30% of majority (better than 50% for high imbalance)\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.3, k_neighbors=5)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nAfter SMOTE:\")\n",
    "print(f\"Training samples: {len(X_train_smote):,}\")\n",
    "print(f\"Fraud cases: {y_train_smote.sum():,} ({(y_train_smote.sum()/len(y_train_smote))*100:.2f}%)\")\n",
    "print(f\"Not Fraud cases: {(y_train_smote == 0).sum():,} ({((y_train_smote == 0).sum()/len(y_train_smote))*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n‚úì SMOTE applied successfully\")\n",
    "print(\"Note: Using 0.3 ratio for better generalization on highly imbalanced data\")\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before SMOTE\n",
    "axes[0].bar(['Not Fraud', 'Fraud'], [(y_train == 0).sum(), y_train.sum()], \n",
    "            color=['green', 'red'], edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Before SMOTE', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate([(y_train == 0).sum(), y_train.sum()]):\n",
    "    axes[0].text(i, v, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# After SMOTE\n",
    "axes[1].bar(['Not Fraud', 'Fraud'], [(y_train_smote == 0).sum(), y_train_smote.sum()], \n",
    "            color=['green', 'red'], edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('After SMOTE (30% Ratio)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate([(y_train_smote == 0).sum(), y_train_smote.sum()]):\n",
    "    axes[1].text(i, v, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d628a2",
   "metadata": {},
   "source": [
    "## 5. Model Training Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a418f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, model_name, X_train, y_train, X_val, y_val, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Train a model and return comprehensive evaluation metrics including confusion matrix\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The model to train\n",
    "    - model_name: Name for display\n",
    "    - X_train, y_train: Training data\n",
    "    - X_val, y_val: Validation data\n",
    "    - use_gpu: Whether GPU is being used\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with model and metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING: {model_name}\")\n",
    "    if use_gpu:\n",
    "        print(f\"üöÄ GPU ACCELERATION ENABLED\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úì Model trained in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Get probabilities if available\n",
    "    try:\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "        elif hasattr(model, 'decision_function'):\n",
    "            y_pred_proba = model.decision_function(X_val)\n",
    "        else:\n",
    "            y_pred_proba = None\n",
    "    except:\n",
    "        y_pred_proba = None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    print(f\"\\nüìä Validation Results:\")\n",
    "    print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"   Precision: {precision:.4f} (of predicted frauds, how many are correct)\")\n",
    "    print(f\"   Recall:    {recall:.4f} (of actual frauds, how many we caught)\")\n",
    "    print(f\"   F1-Score:  {f1:.4f} (harmonic mean of precision & recall)\")\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "            print(f\"   ROC-AUC:   {roc_auc:.4f}\")\n",
    "        except:\n",
    "            roc_auc = None\n",
    "    else:\n",
    "        roc_auc = None\n",
    "    \n",
    "    print(f\"\\nüìà Confusion Matrix:\")\n",
    "    print(f\"   TN: {cm[0,0]:6,}  |  FP: {cm[0,1]:6,}\")\n",
    "    print(f\"   FN: {cm[1,0]:6,}  |  TP: {cm[1,1]:6,}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'model_name': model_name,\n",
    "        'training_time': training_time,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'gpu_accelerated': use_gpu\n",
    "    }\n",
    "\n",
    "print(\"‚úì Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a389ae",
   "metadata": {},
   "source": [
    "## 6. Model 1: Logistic Regression (GPU-Accelerated or CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f97693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rapids_available:\n",
    "    # cuML GPU-accelerated Logistic Regression\n",
    "    lr_model = cuLogisticRegression(\n",
    "        max_iter=1000,\n",
    "        solver='qn',  # Quasi-Newton (GPU optimized)\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Convert to cuDF for GPU training\n",
    "    X_train_gpu = cudf.DataFrame(X_train_smote)\n",
    "    y_train_gpu = cudf.Series(y_train_smote)\n",
    "    X_val_gpu = cudf.DataFrame(X_val)\n",
    "    y_val_gpu = cudf.Series(y_val)\n",
    "    \n",
    "    lr_results = train_and_evaluate_model(\n",
    "        lr_model, \n",
    "        'Logistic Regression (cuML GPU)', \n",
    "        X_train_gpu, y_train_gpu, \n",
    "        X_val_gpu, y_val_gpu.to_numpy(),\n",
    "        use_gpu=True\n",
    "    )\n",
    "    \n",
    "    # Save model (convert to CPU for compatibility)\n",
    "    with open('model_logistic_regression_rapids.pkl', 'wb') as f:\n",
    "        pickle.dump(lr_results['model'], f)\n",
    "    print(\"\\n‚úì Model saved: model_logistic_regression_rapids.pkl\")\n",
    "    \n",
    "else:\n",
    "    # CPU fallback\n",
    "    lr_model = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    lr_results = train_and_evaluate_model(\n",
    "        lr_model, \n",
    "        'Logistic Regression (CPU)', \n",
    "        X_train_smote, y_train_smote, \n",
    "        X_val, y_val\n",
    "    )\n",
    "    \n",
    "    with open('model_logistic_regression_cpu.pkl', 'wb') as f:\n",
    "        pickle.dump(lr_results['model'], f)\n",
    "    print(\"\\n‚úì Model saved: model_logistic_regression_cpu.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40748af3",
   "metadata": {},
   "source": [
    "## 7. Model 2: Decision Tree (Interpretable - CPU Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree - Interpretable model with pruning to prevent overfitting\n",
    "# Note: cuML doesn't have DecisionTree, using sklearn\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    max_depth=15,  # Increased from 10 for better performance\n",
    "    min_samples_split=50,  # Reduced for more splits\n",
    "    min_samples_leaf=25,  # Minimum samples per leaf\n",
    "    class_weight='balanced',\n",
    "    criterion='gini'  # or 'entropy'\n",
    ")\n",
    "\n",
    "dt_results = train_and_evaluate_model(\n",
    "    dt_model, \"Decision Tree\",\n",
    "    X_train_smote, y_train_smote, X_val, y_val,\n",
    "    use_gpu=False  # CPU only\n",
    ")\n",
    "\n",
    "print(f\"\\nTree Statistics:\")\n",
    "print(f\"   Tree depth: {dt_model.get_depth()}\")\n",
    "print(f\"   Number of leaves: {dt_model.get_n_leaves()}\")\n",
    "\n",
    "# Save model\n",
    "with open('model_decision_tree_rapids.pkl', 'wb') as f:\n",
    "    pickle.dump(dt_model, f)\n",
    "print(\"\\n‚úì Model saved: model_decision_tree_rapids.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf36af",
   "metadata": {},
   "source": [
    "## 8. Model 3: Random Forest (GPU-Accelerated or CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rapids_available:\n",
    "    # cuML GPU-accelerated Random Forest\n",
    "    rf_model = cuRandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        max_features='sqrt',\n",
    "        min_samples_split=10,\n",
    "        random_state=42,\n",
    "        n_streams=4  # GPU parallelism\n",
    "    )\n",
    "    \n",
    "    # Use GPU data\n",
    "    rf_results = train_and_evaluate_model(\n",
    "        rf_model, \n",
    "        'Random Forest (cuML GPU)', \n",
    "        X_train_gpu, y_train_gpu, \n",
    "        X_val_gpu, y_val_gpu.to_numpy(),\n",
    "        use_gpu=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel Statistics:\")\n",
    "    print(f\"   Number of trees: {rf_model.n_estimators}\")\n",
    "    print(f\"   Max features per split: sqrt({X_train.shape[1]}) = {int(np.sqrt(X_train.shape[1]))}\")\n",
    "    \n",
    "    with open('model_random_forest_rapids.pkl', 'wb') as f:\n",
    "        pickle.dump(rf_results['model'], f)\n",
    "    print(\"\\n‚úì Model saved: model_random_forest_rapids.pkl\")\n",
    "    \n",
    "else:\n",
    "    # CPU fallback - Best balance of performance and interpretability\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=200,  # More trees for better performance\n",
    "        random_state=42,\n",
    "        max_depth=20,  # Deeper trees\n",
    "        min_samples_split=20,  # More aggressive splitting\n",
    "        min_samples_leaf=10,\n",
    "        max_features='sqrt',  # Good default for classification\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,  # Use all CPU cores\n",
    "        bootstrap=True\n",
    "    )\n",
    "    \n",
    "    rf_results = train_and_evaluate_model(\n",
    "        rf_model, \n",
    "        'Random Forest (CPU)', \n",
    "        X_train_smote, y_train_smote, \n",
    "        X_val, y_val,\n",
    "        use_gpu=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel Statistics:\")\n",
    "    print(f\"   Number of trees: {rf_model.n_estimators}\")\n",
    "    print(f\"   Max features per split: sqrt({X_train.shape[1]}) = {int(np.sqrt(X_train.shape[1]))}\")\n",
    "    \n",
    "    with open('model_random_forest_rapids.pkl', 'wb') as f:\n",
    "        pickle.dump(rf_results['model'], f)\n",
    "    print(\"\\n‚úì Model saved: model_random_forest_rapids.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b793c",
   "metadata": {},
   "source": [
    "## 9. Model 4: XGBoost (GPU-Accelerated or CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d1077",
   "metadata": {},
   "outputs": [],
   "source": [
    "if xgboost_available:\n",
    "    # XGBoost - Excellent for imbalanced classification\n",
    "    # Calculate scale_pos_weight for imbalance\n",
    "    scale_pos_weight = (y_train_smote == 0).sum() / (y_train_smote == 1).sum()\n",
    "    \n",
    "    if rapids_available:\n",
    "        # XGBoost with GPU acceleration\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,  # Row sampling\n",
    "            colsample_bytree=0.8,  # Column sampling\n",
    "            gamma=0,  # Minimum loss reduction\n",
    "            min_child_weight=3,\n",
    "            scale_pos_weight=scale_pos_weight,  # Handle imbalance\n",
    "            tree_method='gpu_hist',  # GPU acceleration\n",
    "            predictor='gpu_predictor',  # GPU prediction\n",
    "            gpu_id=0,\n",
    "            random_state=42,\n",
    "            eval_metric='logloss',\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "        \n",
    "        xgb_results = train_and_evaluate_model(\n",
    "            xgb_model, \n",
    "            'XGBoost (GPU)', \n",
    "            X_train_smote, y_train_smote, \n",
    "            X_val, y_val,\n",
    "            use_gpu=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel Statistics:\")\n",
    "        print(f\"   Number of boosting rounds: {xgb_model.n_estimators}\")\n",
    "        print(f\"   Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "        \n",
    "        xgb_model.save_model('model_xgboost_rapids.json')\n",
    "        print(\"\\n‚úì Model saved: model_xgboost_rapids.json\")\n",
    "        \n",
    "    else:\n",
    "        # CPU fallback\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,  # Row sampling\n",
    "            colsample_bytree=0.8,  # Column sampling\n",
    "            gamma=0,  # Minimum loss reduction\n",
    "            min_child_weight=3,\n",
    "            scale_pos_weight=scale_pos_weight,  # Handle imbalance\n",
    "            tree_method='hist',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            eval_metric='logloss',\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "        \n",
    "        xgb_results = train_and_evaluate_model(\n",
    "            xgb_model, \n",
    "            'XGBoost (CPU)', \n",
    "            X_train_smote, y_train_smote, \n",
    "            X_val, y_val,\n",
    "            use_gpu=False\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel Statistics:\")\n",
    "        print(f\"   Number of boosting rounds: {xgb_model.n_estimators}\")\n",
    "        print(f\"   Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "        \n",
    "        xgb_model.save_model('model_xgboost_rapids.json')\n",
    "        print(\"\\n‚úì Model saved: model_xgboost_rapids.json\")\n",
    "else:\n",
    "    print(\"‚ö† XGBoost not available. Skipping...\")\n",
    "    xgb_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82e440",
   "metadata": {},
   "source": [
    "## 10. Model 5: LightGBM (Fast Gradient Boosting - CPU Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lightgbm_available:\n",
    "    # LightGBM - Fast and efficient gradient boosting\n",
    "    # Note: sklearn API doesn't support GPU, use CPU\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,  # Should be < 2^max_depth\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_samples=20,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    lgb_results = train_and_evaluate_model(\n",
    "        lgb_model, \"LightGBM\",\n",
    "        X_train_smote, y_train_smote, X_val, y_val,\n",
    "        use_gpu=False  # CPU only for sklearn API\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel Statistics:\")\n",
    "    print(f\"   Number of boosting rounds: {lgb_model.n_estimators}\")\n",
    "    print(f\"   Number of leaves: {lgb_model.num_leaves}\")\n",
    "    \n",
    "    # Save model\n",
    "    with open('model_lightgbm_rapids.pkl', 'wb') as f:\n",
    "        pickle.dump(lgb_model, f)\n",
    "    print(\"\\n‚úì Model saved: model_lightgbm_rapids.pkl\")\n",
    "else:\n",
    "    print(\"‚ö† LightGBM not available. Skipping...\")\n",
    "    lgb_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36465c00",
   "metadata": {},
   "source": [
    "## 11. Model 6: Gradient Boosting (Scikit-learn - CPU Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfd1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting - Powerful ensemble method\n",
    "# Note: scikit-learn doesn't have GPU support\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,  # Stochastic gradient boosting\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    validation_fraction=0.1,  # For early stopping monitoring\n",
    "    n_iter_no_change=10  # Early stopping\n",
    ")\n",
    "\n",
    "gb_results = train_and_evaluate_model(\n",
    "    gb_model, \"Gradient Boosting\",\n",
    "    X_train_smote, y_train_smote, X_val, y_val,\n",
    "    use_gpu=False  # CPU only\n",
    ")\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"   Number of boosting stages: {gb_model.n_estimators}\")\n",
    "print(f\"   Effective estimators used: {gb_model.n_estimators_}\")\n",
    "\n",
    "# Save model\n",
    "with open('model_gradient_boosting_rapids.pkl', 'wb') as f:\n",
    "    pickle.dump(gb_model, f)\n",
    "print(\"\\n‚úì Model saved: model_gradient_boosting_rapids.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc26f159",
   "metadata": {},
   "source": [
    "## 12. Save All Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b9978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = [lr_results, dt_results, rf_results]\n",
    "\n",
    "if xgboost_available and xgb_results:\n",
    "    all_results.append(xgb_results)\n",
    "\n",
    "if lightgbm_available and lgb_results:\n",
    "    all_results.append(lgb_results)\n",
    "\n",
    "all_results.append(gb_results)\n",
    "\n",
    "# Create summary DataFrame\n",
    "results_summary = pd.DataFrame([{\n",
    "    'Model': r['model_name'],\n",
    "    'GPU Accelerated': 'üöÄ Yes' if r['gpu_accelerated'] else 'No',\n",
    "    'Training Time (s)': r['training_time'],\n",
    "    'Accuracy': r['accuracy'],\n",
    "    'Precision': r['precision'],\n",
    "    'Recall': r['recall'],\n",
    "    'F1-Score': r['f1'],\n",
    "    'ROC-AUC': r['roc_auc'] if r['roc_auc'] else 'N/A'\n",
    "} for r in all_results])\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"TRAINING SUMMARY - ALL MODELS\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "print(results_summary.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "results_summary.to_csv('training_results_summary_rapids.csv', index=False)\n",
    "print(f\"\\n‚úì Training summary saved: training_results_summary_rapids.csv\")\n",
    "\n",
    "# Save all results for evaluation notebook\n",
    "with open('all_model_results_rapids.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "print(f\"‚úì All model results saved: all_model_results_rapids.pkl\")\n",
    "\n",
    "# Save train/val split for evaluation\n",
    "with open('train_val_split_rapids.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train': X_train,\n",
    "        'X_val': X_val,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'X_train_smote': X_train_smote,\n",
    "        'y_train_smote': y_train_smote\n",
    "    }, f)\n",
    "print(f\"‚úì Train/val split saved: train_val_split_rapids.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40c98a",
   "metadata": {},
   "source": [
    "## 13. Training Complete - Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*100}\")\n",
    "print(\"‚úÖ RAPIDS GPU-ACCELERATED TRAINING COMPLETE!\" if rapids_available else \"‚úÖ MODEL TRAINING COMPLETE!\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "print(f\"üìä Trained {len(all_results)} models:\")\n",
    "for i, r in enumerate(all_results, 1):\n",
    "    gpu_badge = 'üöÄ ' if r['gpu_accelerated'] else ''\n",
    "    print(f\"   {i}. {gpu_badge}{r['model_name']}\")\n",
    "    print(f\"      ‚Ä¢ F1-Score: {r['f1']:.4f}\")\n",
    "    print(f\"      ‚Ä¢ Recall: {r['recall']:.4f}\")\n",
    "    print(f\"      ‚Ä¢ Training Time: {r['training_time']:.2f}s\")\n",
    "\n",
    "# Find best model by F1-score\n",
    "best_model = max(all_results, key=lambda x: x['f1'])\n",
    "print(f\"\\nüèÜ Best Model (by F1-Score): {best_model['model_name']}\")\n",
    "print(f\"   F1-Score: {best_model['f1']:.4f}\")\n",
    "if best_model['gpu_accelerated']:\n",
    "    print(f\"   üöÄ GPU-Accelerated\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "print(f\"   Models:\")\n",
    "print(f\"   ‚Ä¢ model_logistic_regression_rapids.pkl\")\n",
    "print(f\"   ‚Ä¢ model_decision_tree_rapids.pkl\")\n",
    "print(f\"   ‚Ä¢ model_random_forest_rapids.pkl\")\n",
    "if xgboost_available:\n",
    "    print(f\"   ‚Ä¢ model_xgboost_rapids.json\")\n",
    "if lightgbm_available:\n",
    "    print(f\"   ‚Ä¢ model_lightgbm_rapids.pkl\")\n",
    "print(f\"   ‚Ä¢ model_gradient_boosting_rapids.pkl\")\n",
    "\n",
    "print(f\"\\n   Results:\")\n",
    "print(f\"   ‚Ä¢ training_results_summary_rapids.csv\")\n",
    "print(f\"   ‚Ä¢ all_model_results_rapids.pkl\")\n",
    "print(f\"   ‚Ä¢ train_val_split_rapids.pkl\")\n",
    "\n",
    "if rapids_available:\n",
    "    gpu_count = sum(1 for r in all_results if r['gpu_accelerated'])\n",
    "    print(f\"\\n‚ö° GPU ACCELERATION SUMMARY:\")\n",
    "    print(f\"   ‚Ä¢ Models trained on GPU: {gpu_count}/{len(all_results)}\")\n",
    "    print(f\"   ‚Ä¢ GPU-accelerated models: {', '.join([r['model_name'] for r in all_results if r['gpu_accelerated']])}\")\n",
    "    print(f\"   ‚Ä¢ Expected speedup: 10-100x on large datasets\")\n",
    "    print(f\"   ‚Ä¢ Memory efficiency: Improved with cuDF\")\n",
    "\n",
    "print(f\"\\nüí° Next Steps:\")\n",
    "print(f\"   1. Open Model_Evaluation.ipynb for detailed model comparison\")\n",
    "print(f\"   2. Analyze confusion matrices, ROC curves, and feature importance\")\n",
    "print(f\"   3. Select the best model based on business requirements\")\n",
    "print(f\"   4. Apply the best model to test_transaction.csv\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"Proceed to Model_Evaluation.ipynb for comprehensive evaluation!\")\n",
    "print(f\"{'='*100}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
