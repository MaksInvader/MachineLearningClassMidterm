{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa17bf3",
   "metadata": {},
   "source": [
    "# Transaction Fraud Detection - Model Evaluation\n",
    "## Comprehensive Model Comparison and Analysis\n",
    "\n",
    "This notebook performs detailed evaluation and comparison of all trained models:\n",
    "- Load trained models and results\n",
    "- Confusion matrices and classification reports\n",
    "- ROC curves and PR curves comparison\n",
    "- Feature importance analysis\n",
    "- Threshold tuning for optimal fraud detection\n",
    "- Final model selection and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78952f32",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    roc_curve, auc, precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943b7b7f",
   "metadata": {},
   "source": [
    "## 2. Load Trained Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all model results\n",
    "with open('all_model_results.pkl', 'rb') as f:\n",
    "    all_results = pickle.load(f)\n",
    "\n",
    "# Load train/val split\n",
    "with open('train_val_split.pkl', 'rb') as f:\n",
    "    split_data = pickle.load(f)\n",
    "    \n",
    "X_val = split_data['X_val']\n",
    "y_val = split_data['y_val']\n",
    "\n",
    "print(f\"âœ“ Loaded {len(all_results)} model results\")\n",
    "print(f\"âœ“ Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"âœ“ Fraud rate in validation: {y_val.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5fc07a",
   "metadata": {},
   "source": [
    "## 3. Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f59791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame([{\n",
    "    'Model': r['model_name'],\n",
    "    'Accuracy': f\"{r['accuracy']:.4f}\",\n",
    "    'Precision': f\"{r['precision']:.4f}\",\n",
    "    'Recall': f\"{r['recall']:.4f}\",\n",
    "    'F1-Score': f\"{r['f1']:.4f}\",\n",
    "    'ROC-AUC': f\"{r['roc_auc']:.4f}\" if r['roc_auc'] else 'N/A',\n",
    "    'Training Time': f\"{r['training_time']:.2f}s\"\n",
    "} for r in all_results])\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Highlight best performers\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"ðŸ† BEST PERFORMERS\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "metrics = ['f1', 'recall', 'precision', 'roc_auc']\n",
    "metric_names = ['F1-Score', 'Recall (Fraud Detection)', 'Precision', 'ROC-AUC']\n",
    "\n",
    "for metric, name in zip(metrics, metric_names):\n",
    "    if metric == 'roc_auc':\n",
    "        valid_results = [r for r in all_results if r[metric] is not None]\n",
    "        if not valid_results:\n",
    "            continue\n",
    "        best = max(valid_results, key=lambda x: x[metric])\n",
    "    else:\n",
    "        best = max(all_results, key=lambda x: x[metric])\n",
    "    print(f\"Best {name:25s}: {best['model_name']:30s} ({best[metric]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc06fbb",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "n_models = len(all_results)\n",
    "n_cols = 3\n",
    "n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "axes = axes.flatten() if n_models > 1 else [axes]\n",
    "\n",
    "for idx, result in enumerate(all_results):\n",
    "    cm = result['confusion_matrix']\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Legitimate', 'Fraud'],\n",
    "                yticklabels=['Legitimate', 'Fraud'])\n",
    "    \n",
    "    ax.set_title(f\"{result['model_name']}\\nF1: {result['f1']:.4f} | Recall: {result['recall']:.4f}\", \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "\n",
    "# Hide extra subplots\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Confusion matrices saved: confusion_matrices_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d9089",
   "metadata": {},
   "source": [
    "## 5. ROC Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e38aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for result in all_results:\n",
    "    if result['roc_auc'] is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_val, result['y_pred_proba'])\n",
    "        plt.plot(fpr, tpr, linewidth=2, \n",
    "                label=f\"{result['model_name']} (AUC = {result['roc_auc']:.4f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "plt.title('ROC Curves - All Models', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ ROC curves saved: roc_curves_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ed1d0",
   "metadata": {},
   "source": [
    "## 6. Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for result in all_results:\n",
    "    if result['y_pred_proba'] is not None:\n",
    "        precision, recall, _ = precision_recall_curve(y_val, result['y_pred_proba'])\n",
    "        avg_precision = average_precision_score(y_val, result['y_pred_proba'])\n",
    "        plt.plot(recall, precision, linewidth=2,\n",
    "                label=f\"{result['model_name']} (AP = {avg_precision:.4f})\")\n",
    "\n",
    "# Baseline (random classifier)\n",
    "baseline = y_val.mean()\n",
    "plt.axhline(y=baseline, color='k', linestyle='--', linewidth=2, \n",
    "           label=f'Random Classifier (AP = {baseline:.4f})')\n",
    "\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curves - All Models', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('precision_recall_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Precision-Recall curves saved: precision_recall_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5700c0",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b69f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = X_val.columns.tolist()\n",
    "\n",
    "# Models with feature importance\n",
    "importance_models = ['Decision Tree', 'Random Forest', 'XGBoost', 'LightGBM', 'Gradient Boosting']\n",
    "importance_results = [r for r in all_results if r['model_name'] in importance_models]\n",
    "\n",
    "if importance_results:\n",
    "    n_importance_models = len(importance_results)\n",
    "    fig, axes = plt.subplots(n_importance_models, 1, figsize=(14, 6*n_importance_models))\n",
    "    \n",
    "    if n_importance_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, result in enumerate(importance_results):\n",
    "        model = result['model']\n",
    "        \n",
    "        # Get feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        # Create DataFrame and sort\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False).head(20)\n",
    "        \n",
    "        # Plot\n",
    "        ax = axes[idx]\n",
    "        ax.barh(range(len(importance_df)), importance_df['Importance'], color='steelblue')\n",
    "        ax.set_yticks(range(len(importance_df)))\n",
    "        ax.set_yticklabels(importance_df['Feature'])\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_xlabel('Importance', fontsize=11)\n",
    "        ax.set_title(f\"Top 20 Features - {result['model_name']}\", fontsize=13, fontweight='bold')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ“ Feature importance plots saved: feature_importance_comparison.png\")\n",
    "else:\n",
    "    print(\"âš  No models with feature importance available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeead7c",
   "metadata": {},
   "source": [
    "## 8. Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb49e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*100}\")\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "for result in all_results:\n",
    "    print(f\"\\n{'â”€'*100}\")\n",
    "    print(f\"{result['model_name']}\")\n",
    "    print(f\"{'â”€'*100}\")\n",
    "    print(classification_report(y_val, result['y_pred'], \n",
    "                                target_names=['Legitimate (0)', 'Fraud (1)'],\n",
    "                                digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7dc88a",
   "metadata": {},
   "source": [
    "## 9. Threshold Tuning for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fdba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model by F1-score\n",
    "best_model_result = max(all_results, key=lambda x: x['f1'])\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"THRESHOLD TUNING: {best_model_result['model_name']}\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "if best_model_result['y_pred_proba'] is not None:\n",
    "    # Test different thresholds\n",
    "    thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "    metrics_by_threshold = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresh = (best_model_result['y_pred_proba'] >= threshold).astype(int)\n",
    "        \n",
    "        from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "        \n",
    "        f1 = f1_score(y_val, y_pred_thresh)\n",
    "        precision = precision_score(y_val, y_pred_thresh)\n",
    "        recall = recall_score(y_val, y_pred_thresh)\n",
    "        \n",
    "        metrics_by_threshold.append({\n",
    "            'Threshold': threshold,\n",
    "            'F1-Score': f1,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall\n",
    "        })\n",
    "    \n",
    "    threshold_df = pd.DataFrame(metrics_by_threshold)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Line plot\n",
    "    ax = axes[0]\n",
    "    ax.plot(threshold_df['Threshold'], threshold_df['F1-Score'], \n",
    "           marker='o', linewidth=2, label='F1-Score')\n",
    "    ax.plot(threshold_df['Threshold'], threshold_df['Precision'], \n",
    "           marker='s', linewidth=2, label='Precision')\n",
    "    ax.plot(threshold_df['Threshold'], threshold_df['Recall'], \n",
    "           marker='^', linewidth=2, label='Recall')\n",
    "    ax.set_xlabel('Threshold', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Metrics vs Threshold', fontsize=13, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Best thresholds\n",
    "    best_f1_idx = threshold_df['F1-Score'].idxmax()\n",
    "    best_recall_idx = threshold_df['Recall'].idxmax()\n",
    "    \n",
    "    ax = axes[1]\n",
    "    bars = ['Default (0.5)', 'Best F1', 'Best Recall']\n",
    "    \n",
    "    default_metrics = threshold_df.iloc[(threshold_df['Threshold'] - 0.5).abs().argmin()]\n",
    "    best_f1_metrics = threshold_df.iloc[best_f1_idx]\n",
    "    best_recall_metrics = threshold_df.iloc[best_recall_idx]\n",
    "    \n",
    "    f1_scores = [default_metrics['F1-Score'], best_f1_metrics['F1-Score'], best_recall_metrics['F1-Score']]\n",
    "    recalls = [default_metrics['Recall'], best_f1_metrics['Recall'], best_recall_metrics['Recall']]\n",
    "    precisions = [default_metrics['Precision'], best_f1_metrics['Precision'], best_recall_metrics['Precision']]\n",
    "    \n",
    "    x = np.arange(len(bars))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax.bar(x - width, f1_scores, width, label='F1-Score', color='steelblue')\n",
    "    ax.bar(x, recalls, width, label='Recall', color='coral')\n",
    "    ax.bar(x + width, precisions, width, label='Precision', color='lightgreen')\n",
    "    \n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Comparison of Threshold Strategies', fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(bars)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('threshold_tuning_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Default Threshold (0.5):\")\n",
    "    print(f\"  F1-Score: {default_metrics['F1-Score']:.4f}\")\n",
    "    print(f\"  Precision: {default_metrics['Precision']:.4f}\")\n",
    "    print(f\"  Recall: {default_metrics['Recall']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest F1-Score Threshold ({best_f1_metrics['Threshold']:.2f}):\")\n",
    "    print(f\"  F1-Score: {best_f1_metrics['F1-Score']:.4f}\")\n",
    "    print(f\"  Precision: {best_f1_metrics['Precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_f1_metrics['Recall']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest Recall Threshold ({best_recall_metrics['Threshold']:.2f}):\")\n",
    "    print(f\"  F1-Score: {best_recall_metrics['F1-Score']:.4f}\")\n",
    "    print(f\"  Precision: {best_recall_metrics['Precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_recall_metrics['Recall']:.4f}\")\n",
    "    \n",
    "    print(\"\\nâœ“ Threshold tuning analysis saved: threshold_tuning_analysis.png\")\n",
    "else:\n",
    "    print(\"âš  Best model does not support probability predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe7d83",
   "metadata": {},
   "source": [
    "## 10. Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5450e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame([{\n",
    "    'Model': r['model_name'],\n",
    "    'F1-Score': r['f1'],\n",
    "    'Recall': r['recall'],\n",
    "    'Precision': r['precision'],\n",
    "    'Accuracy': r['accuracy']\n",
    "} for r in all_results])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['F1-Score', 'Recall', 'Precision', 'Accuracy']\n",
    "colors = ['steelblue', 'coral', 'lightgreen', 'mediumpurple']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    sorted_df = comparison_df.sort_values(metric, ascending=True)\n",
    "    \n",
    "    ax.barh(range(len(sorted_df)), sorted_df[metric], color=color, alpha=0.7)\n",
    "    ax.set_yticks(range(len(sorted_df)))\n",
    "    ax.set_yticklabels(sorted_df['Model'])\n",
    "    ax.set_xlabel('Score', fontsize=11)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=13, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(sorted_df[metric]):\n",
    "        ax.text(v + 0.005, i, f'{v:.4f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Model comparison visualization saved: model_comparison_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57151bb5",
   "metadata": {},
   "source": [
    "## 11. Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*100}\")\n",
    "print(\"FINAL MODEL SELECTION RECOMMENDATIONS\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "# Best by different criteria\n",
    "best_f1 = max(all_results, key=lambda x: x['f1'])\n",
    "best_recall = max(all_results, key=lambda x: x['recall'])\n",
    "best_precision = max(all_results, key=lambda x: x['precision'])\n",
    "\n",
    "roc_results = [r for r in all_results if r['roc_auc'] is not None]\n",
    "best_roc = max(roc_results, key=lambda x: x['roc_auc']) if roc_results else None\n",
    "\n",
    "print(\"ðŸ“Š Best Models by Metric:\\n\")\n",
    "print(f\"1. Best F1-Score (Balanced Performance):\")\n",
    "print(f\"   Model: {best_f1['model_name']}\")\n",
    "print(f\"   F1: {best_f1['f1']:.4f} | Precision: {best_f1['precision']:.4f} | Recall: {best_f1['recall']:.4f}\\n\")\n",
    "\n",
    "print(f\"2. Best Recall (Catch Most Frauds):\")\n",
    "print(f\"   Model: {best_recall['model_name']}\")\n",
    "print(f\"   Recall: {best_recall['recall']:.4f} | F1: {best_recall['f1']:.4f} | Precision: {best_recall['precision']:.4f}\\n\")\n",
    "\n",
    "print(f\"3. Best Precision (Minimize False Alarms):\")\n",
    "print(f\"   Model: {best_precision['model_name']}\")\n",
    "print(f\"   Precision: {best_precision['precision']:.4f} | F1: {best_precision['f1']:.4f} | Recall: {best_precision['recall']:.4f}\\n\")\n",
    "\n",
    "if best_roc:\n",
    "    print(f\"4. Best ROC-AUC (Overall Discrimination):\")\n",
    "    print(f\"   Model: {best_roc['model_name']}\")\n",
    "    print(f\"   ROC-AUC: {best_roc['roc_auc']:.4f} | F1: {best_roc['f1']:.4f}\\n\")\n",
    "\n",
    "print(f\"\\n{'â”€'*100}\\n\")\n",
    "print(\"ðŸ’¡ Business Context Recommendations:\\n\")\n",
    "\n",
    "print(\"For FRAUD DETECTION (High Stakes):\")\n",
    "print(f\"  â†’ Prioritize RECALL to catch maximum fraudulent transactions\")\n",
    "print(f\"  â†’ Recommended: {best_recall['model_name']} (Recall: {best_recall['recall']:.4f})\")\n",
    "print(f\"  â†’ Consider lowering threshold to increase recall further\\n\")\n",
    "\n",
    "print(\"For BALANCED APPROACH:\")\n",
    "print(f\"  â†’ Use F1-Score to balance precision and recall\")\n",
    "print(f\"  â†’ Recommended: {best_f1['model_name']} (F1: {best_f1['f1']:.4f})\")\n",
    "print(f\"  â†’ Best overall performance for most use cases\\n\")\n",
    "\n",
    "print(\"For MINIMAL FALSE ALARMS:\")\n",
    "print(f\"  â†’ Prioritize PRECISION when manual review is costly\")\n",
    "print(f\"  â†’ Recommended: {best_precision['model_name']} (Precision: {best_precision['precision']:.4f})\")\n",
    "print(f\"  â†’ Consider raising threshold to reduce false positives\\n\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"NEXT STEPS\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "print(\"1. Load the best model based on your business requirements\")\n",
    "print(\"2. Apply threshold tuning if needed\")\n",
    "print(\"3. Make predictions on test_transaction.csv\")\n",
    "print(\"4. Monitor model performance in production\")\n",
    "print(\"5. Consider ensemble methods combining top models\\n\")\n",
    "\n",
    "print(f\"{'='*100}\")\n",
    "print(\"âœ… EVALUATION COMPLETE - All visualizations saved!\")\n",
    "print(f\"{'='*100}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
