{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7fcb73",
   "metadata": {},
   "source": [
    "# üìä Customer Clustering - Model Evaluation\n",
    "\n",
    "This notebook provides comprehensive evaluation and comparison of all trained clustering models.\n",
    "\n",
    "## Evaluation Sections:\n",
    "1. **Load Models & Results** - Import all trained models\n",
    "2. **Performance Metrics** - Detailed metric comparison\n",
    "3. **Cluster Visualization (2D/3D)** - PCA and t-SNE projections\n",
    "4. **Cluster Profiles** - Analyze characteristics of each cluster\n",
    "5. **Silhouette Analysis** - Per-cluster silhouette scores\n",
    "6. **Cluster Size Distribution** - Balance analysis\n",
    "7. **Feature Importance** - Which features drive clustering?\n",
    "8. **Business Insights** - Actionable customer segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Clustering and metrics\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Statistics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100f435",
   "metadata": {},
   "source": [
    "## üìÇ Load Data, Models, and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e00b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scaled data\n",
    "print(\"Loading data...\")\n",
    "df_scaled = pd.read_csv('clustering_scaled_standard.csv')\n",
    "X = df_scaled.values\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {X.shape[0]:,}\")\n",
    "\n",
    "# Load original cleaned data for profiling\n",
    "df_original = pd.read_csv('clustering_cleaned.csv')\n",
    "print(f\"\\n‚úì Original data loaded for cluster profiling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clustering results\n",
    "print(\"\\nLoading clustering results...\")\n",
    "\n",
    "with open('all_clustering_results.pkl', 'rb') as f:\n",
    "    all_results = pickle.load(f)\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv('clustering_labels.csv')\n",
    "\n",
    "# Load comparison results\n",
    "comparison_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "print(f\"‚úì Loaded results for {len(all_results)} models\")\n",
    "print(f\"\\nModels: {list(labels_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b46b2",
   "metadata": {},
   "source": [
    "## üìä Section 1: Performance Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed comparison\n",
    "print(\"=\"*80)\n",
    "print(\"CLUSTERING MODELS - PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "display(comparison_df.style.background_gradient(subset=['Silhouette'], cmap='Greens')\n",
    "                           .background_gradient(subset=['Calinski-Harabasz'], cmap='Blues'))\n",
    "\n",
    "# Identify best models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ BEST MODELS BY METRIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_silhouette = comparison_df.loc[comparison_df['Silhouette'].idxmax()]\n",
    "print(f\"\\n‚ú® Best Silhouette Score: {best_silhouette['Model']} ({best_silhouette['Silhouette']:.4f})\")\n",
    "\n",
    "best_calinski = comparison_df.loc[comparison_df['Calinski-Harabasz'].idxmax()]\n",
    "print(f\"‚ú® Best Calinski-Harabasz: {best_calinski['Model']} ({best_calinski['Calinski-Harabasz']:.2f})\")\n",
    "\n",
    "# Davies-Bouldin (lower is better)\n",
    "db_valid = comparison_df[comparison_df['Davies-Bouldin'] != float('inf')]\n",
    "if len(db_valid) > 0:\n",
    "    best_db = db_valid.loc[db_valid['Davies-Bouldin'].idxmin()]\n",
    "    print(f\"‚ú® Best Davies-Bouldin: {best_db['Model']} ({best_db['Davies-Bouldin']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c4046",
   "metadata": {},
   "source": [
    "## üé® Section 2: Cluster Visualizations (2D & 3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for 2D visualization\n",
    "print(\"Applying PCA for 2D visualization...\")\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "X_pca_2d = pca_2d.fit_transform(X)\n",
    "\n",
    "explained_var_2d = pca_2d.explained_variance_ratio_\n",
    "print(f\"‚úì PCA 2D: {explained_var_2d[0]*100:.2f}% + {explained_var_2d[1]*100:.2f}% = {sum(explained_var_2d)*100:.2f}% variance explained\")\n",
    "\n",
    "# Apply PCA for 3D visualization\n",
    "print(\"\\nApplying PCA for 3D visualization...\")\n",
    "pca_3d = PCA(n_components=3, random_state=42)\n",
    "X_pca_3d = pca_3d.fit_transform(X)\n",
    "\n",
    "explained_var_3d = pca_3d.explained_variance_ratio_\n",
    "print(f\"‚úì PCA 3D: {sum(explained_var_3d)*100:.2f}% variance explained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1477af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D PCA Visualization for all models\n",
    "model_names = list(labels_df.columns)\n",
    "n_models = len(model_names)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    ax = axes[idx]\n",
    "    labels = labels_df[model_name].values\n",
    "    \n",
    "    # Get unique clusters (exclude noise if present)\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels[unique_labels >= 0])\n",
    "    \n",
    "    # Plot clusters\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            # Noise points (DBSCAN)\n",
    "            mask = labels == label\n",
    "            ax.scatter(X_pca_2d[mask, 0], X_pca_2d[mask, 1], \n",
    "                      c='gray', marker='x', s=30, alpha=0.3, label='Noise')\n",
    "        else:\n",
    "            mask = labels == label\n",
    "            ax.scatter(X_pca_2d[mask, 0], X_pca_2d[mask, 1], \n",
    "                      s=50, alpha=0.6, label=f'Cluster {label}', edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel(f'PC1 ({explained_var_2d[0]*100:.1f}%)', fontweight='bold')\n",
    "    ax.set_ylabel(f'PC2 ({explained_var_2d[1]*100:.1f}%)', fontweight='bold')\n",
    "    ax.set_title(f'{model_name}\\n({n_clusters} clusters)', fontsize=12, fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Add legend if not too many clusters\n",
    "    if n_clusters <= 10:\n",
    "        ax.legend(loc='best', fontsize=8, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('clusters_pca_2d.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì 2D PCA visualization saved: clusters_pca_2d.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c265ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D PCA Visualization for best model\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_labels = labels_df[best_model_name].values\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "unique_labels = np.unique(best_labels[best_labels >= 0])\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for idx, label in enumerate(unique_labels):\n",
    "    mask = best_labels == label\n",
    "    ax.scatter(X_pca_3d[mask, 0], X_pca_3d[mask, 1], X_pca_3d[mask, 2],\n",
    "              c=[colors[idx]], s=50, alpha=0.6, label=f'Cluster {label}', \n",
    "              edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Handle noise points if present\n",
    "if -1 in best_labels:\n",
    "    mask = best_labels == -1\n",
    "    ax.scatter(X_pca_3d[mask, 0], X_pca_3d[mask, 1], X_pca_3d[mask, 2],\n",
    "              c='gray', marker='x', s=30, alpha=0.3, label='Noise')\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({explained_var_3d[0]*100:.1f}%)', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel(f'PC2 ({explained_var_3d[1]*100:.1f}%)', fontweight='bold', fontsize=12)\n",
    "ax.set_zlabel(f'PC3 ({explained_var_3d[2]*100:.1f}%)', fontweight='bold', fontsize=12)\n",
    "ax.set_title(f'3D PCA Visualization - {best_model_name}\\n(Best Model)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('clusters_pca_3d_best.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì 3D visualization saved: clusters_pca_3d_best.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a74f0",
   "metadata": {},
   "source": [
    "## üìà Section 3: Silhouette Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed silhouette analysis for best model\n",
    "print(\"=\"*80)\n",
    "print(f\"SILHOUETTE ANALYSIS - {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate silhouette scores for each sample\n",
    "mask = best_labels >= 0  # Exclude noise\n",
    "sample_silhouette_values = silhouette_samples(X[mask], best_labels[mask])\n",
    "\n",
    "# Create silhouette plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "y_lower = 10\n",
    "unique_labels = np.unique(best_labels[best_labels >= 0])\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for idx, label in enumerate(unique_labels):\n",
    "    # Get silhouette scores for this cluster\n",
    "    cluster_mask = best_labels[mask] == label\n",
    "    cluster_silhouette_values = sample_silhouette_values[cluster_mask]\n",
    "    cluster_silhouette_values.sort()\n",
    "    \n",
    "    size_cluster = cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster\n",
    "    \n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_values,\n",
    "                     facecolor=colors[idx], edgecolor=colors[idx], alpha=0.7)\n",
    "    \n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax.text(-0.05, y_lower + 0.5 * size_cluster, f'Cluster {label}', \n",
    "            fontsize=10, fontweight='bold')\n",
    "    \n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "# Overall average silhouette score\n",
    "avg_silhouette = silhouette_score(X[mask], best_labels[mask])\n",
    "ax.axvline(x=avg_silhouette, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Average Silhouette: {avg_silhouette:.3f}')\n",
    "\n",
    "ax.set_xlabel('Silhouette Coefficient', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Cluster', fontweight='bold', fontsize=12)\n",
    "ax.set_title(f'Silhouette Analysis - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_yticks([])\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('silhouette_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Silhouette analysis saved: silhouette_analysis.png\")\n",
    "print(f\"\\nAverage Silhouette Score: {avg_silhouette:.4f}\")\n",
    "\n",
    "# Per-cluster statistics\n",
    "print(\"\\nPer-Cluster Silhouette Scores:\")\n",
    "for label in unique_labels:\n",
    "    cluster_mask = best_labels[mask] == label\n",
    "    cluster_silhouette = sample_silhouette_values[cluster_mask].mean()\n",
    "    cluster_size = cluster_mask.sum()\n",
    "    print(f\"  Cluster {label}: {cluster_silhouette:.4f} ({cluster_size:,} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f69d2",
   "metadata": {},
   "source": [
    "## üì¶ Section 4: Cluster Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d39974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster size distribution for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    ax = axes[idx]\n",
    "    labels = labels_df[model_name].values\n",
    "    \n",
    "    # Count samples per cluster\n",
    "    unique, counts = np.unique(labels[labels >= 0], return_counts=True)\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.bar([f'C{l}' for l in unique], counts, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Color bars\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique)))\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_facecolor(color)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = counts.sum()\n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{count:,}\\n({count/total*100:.1f}%)',\n",
    "               ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Handle noise\n",
    "    n_noise = np.sum(labels == -1)\n",
    "    if n_noise > 0:\n",
    "        ax.text(0.02, 0.98, f'Noise: {n_noise:,} ({n_noise/len(labels)*100:.1f}%)',\n",
    "               transform=ax.transAxes, va='top', fontsize=9,\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    ax.set_xlabel('Cluster', fontweight='bold')\n",
    "    ax.set_ylabel('Number of Samples', fontweight='bold')\n",
    "    ax.set_title(f'{model_name}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cluster_size_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Cluster size distribution saved: cluster_size_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e856cd",
   "metadata": {},
   "source": [
    "## üîç Section 5: Cluster Profiling (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c12d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluster profiles using original data\n",
    "print(\"=\"*80)\n",
    "print(f\"CLUSTER PROFILING - {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Add cluster labels to original data\n",
    "df_profiling = df_original.copy()\n",
    "df_profiling['Cluster'] = best_labels\n",
    "\n",
    "# Remove noise points if any\n",
    "df_profiling_clean = df_profiling[df_profiling['Cluster'] >= 0]\n",
    "\n",
    "# Calculate statistics per cluster\n",
    "cluster_profiles = df_profiling_clean.groupby('Cluster').agg(['mean', 'std', 'min', 'max'])\n",
    "\n",
    "print(\"\\nCluster Profiles (Mean values):\")\n",
    "display(cluster_profiles.xs('mean', level=1, axis=1).style.background_gradient(cmap='YlOrRd'))\n",
    "\n",
    "# Save profiles\n",
    "cluster_profiles.to_csv('cluster_profiles.csv')\n",
    "print(\"\\n‚úì Cluster profiles saved: cluster_profiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a94a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster profiles with radar chart\n",
    "from math import pi\n",
    "\n",
    "# Get mean values for each cluster\n",
    "cluster_means = df_profiling_clean.groupby('Cluster').mean()\n",
    "features = cluster_means.columns.tolist()\n",
    "n_features = len(features)\n",
    "\n",
    "# Normalize to 0-1 for radar chart\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "cluster_means_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(cluster_means),\n",
    "    columns=features,\n",
    "    index=cluster_means.index\n",
    ")\n",
    "\n",
    "# Create radar chart for each cluster\n",
    "n_clusters = len(cluster_means_scaled)\n",
    "angles = [n / float(n_features) * 2 * pi for n in range(n_features)]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, axes = plt.subplots(1, min(n_clusters, 3), figsize=(18, 6), \n",
    "                        subplot_kw=dict(projection='polar'))\n",
    "\n",
    "if n_clusters == 1:\n",
    "    axes = [axes]\n",
    "elif n_clusters == 2:\n",
    "    axes = axes\n",
    "else:\n",
    "    axes = axes[:min(n_clusters, 3)]\n",
    "\n",
    "for idx, (cluster_id, ax) in enumerate(zip(cluster_means_scaled.index[:3], axes)):\n",
    "    values = cluster_means_scaled.loc[cluster_id].values.tolist()\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=f'Cluster {cluster_id}')\n",
    "    ax.fill(angles, values, alpha=0.25)\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(features, size=8)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(f'Cluster {cluster_id} Profile', size=12, fontweight='bold', pad=20)\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cluster_profiles_radar.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Radar chart saved: cluster_profiles_radar.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ce25b",
   "metadata": {},
   "source": [
    "## üéØ Section 6: Feature Importance for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3df42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature variance between clusters\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE FOR CLUSTERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate variance between cluster means\n",
    "cluster_means = df_profiling_clean.groupby('Cluster').mean()\n",
    "feature_variance = cluster_means.var(axis=0).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nFeature Variance Between Clusters (Higher = More Important):\")\n",
    "print(feature_variance)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(feature_variance)), feature_variance.values, alpha=0.7, color='steelblue')\n",
    "plt.yticks(range(len(feature_variance)), feature_variance.index)\n",
    "plt.xlabel('Variance Between Cluster Means', fontweight='bold')\n",
    "plt.ylabel('Feature', fontweight='bold')\n",
    "plt.title('Feature Importance for Clustering', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Feature importance saved: feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a288a5",
   "metadata": {},
   "source": [
    "## üíº Section 7: Business Insights & Customer Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate business insights\n",
    "print(\"=\"*80)\n",
    "print(\"BUSINESS INSIGHTS & CUSTOMER SEGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "unique_clusters = np.unique(best_labels[best_labels >= 0])\n",
    "\n",
    "# For each cluster, identify key characteristics\n",
    "for cluster_id in unique_clusters:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLUSTER {cluster_id} - CUSTOMER SEGMENT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    cluster_data = df_profiling_clean[df_profiling_clean['Cluster'] == cluster_id]\n",
    "    cluster_size = len(cluster_data)\n",
    "    cluster_pct = cluster_size / len(df_profiling_clean) * 100\n",
    "    \n",
    "    print(f\"\\nüìä Size: {cluster_size:,} customers ({cluster_pct:.1f}% of total)\")\n",
    "    \n",
    "    # Find distinguishing features (highest deviation from overall mean)\n",
    "    overall_mean = df_profiling_clean.drop('Cluster', axis=1).mean()\n",
    "    cluster_mean = cluster_data.drop('Cluster', axis=1).mean()\n",
    "    deviations = ((cluster_mean - overall_mean) / overall_mean * 100).abs().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nüîç Top 5 Distinguishing Features:\")\n",
    "    for feature in deviations.head(5).index:\n",
    "        cluster_val = cluster_mean[feature]\n",
    "        overall_val = overall_mean[feature]\n",
    "        deviation = deviations[feature]\n",
    "        direction = \"higher\" if cluster_val > overall_val else \"lower\"\n",
    "        print(f\"   ‚Ä¢ {feature}: {cluster_val:.2f} ({deviation:.1f}% {direction} than average)\")\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(f\"\\nüìà Statistical Summary:\")\n",
    "    summary = cluster_data.drop('Cluster', axis=1).describe().loc[['mean', 'std']]\n",
    "    print(summary.T.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ BUSINESS INSIGHTS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51839ff5",
   "metadata": {},
   "source": [
    "## üìù Section 8: Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "print(\"=\"*80)\n",
    "print(\"üìä CLUSTERING EVALUATION - FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ MODELS EVALUATED\")\n",
    "print(f\"   Total models: {len(all_results)}\")\n",
    "print(f\"   Models: {', '.join(model_names)}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ BEST PERFORMING MODEL\")\n",
    "print(f\"   üèÜ Model: {best_model_name}\")\n",
    "print(f\"   üìä Silhouette Score: {best_silhouette['Silhouette']:.4f}\")\n",
    "print(f\"   üìä Calinski-Harabasz: {best_silhouette['Calinski-Harabasz']:.2f}\")\n",
    "print(f\"   üìä Number of Clusters: {best_silhouette['N_Clusters']}\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ CLUSTER CHARACTERISTICS\")\n",
    "for cluster_id in unique_clusters:\n",
    "    cluster_size = len(df_profiling_clean[df_profiling_clean['Cluster'] == cluster_id])\n",
    "    cluster_pct = cluster_size / len(df_profiling_clean) * 100\n",
    "    print(f\"   ‚Ä¢ Cluster {cluster_id}: {cluster_size:,} customers ({cluster_pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ DATA QUALITY\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(X):,}\")\n",
    "print(f\"   ‚Ä¢ Features: {X.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Noise points: {np.sum(best_labels == -1)} ({np.sum(best_labels == -1)/len(best_labels)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ OUTPUT FILES CREATED\")\n",
    "print(\"   ‚úÖ clustering_results.csv - Model comparison\")\n",
    "print(\"   ‚úÖ cluster_profiles.csv - Cluster characteristics\")\n",
    "print(\"   ‚úÖ clusters_pca_2d.png - 2D visualizations\")\n",
    "print(\"   ‚úÖ clusters_pca_3d_best.png - 3D visualization\")\n",
    "print(\"   ‚úÖ silhouette_analysis.png - Silhouette plot\")\n",
    "print(\"   ‚úÖ cluster_size_distribution.png - Size distribution\")\n",
    "print(\"   ‚úÖ cluster_profiles_radar.png - Radar charts\")\n",
    "print(\"   ‚úÖ feature_importance.png - Feature analysis\")\n",
    "\n",
    "print(\"\\n6Ô∏è‚É£ RECOMMENDATIONS\")\n",
    "print(f\"   üìå Use {best_model_name} for production\")\n",
    "print(f\"   üìå Focus on top {min(5, len(feature_variance))} distinguishing features\")\n",
    "print(f\"   üìå Develop targeted strategies for each of {len(unique_clusters)} customer segments\")\n",
    "print(f\"   üìå Monitor cluster stability over time\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ CLUSTERING EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f550f94",
   "metadata": {},
   "source": [
    "## üíæ Export Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99832805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data with cluster assignments\n",
    "print(\"Exporting final results...\")\n",
    "\n",
    "# Add best model's clusters to original data\n",
    "df_final = df_original.copy()\n",
    "df_final['Cluster'] = best_labels\n",
    "df_final['Model'] = best_model_name\n",
    "\n",
    "# Save\n",
    "df_final.to_csv('customers_with_clusters.csv', index=False)\n",
    "print(\"‚úì Final dataset with clusters saved: customers_with_clusters.csv\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'Best_Model': best_model_name,\n",
    "    'N_Clusters': len(unique_clusters),\n",
    "    'Silhouette_Score': best_silhouette['Silhouette'],\n",
    "    'Calinski_Harabasz': best_silhouette['Calinski-Harabasz'],\n",
    "    'Davies_Bouldin': best_silhouette['Davies-Bouldin'],\n",
    "    'Total_Samples': len(X),\n",
    "    'Noise_Points': int(np.sum(best_labels == -1))\n",
    "}\n",
    "\n",
    "with open('clustering_summary.txt', 'w') as f:\n",
    "    f.write(\"CLUSTERING EVALUATION SUMMARY\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    for key, value in summary_stats.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(\"‚úì Summary statistics saved: clustering_summary.txt\")\n",
    "print(\"\\nüéâ All results exported successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
