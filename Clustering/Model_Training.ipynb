{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f93b7a",
   "metadata": {},
   "source": [
    "# üéØ Customer Clustering - Model Training\n",
    "\n",
    "This notebook trains multiple clustering models to group customers based on their features.\n",
    "\n",
    "**Dataset Context:** Customer segmentation for marketing and business strategy\n",
    "\n",
    "## Models to Train:\n",
    "1. **K-Means Clustering** - Popular partitioning algorithm\n",
    "2. **Mini-Batch K-Means** - Faster variant for large datasets\n",
    "3. **Agglomerative Hierarchical Clustering** - Bottom-up approach\n",
    "4. **DBSCAN** - Density-based clustering\n",
    "5. **Gaussian Mixture Model (GMM)** - Probabilistic clustering\n",
    "6. **Spectral Clustering** - Graph-based approach\n",
    "\n",
    "## Evaluation Metrics:\n",
    "- **Silhouette Score** - Cluster cohesion and separation (-1 to 1, higher is better)\n",
    "- **Calinski-Harabasz Index** - Variance ratio (higher is better)\n",
    "- **Davies-Bouldin Index** - Average similarity between clusters (lower is better)\n",
    "- **Inertia** - Sum of squared distances to cluster centers (K-Means only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Clustering algorithms\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering, DBSCAN, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Dimensionality reduction for visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e470f48",
   "metadata": {},
   "source": [
    "## Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8da95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scaled dataset (StandardScaler version recommended)\n",
    "print(\"Loading processed clustering data...\\n\")\n",
    "\n",
    "df = pd.read_csv('clustering_scaled_standard.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Features: {df.shape[1]}\")\n",
    "print(f\"Samples: {df.shape[0]:,}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a8de1",
   "metadata": {},
   "source": [
    "## üîç Determine Optimal Number of Clusters\n",
    "\n",
    "We'll use multiple methods to find the optimal k:\n",
    "1. **Elbow Method** - Find the \"elbow\" in inertia curve\n",
    "2. **Silhouette Analysis** - Maximize silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd14d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df.values\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {X.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5266dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method and Silhouette Analysis\n",
    "print(\"=\"*80)\n",
    "print(\"FINDING OPTIMAL NUMBER OF CLUSTERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "calinski_scores = []\n",
    "davies_bouldin_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    print(f\"\\nTesting k={k}...\")\n",
    "    \n",
    "    # Fit K-Means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    inertia = kmeans.inertia_\n",
    "    silhouette = silhouette_score(X, labels)\n",
    "    calinski = calinski_harabasz_score(X, labels)\n",
    "    davies_bouldin = davies_bouldin_score(X, labels)\n",
    "    \n",
    "    inertias.append(inertia)\n",
    "    silhouette_scores.append(silhouette)\n",
    "    calinski_scores.append(calinski)\n",
    "    davies_bouldin_scores.append(davies_bouldin)\n",
    "    \n",
    "    print(f\"  Inertia: {inertia:.2f}\")\n",
    "    print(f\"  Silhouette: {silhouette:.4f}\")\n",
    "    print(f\"  Calinski-Harabasz: {calinski:.2f}\")\n",
    "    print(f\"  Davies-Bouldin: {davies_bouldin:.4f}\")\n",
    "\n",
    "print(\"\\n‚úì Cluster analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff8ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Elbow Method\n",
    "axes[0, 0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel('Number of Clusters (k)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Inertia (Within-cluster sum of squares)', fontweight='bold')\n",
    "axes[0, 0].set_title('Elbow Method', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "axes[0, 0].set_xticks(k_range)\n",
    "\n",
    "# 2. Silhouette Score\n",
    "axes[0, 1].plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_xlabel('Number of Clusters (k)', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Silhouette Score', fontweight='bold')\n",
    "axes[0, 1].set_title('Silhouette Analysis (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].set_xticks(k_range)\n",
    "\n",
    "# Mark best k\n",
    "best_k_silhouette = list(k_range)[np.argmax(silhouette_scores)]\n",
    "axes[0, 1].axvline(x=best_k_silhouette, color='red', linestyle='--', alpha=0.7, label=f'Best k={best_k_silhouette}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Calinski-Harabasz Index\n",
    "axes[1, 0].plot(k_range, calinski_scores, 'mo-', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_xlabel('Number of Clusters (k)', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Calinski-Harabasz Index', fontweight='bold')\n",
    "axes[1, 0].set_title('Calinski-Harabasz Index (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "axes[1, 0].set_xticks(k_range)\n",
    "\n",
    "# 4. Davies-Bouldin Index\n",
    "axes[1, 1].plot(k_range, davies_bouldin_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1, 1].set_xlabel('Number of Clusters (k)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Davies-Bouldin Index', fontweight='bold')\n",
    "axes[1, 1].set_title('Davies-Bouldin Index (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "axes[1, 1].set_xticks(k_range)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimal_clusters_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Analysis saved as 'optimal_clusters_analysis.png'\")\n",
    "print(f\"\\nüìä Recommended k based on Silhouette Score: {best_k_silhouette}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10299c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimal k for training\n",
    "optimal_k = best_k_silhouette\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"OPTIMAL NUMBER OF CLUSTERS: k = {optimal_k}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"This value will be used for clustering algorithms that require k parameter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e565be5",
   "metadata": {},
   "source": [
    "## Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(X, labels, model_name, training_time, model=None):\n",
    "    \"\"\"\n",
    "    Evaluate clustering model performance\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Number of clusters\n",
    "    n_clusters = len(np.unique(labels[labels >= 0]))  # Exclude noise points (-1)\n",
    "    n_noise = np.sum(labels == -1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if n_clusters > 1 and n_noise < len(labels):  # Need at least 2 clusters\n",
    "        # For DBSCAN, exclude noise points\n",
    "        mask = labels >= 0\n",
    "        X_clean = X[mask]\n",
    "        labels_clean = labels[mask]\n",
    "        \n",
    "        if len(np.unique(labels_clean)) > 1:\n",
    "            silhouette = silhouette_score(X_clean, labels_clean)\n",
    "            calinski = calinski_harabasz_score(X_clean, labels_clean)\n",
    "            davies_bouldin = davies_bouldin_score(X_clean, labels_clean)\n",
    "        else:\n",
    "            silhouette = -1\n",
    "            calinski = 0\n",
    "            davies_bouldin = float('inf')\n",
    "    else:\n",
    "        silhouette = -1\n",
    "        calinski = 0\n",
    "        davies_bouldin = float('inf')\n",
    "    \n",
    "    # Get inertia for K-Means models\n",
    "    inertia = model.inertia_ if hasattr(model, 'inertia_') else None\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüìä Clustering Results:\")\n",
    "    print(f\"   Number of Clusters: {n_clusters}\")\n",
    "    if n_noise > 0:\n",
    "        print(f\"   Noise Points: {n_noise} ({n_noise/len(labels)*100:.2f}%)\")\n",
    "    print(f\"   Training Time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    print(f\"\\nüìà Evaluation Metrics:\")\n",
    "    print(f\"   Silhouette Score: {silhouette:.4f}\")\n",
    "    print(f\"   Calinski-Harabasz Index: {calinski:.2f}\")\n",
    "    print(f\"   Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "    if inertia is not None:\n",
    "        print(f\"   Inertia: {inertia:.2f}\")\n",
    "    \n",
    "    # Cluster size distribution\n",
    "    unique, counts = np.unique(labels[labels >= 0], return_counts=True)\n",
    "    print(f\"\\nüì¶ Cluster Sizes:\")\n",
    "    for cluster_id, count in zip(unique, counts):\n",
    "        print(f\"   Cluster {cluster_id}: {count:,} samples ({count/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'labels': labels,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'training_time': training_time,\n",
    "        'silhouette_score': silhouette,\n",
    "        'calinski_harabasz_score': calinski,\n",
    "        'davies_bouldin_score': davies_bouldin,\n",
    "        'inertia': inertia\n",
    "    }\n",
    "\n",
    "print(\"‚úì Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0610214",
   "metadata": {},
   "source": [
    "## Train Clustering Models\n",
    "\n",
    "### 1. K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa024c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering\n",
    "print(\"Training K-Means Clustering...\")\n",
    "\n",
    "start_time = time.time()\n",
    "kmeans_model = KMeans(\n",
    "    n_clusters=optimal_k,\n",
    "    init='k-means++',\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "kmeans_labels = kmeans_model.fit_predict(X)\n",
    "kmeans_time = time.time() - start_time\n",
    "\n",
    "kmeans_results = evaluate_clustering(X, kmeans_labels, 'K-Means', kmeans_time, kmeans_model)\n",
    "\n",
    "# Save model\n",
    "with open('model_kmeans.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans_model, f)\n",
    "print(\"\\n‚úì Model saved: model_kmeans.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0aef23",
   "metadata": {},
   "source": [
    "### 2. Mini-Batch K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e75b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Batch K-Means (faster for large datasets)\n",
    "print(\"Training Mini-Batch K-Means...\")\n",
    "\n",
    "start_time = time.time()\n",
    "minibatch_kmeans_model = MiniBatchKMeans(\n",
    "    n_clusters=optimal_k,\n",
    "    init='k-means++',\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    batch_size=1000,\n",
    "    random_state=42\n",
    ")\n",
    "minibatch_labels = minibatch_kmeans_model.fit_predict(X)\n",
    "minibatch_time = time.time() - start_time\n",
    "\n",
    "minibatch_results = evaluate_clustering(X, minibatch_labels, 'Mini-Batch K-Means', minibatch_time, minibatch_kmeans_model)\n",
    "\n",
    "# Save model\n",
    "with open('model_minibatch_kmeans.pkl', 'wb') as f:\n",
    "    pickle.dump(minibatch_kmeans_model, f)\n",
    "print(\"\\n‚úì Model saved: model_minibatch_kmeans.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b17fc4",
   "metadata": {},
   "source": [
    "### 3. Agglomerative Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72910130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative Clustering\n",
    "print(\"Training Agglomerative Hierarchical Clustering...\")\n",
    "\n",
    "start_time = time.time()\n",
    "agglomerative_model = AgglomerativeClustering(\n",
    "    n_clusters=optimal_k,\n",
    "    linkage='ward'\n",
    ")\n",
    "agglomerative_labels = agglomerative_model.fit_predict(X)\n",
    "agglomerative_time = time.time() - start_time\n",
    "\n",
    "agglomerative_results = evaluate_clustering(X, agglomerative_labels, 'Agglomerative Clustering', agglomerative_time, agglomerative_model)\n",
    "\n",
    "# Save model\n",
    "with open('model_agglomerative.pkl', 'wb') as f:\n",
    "    pickle.dump(agglomerative_model, f)\n",
    "print(\"\\n‚úì Model saved: model_agglomerative.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccbaceb",
   "metadata": {},
   "source": [
    "### 4. DBSCAN (Density-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa24da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN - Density-based clustering\n",
    "print(\"Training DBSCAN...\")\n",
    "\n",
    "start_time = time.time()\n",
    "dbscan_model = DBSCAN(\n",
    "    eps=0.5,\n",
    "    min_samples=5,\n",
    "    metric='euclidean',\n",
    "    n_jobs=-1\n",
    ")\n",
    "dbscan_labels = dbscan_model.fit_predict(X)\n",
    "dbscan_time = time.time() - start_time\n",
    "\n",
    "dbscan_results = evaluate_clustering(X, dbscan_labels, 'DBSCAN', dbscan_time, dbscan_model)\n",
    "\n",
    "# Save model\n",
    "with open('model_dbscan.pkl', 'wb') as f:\n",
    "    pickle.dump(dbscan_model, f)\n",
    "print(\"\\n‚úì Model saved: model_dbscan.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46302650",
   "metadata": {},
   "source": [
    "### 5. Gaussian Mixture Model (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1fa158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Mixture Model\n",
    "print(\"Training Gaussian Mixture Model...\")\n",
    "\n",
    "start_time = time.time()\n",
    "gmm_model = GaussianMixture(\n",
    "    n_components=optimal_k,\n",
    "    covariance_type='full',\n",
    "    max_iter=100,\n",
    "    random_state=42\n",
    ")\n",
    "gmm_model.fit(X)\n",
    "gmm_labels = gmm_model.predict(X)\n",
    "gmm_time = time.time() - start_time\n",
    "\n",
    "gmm_results = evaluate_clustering(X, gmm_labels, 'Gaussian Mixture Model', gmm_time, gmm_model)\n",
    "\n",
    "# Save model\n",
    "with open('model_gmm.pkl', 'wb') as f:\n",
    "    pickle.dump(gmm_model, f)\n",
    "print(\"\\n‚úì Model saved: model_gmm.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc269633",
   "metadata": {},
   "source": [
    "### 6. Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral Clustering\n",
    "print(\"Training Spectral Clustering...\")\n",
    "\n",
    "start_time = time.time()\n",
    "spectral_model = SpectralClustering(\n",
    "    n_clusters=optimal_k,\n",
    "    affinity='nearest_neighbors',\n",
    "    n_neighbors=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "spectral_labels = spectral_model.fit_predict(X)\n",
    "spectral_time = time.time() - start_time\n",
    "\n",
    "spectral_results = evaluate_clustering(X, spectral_labels, 'Spectral Clustering', spectral_time, spectral_model)\n",
    "\n",
    "# Save model\n",
    "with open('model_spectral.pkl', 'wb') as f:\n",
    "    pickle.dump(spectral_model, f)\n",
    "print(\"\\n‚úì Model saved: model_spectral.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0dc2b",
   "metadata": {},
   "source": [
    "## üìä Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a677a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = [\n",
    "    kmeans_results,\n",
    "    minibatch_results,\n",
    "    agglomerative_results,\n",
    "    dbscan_results,\n",
    "    gmm_results,\n",
    "    spectral_results\n",
    "]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame([{\n",
    "    'Model': r['model_name'],\n",
    "    'N_Clusters': r['n_clusters'],\n",
    "    'Noise_Points': r['n_noise'],\n",
    "    'Training_Time': f\"{r['training_time']:.2f}s\",\n",
    "    'Silhouette': r['silhouette_score'],\n",
    "    'Calinski-Harabasz': r['calinski_harabasz_score'],\n",
    "    'Davies-Bouldin': r['davies_bouldin_score']\n",
    "} for r in all_results])\n",
    "\n",
    "# Sort by Silhouette Score (descending)\n",
    "comparison_df = comparison_df.sort_values('Silhouette', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df)\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_silhouette = comparison_df.iloc[0]['Silhouette']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} (Silhouette Score = {best_silhouette:.4f})\")\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv('clustering_results.csv', index=False)\n",
    "print(\"\\n‚úì Results saved: clustering_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc004a",
   "metadata": {},
   "source": [
    "## üìà Visualize Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb75ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "models = comparison_df['Model']\n",
    "\n",
    "# 1. Silhouette Score\n",
    "axes[0, 0].barh(models, comparison_df['Silhouette'], color='skyblue', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Silhouette Score (Higher is Better)', fontweight='bold')\n",
    "axes[0, 0].set_title('Silhouette Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].invert_yaxis()\n",
    "axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Calinski-Harabasz Index\n",
    "axes[0, 1].barh(models, comparison_df['Calinski-Harabasz'], color='lightgreen', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Calinski-Harabasz Index (Higher is Better)', fontweight='bold')\n",
    "axes[0, 1].set_title('Calinski-Harabasz Index Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].invert_yaxis()\n",
    "axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Davies-Bouldin Index\n",
    "# Filter out infinity values\n",
    "db_valid = comparison_df[comparison_df['Davies-Bouldin'] != float('inf')]\n",
    "axes[1, 0].barh(db_valid['Model'], db_valid['Davies-Bouldin'], color='coral', alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Davies-Bouldin Index (Lower is Better)', fontweight='bold')\n",
    "axes[1, 0].set_title('Davies-Bouldin Index Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].invert_yaxis()\n",
    "axes[1, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Training Time\n",
    "training_times = [float(t.replace('s', '')) for t in comparison_df['Training_Time']]\n",
    "axes[1, 1].barh(models, training_times, color='gold', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Training Time (seconds)', fontweight='bold')\n",
    "axes[1, 1].set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].invert_yaxis()\n",
    "axes[1, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('clustering_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Comparison chart saved as 'clustering_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd2d5a",
   "metadata": {},
   "source": [
    "## üíæ Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13aac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results including labels\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING ALL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save results with pickle\n",
    "with open('all_clustering_results.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "print(\"‚úì All results saved: all_clustering_results.pkl\")\n",
    "\n",
    "# Save labels for each model\n",
    "labels_df = pd.DataFrame({\n",
    "    'K-Means': kmeans_labels,\n",
    "    'Mini-Batch K-Means': minibatch_labels,\n",
    "    'Agglomerative': agglomerative_labels,\n",
    "    'DBSCAN': dbscan_labels,\n",
    "    'GMM': gmm_labels,\n",
    "    'Spectral': spectral_labels\n",
    "})\n",
    "labels_df.to_csv('clustering_labels.csv', index=False)\n",
    "print(\"‚úì All labels saved: clustering_labels.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ CLUSTERING TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total models trained: {len(all_results)}\")\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  ‚Ä¢ clustering_results.csv - Performance comparison\")\n",
    "print(f\"  ‚Ä¢ clustering_labels.csv - All cluster labels\")\n",
    "print(f\"  ‚Ä¢ all_clustering_results.pkl - Complete results\")\n",
    "print(f\"  ‚Ä¢ model_*.pkl - Individual model files (6 models)\")\n",
    "print(f\"  ‚Ä¢ clustering_comparison.png - Visualization\")\n",
    "print(f\"  ‚Ä¢ optimal_clusters_analysis.png - Optimal k analysis\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
