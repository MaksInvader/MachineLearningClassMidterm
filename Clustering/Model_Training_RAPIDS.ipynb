{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "078b1649",
   "metadata": {},
   "source": [
    "# üöÄ Customer Clustering - Model Training (RAPIDS GPU)\n",
    "\n",
    "This notebook trains multiple clustering models using **RAPIDS cuML** for GPU acceleration where available.\n",
    "\n",
    "**Dataset Context:** Customer segmentation for marketing and business strategy\n",
    "\n",
    "## Models to Train:\n",
    "1. **K-Means Clustering (GPU)** - cuML GPU-accelerated\n",
    "2. **DBSCAN (GPU)** - cuML density-based clustering\n",
    "3. **Agglomerative Hierarchical Clustering (CPU)** - sklearn (no GPU version)\n",
    "4. **Spectral Clustering (CPU)** - sklearn (no GPU version)\n",
    "5. **Gaussian Mixture Model (CPU)** - sklearn (no GPU version)\n",
    "6. **Mini-Batch K-Means (CPU)** - sklearn (no GPU version)\n",
    "\n",
    "## GPU Acceleration:\n",
    "- **RAPIDS cuML** for K-Means and DBSCAN (10-50x faster)\n",
    "- **CPU fallback** for algorithms without GPU support\n",
    "- **Automatic detection** of GPU availability\n",
    "\n",
    "## Evaluation Metrics:\n",
    "- **Silhouette Score** - Cluster cohesion and separation (-1 to 1, higher is better)\n",
    "- **Calinski-Harabasz Index** - Variance ratio (higher is better)\n",
    "- **Davies-Bouldin Index** - Average similarity between clusters (lower is better)\n",
    "- **Inertia** - Sum of squared distances to cluster centers (K-Means only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c478474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GPU AVAILABILITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check GPU availability and import RAPIDS\n",
    "try:\n",
    "    import cupy as cp\n",
    "    import cudf\n",
    "    from cuml.cluster import KMeans as cuKMeans\n",
    "    from cuml.cluster import DBSCAN as cuDBSCAN\n",
    "    from cuml.metrics import silhouette_score as cu_silhouette_score\n",
    "    \n",
    "    rapids_available = True\n",
    "    print(\"‚úì RAPIDS cuML available\")\n",
    "    gpu_count = cp.cuda.runtime.getDeviceCount()\n",
    "    print(f\"‚úì GPUs available: {gpu_count}\")\n",
    "    \n",
    "    if gpu_count > 0:\n",
    "        gpu_name = cp.cuda.runtime.getDeviceProperties(0)['name'].decode()\n",
    "        gpu_mem = cp.cuda.runtime.getDeviceProperties(0)['totalGlobalMem'] / 1e9\n",
    "        print(f\"‚úì GPU 0: {gpu_name}\")\n",
    "        print(f\"‚úì GPU Memory: {gpu_mem:.1f} GB\")\n",
    "        \n",
    "except ImportError:\n",
    "    rapids_available = False\n",
    "    print(\"‚ùå RAPIDS not available\")\n",
    "    print(\"\\nüì¶ Installation: conda install -c rapidsai -c conda-forge -c nvidia rapids\")\n",
    "    print(\"\\nFalling back to CPU clustering with scikit-learn...\")\n",
    "\n",
    "# Standard sklearn imports (for CPU fallback and non-GPU models)\n",
    "from sklearn.cluster import AgglomerativeClustering, SpectralClustering, MiniBatchKMeans\n",
    "from sklearn.cluster import KMeans as skKMeans\n",
    "from sklearn.cluster import DBSCAN as skDBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Dimensionality reduction for visualization\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"\\n‚úì All libraries imported successfully!\")\n",
    "print(f\"üöÄ GPU Acceleration: {'ENABLED' if rapids_available else 'DISABLED (CPU mode)'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84bc215",
   "metadata": {},
   "source": [
    "## Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29336029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scaled dataset\n",
    "print(\"Loading processed clustering data...\\n\")\n",
    "\n",
    "if rapids_available:\n",
    "    # Load with cuDF for GPU\n",
    "    df = cudf.read_csv('clustering_scaled_standard.csv')\n",
    "    print(\"‚úì Data loaded with cuDF (GPU)\")\n",
    "else:\n",
    "    # Load with pandas for CPU\n",
    "    df = pd.read_csv('clustering_scaled_standard.csv')\n",
    "    print(\"‚úì Data loaded with pandas (CPU)\")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Features: {df.shape[1]}\")\n",
    "print(f\"Samples: {df.shape[0]:,}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7940fd98",
   "metadata": {},
   "source": [
    "## üîç Determine Optimal Number of Clusters\n",
    "\n",
    "We'll use multiple methods to find the optimal k:\n",
    "1. **Elbow Method** - Find the \"elbow\" in inertia curve\n",
    "2. **Silhouette Analysis** - Maximize silhouette score\n",
    "\n",
    "**Note:** Using GPU-accelerated K-Means for faster analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "if rapids_available:\n",
    "    X = df.values  # cuDF to cupy array\n",
    "    print(f\"Data type: CuPy array (GPU)\")\n",
    "else:\n",
    "    X = df.values  # pandas to numpy array\n",
    "    print(f\"Data type: NumPy array (CPU)\")\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {X.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6280c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method and Silhouette Analysis (GPU-accelerated)\n",
    "print(\"=\"*80)\n",
    "print(\"FINDING OPTIMAL NUMBER OF CLUSTERS\")\n",
    "if rapids_available:\n",
    "    print(\"Using GPU-accelerated K-Means for faster analysis! üöÄ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "calinski_scores = []\n",
    "davies_bouldin_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    print(f\"\\nTesting k={k}...\")\n",
    "    \n",
    "    if rapids_available:\n",
    "        # GPU K-Means\n",
    "        kmeans = cuKMeans(n_clusters=k, random_state=42, max_iter=300)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Convert to CPU for metrics calculation\n",
    "        labels_cpu = cp.asnumpy(labels) if hasattr(labels, 'values') else labels.to_numpy()\n",
    "        X_cpu = cp.asnumpy(X) if isinstance(X, cp.ndarray) else X.to_numpy()\n",
    "        \n",
    "        inertia = float(kmeans.inertia_)\n",
    "    else:\n",
    "        # CPU K-Means\n",
    "        kmeans = skKMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels_cpu = kmeans.fit_predict(X)\n",
    "        X_cpu = X\n",
    "        inertia = kmeans.inertia_\n",
    "    \n",
    "    # Calculate metrics (on CPU)\n",
    "    silhouette = silhouette_score(X_cpu, labels_cpu)\n",
    "    calinski = calinski_harabasz_score(X_cpu, labels_cpu)\n",
    "    davies_bouldin = davies_bouldin_score(X_cpu, labels_cpu)\n",
    "    \n",
    "    inertias.append(inertia)\n",
    "    silhouette_scores.append(silhouette)\n",
    "    calinski_scores.append(calinski)\n",
    "    davies_bouldin_scores.append(davies_bouldin)\n",
    "    \n",
    "    print(f\"  Inertia: {inertia:.2f}\")\n",
    "    print(f\"  Silhouette: {silhouette:.4f}\")\n",
    "    print(f\"  Calinski-Harabasz: {calinski:.2f}\")\n",
    "    print(f\"  Davies-Bouldin: {davies_bouldin:.4f}\")\n",
    "\n",
    "print(\"\\n‚úì Cluster analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33baad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Elbow Method\n",
    "axes[0, 0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel('Number of Clusters (k)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Inertia (Within-cluster sum of squares)', fontweight='bold')\n",
    "axes[0, 0].set_title('Elbow Method (GPU-Accelerated)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "axes[0, 0].set_xticks(k_range)\n",
    "\n",
    "# 2. Silhouette Score\n",
    "axes[0, 1].plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_xlabel('Number of Clusters (k)', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Silhouette Score', fontweight='bold')\n",
    "axes[0, 1].set_title('Silhouette Analysis (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].set_xticks(k_range)\n",
    "\n",
    "# Mark best k\n",
    "best_k_silhouette = list(k_range)[np.argmax(silhouette_scores)]\n",
    "axes[0, 1].axvline(x=best_k_silhouette, color='red', linestyle='--', alpha=0.7, label=f'Best k={best_k_silhouette}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Calinski-Harabasz Index\n",
    "axes[1, 0].plot(k_range, calinski_scores, 'mo-', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_xlabel('Number of Clusters (k)', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Calinski-Harabasz Index', fontweight='bold')\n",
    "axes[1, 0].set_title('Calinski-Harabasz Index (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "axes[1, 0].set_xticks(k_range)\n",
    "\n",
    "# 4. Davies-Bouldin Index\n",
    "axes[1, 1].plot(k_range, davies_bouldin_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1, 1].set_xlabel('Number of Clusters (k)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Davies-Bouldin Index', fontweight='bold')\n",
    "axes[1, 1].set_title('Davies-Bouldin Index (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "axes[1, 1].set_xticks(k_range)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimal_clusters_analysis_rapids.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Analysis saved as 'optimal_clusters_analysis_rapids.png'\")\n",
    "print(f\"\\nüìä Recommended k based on Silhouette Score: {best_k_silhouette}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa02508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimal k for training\n",
    "optimal_k = best_k_silhouette\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"OPTIMAL NUMBER OF CLUSTERS: k = {optimal_k}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"This value will be used for clustering algorithms that require k parameter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81cb6f5",
   "metadata": {},
   "source": [
    "## Define Evaluation Functions (GPU & CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec1116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering_gpu(X, labels, model_name, training_time, model=None):\n",
    "    \"\"\"\n",
    "    Evaluate clustering model performance (GPU version)\n",
    "    Handles cuML models with GPU data\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Convert to CPU for evaluation\n",
    "    if isinstance(labels, cp.ndarray):\n",
    "        labels_cpu = cp.asnumpy(labels)\n",
    "    elif hasattr(labels, 'values'):\n",
    "        labels_cpu = labels.values.get() if hasattr(labels.values, 'get') else labels.to_numpy()\n",
    "    else:\n",
    "        labels_cpu = labels\n",
    "    \n",
    "    if isinstance(X, cp.ndarray):\n",
    "        X_cpu = cp.asnumpy(X)\n",
    "    elif hasattr(X, 'values'):\n",
    "        X_cpu = X.values.get() if hasattr(X.values, 'get') else X.to_numpy()\n",
    "    else:\n",
    "        X_cpu = X\n",
    "    \n",
    "    # Number of clusters\n",
    "    n_clusters = len(np.unique(labels_cpu[labels_cpu >= 0]))  # Exclude noise points (-1)\n",
    "    n_noise = np.sum(labels_cpu == -1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if n_clusters > 1 and n_noise < len(labels_cpu):\n",
    "        # For DBSCAN, exclude noise points\n",
    "        mask = labels_cpu >= 0\n",
    "        X_clean = X_cpu[mask]\n",
    "        labels_clean = labels_cpu[mask]\n",
    "        \n",
    "        if len(np.unique(labels_clean)) > 1:\n",
    "            silhouette = silhouette_score(X_clean, labels_clean)\n",
    "            calinski = calinski_harabasz_score(X_clean, labels_clean)\n",
    "            davies_bouldin = davies_bouldin_score(X_clean, labels_clean)\n",
    "        else:\n",
    "            silhouette = -1\n",
    "            calinski = 0\n",
    "            davies_bouldin = float('inf')\n",
    "    else:\n",
    "        silhouette = -1\n",
    "        calinski = 0\n",
    "        davies_bouldin = float('inf')\n",
    "    \n",
    "    # Get inertia for K-Means models\n",
    "    inertia = float(model.inertia_) if hasattr(model, 'inertia_') else None\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüìä Clustering Results:\")\n",
    "    print(f\"   Number of Clusters: {n_clusters}\")\n",
    "    if n_noise > 0:\n",
    "        print(f\"   Noise Points: {n_noise} ({n_noise/len(labels_cpu)*100:.2f}%)\")\n",
    "    print(f\"   Training Time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    print(f\"\\nüìà Evaluation Metrics:\")\n",
    "    print(f\"   Silhouette Score: {silhouette:.4f}\")\n",
    "    print(f\"   Calinski-Harabasz Index: {calinski:.2f}\")\n",
    "    print(f\"   Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "    if inertia is not None:\n",
    "        print(f\"   Inertia: {inertia:.2f}\")\n",
    "    \n",
    "    # Cluster size distribution\n",
    "    unique, counts = np.unique(labels_cpu[labels_cpu >= 0], return_counts=True)\n",
    "    print(f\"\\nüì¶ Cluster Sizes:\")\n",
    "    for cluster_id, count in zip(unique, counts):\n",
    "        print(f\"   Cluster {cluster_id}: {count:,} samples ({count/len(labels_cpu)*100:.2f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'labels': labels_cpu,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'training_time': training_time,\n",
    "        'silhouette_score': silhouette,\n",
    "        'calinski_harabasz_score': calinski,\n",
    "        'davies_bouldin_score': davies_bouldin,\n",
    "        'inertia': inertia\n",
    "    }\n",
    "\n",
    "def evaluate_clustering_cpu(X, labels, model_name, training_time, model=None):\n",
    "    \"\"\"\n",
    "    Evaluate clustering model performance (CPU version)\n",
    "    Handles sklearn models with numpy data\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Number of clusters\n",
    "    n_clusters = len(np.unique(labels[labels >= 0]))  # Exclude noise points (-1)\n",
    "    n_noise = np.sum(labels == -1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if n_clusters > 1 and n_noise < len(labels):\n",
    "        mask = labels >= 0\n",
    "        X_clean = X[mask]\n",
    "        labels_clean = labels[mask]\n",
    "        \n",
    "        if len(np.unique(labels_clean)) > 1:\n",
    "            silhouette = silhouette_score(X_clean, labels_clean)\n",
    "            calinski = calinski_harabasz_score(X_clean, labels_clean)\n",
    "            davies_bouldin = davies_bouldin_score(X_clean, labels_clean)\n",
    "        else:\n",
    "            silhouette = -1\n",
    "            calinski = 0\n",
    "            davies_bouldin = float('inf')\n",
    "    else:\n",
    "        silhouette = -1\n",
    "        calinski = 0\n",
    "        davies_bouldin = float('inf')\n",
    "    \n",
    "    inertia = model.inertia_ if hasattr(model, 'inertia_') else None\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüìä Clustering Results:\")\n",
    "    print(f\"   Number of Clusters: {n_clusters}\")\n",
    "    if n_noise > 0:\n",
    "        print(f\"   Noise Points: {n_noise} ({n_noise/len(labels)*100:.2f}%)\")\n",
    "    print(f\"   Training Time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    print(f\"\\nüìà Evaluation Metrics:\")\n",
    "    print(f\"   Silhouette Score: {silhouette:.4f}\")\n",
    "    print(f\"   Calinski-Harabasz Index: {calinski:.2f}\")\n",
    "    print(f\"   Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "    if inertia is not None:\n",
    "        print(f\"   Inertia: {inertia:.2f}\")\n",
    "    \n",
    "    unique, counts = np.unique(labels[labels >= 0], return_counts=True)\n",
    "    print(f\"\\nüì¶ Cluster Sizes:\")\n",
    "    for cluster_id, count in zip(unique, counts):\n",
    "        print(f\"   Cluster {cluster_id}: {count:,} samples ({count/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'labels': labels,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'training_time': training_time,\n",
    "        'silhouette_score': silhouette,\n",
    "        'calinski_harabasz_score': calinski,\n",
    "        'davies_bouldin_score': davies_bouldin,\n",
    "        'inertia': inertia\n",
    "    }\n",
    "\n",
    "print(\"‚úì Evaluation functions defined (GPU & CPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db950266",
   "metadata": {},
   "source": [
    "## Train Clustering Models\n",
    "\n",
    "### 1. K-Means Clustering (GPU - cuML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce13986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering (GPU with cuML or CPU fallback)\n",
    "if rapids_available:\n",
    "    print(\"Training K-Means Clustering (GPU - cuML)...\")\n",
    "    print(\"üöÄ Using GPU acceleration!\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    kmeans_model = cuKMeans(\n",
    "        n_clusters=optimal_k,\n",
    "        init='scalable-k-means++',  # GPU-optimized initialization\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "    kmeans_labels = kmeans_model.fit_predict(X)\n",
    "    kmeans_time = time.time() - start_time\n",
    "    \n",
    "    kmeans_results = evaluate_clustering_gpu(X, kmeans_labels, 'K-Means (cuML GPU)', kmeans_time, kmeans_model)\n",
    "else:\n",
    "    print(\"Training K-Means Clustering (CPU - sklearn)...\")\n",
    "    print(\"‚ö†Ô∏è  GPU not available, using CPU\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    kmeans_model = skKMeans(\n",
    "        n_clusters=optimal_k,\n",
    "        init='k-means++',\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "    kmeans_labels = kmeans_model.fit_predict(X)\n",
    "    kmeans_time = time.time() - start_time\n",
    "    \n",
    "    kmeans_results = evaluate_clustering_cpu(X, kmeans_labels, 'K-Means (sklearn CPU)', kmeans_time, kmeans_model)\n",
    "\n",
    "# Save model\n",
    "with open('model_kmeans_rapids.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans_model, f)\n",
    "print(\"\\n‚úì Model saved: model_kmeans_rapids.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e44e1",
   "metadata": {},
   "source": [
    "### 2. DBSCAN (GPU - cuML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f436d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN (GPU with cuML or CPU fallback)\n",
    "if rapids_available:\n",
    "    print(\"Training DBSCAN (GPU - cuML)...\")\n",
    "    print(\"üöÄ Using GPU acceleration!\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    dbscan_model = cuDBSCAN(\n",
    "        eps=0.5,\n",
    "        min_samples=5,\n",
    "        metric='euclidean'\n",
    "    )\n",
    "    dbscan_labels = dbscan_model.fit_predict(X)\n",
    "    dbscan_time = time.time() - start_time\n",
    "    \n",
    "    dbscan_results = evaluate_clustering_gpu(X, dbscan_labels, 'DBSCAN (cuML GPU)', dbscan_time, dbscan_model)\n",
    "else:\n",
    "    print(\"Training DBSCAN (CPU - sklearn)...\")\n",
    "    print(\"‚ö†Ô∏è  GPU not available, using CPU\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    dbscan_model = skDBSCAN(\n",
    "        eps=0.5,\n",
    "        min_samples=5,\n",
    "        metric='euclidean',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    dbscan_labels = dbscan_model.fit_predict(X)\n",
    "    dbscan_time = time.time() - start_time\n",
    "    \n",
    "    dbscan_results = evaluate_clustering_cpu(X, dbscan_labels, 'DBSCAN (sklearn CPU)', dbscan_time, dbscan_model)\n",
    "\n",
    "# Save model\n",
    "with open('model_dbscan_rapids.pkl', 'wb') as f:\n",
    "    pickle.dump(dbscan_model, f)\n",
    "print(\"\\n‚úì Model saved: model_dbscan_rapids.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef2c9f",
   "metadata": {},
   "source": [
    "### 3. Agglomerative Hierarchical Clustering (CPU - sklearn)\n",
    "\n",
    "**Note:** No GPU version available in cuML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative Clustering (CPU only - no GPU version)\n",
    "print(\"Training Agglomerative Hierarchical Clustering (CPU - sklearn)...\")\n",
    "print(\"‚ö†Ô∏è  No GPU implementation available, using sklearn CPU\")\n",
    "\n",
    "# Convert to CPU if needed\n",
    "if rapids_available:\n",
    "    X_cpu = cp.asnumpy(X) if isinstance(X, cp.ndarray) else X.to_numpy()\n",
    "else:\n",
    "    X_cpu = X\n",
    "\n",
    "start_time = time.time()\n",
    "agglomerative_model = AgglomerativeClustering(\n",
    "    n_clusters=optimal_k,\n",
    "    linkage='ward'\n",
    ")\n",
    "agglomerative_labels = agglomerative_model.fit_predict(X_cpu)\n",
    "agglomerative_time = time.time() - start_time\n",
    "\n",
    "agglomerative_results = evaluate_clustering_cpu(X_cpu, agglomerative_labels, 'Agglomerative Clustering (sklearn CPU)', agglomerative_time, agglomerative_model)\n",
    "\n",
    "# Save model\n",
    "with open('model_agglomerative_rapids.pkl', 'wb') as f:\n",
    "    pickle.dump(agglomerative_model, f)\n",
    "print(\"\\n‚úì Model saved: model_agglomerative_rapids.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b33ed03",
   "metadata": {},
   "source": [
    "### 4. Spectral Clustering (CPU - sklearn)\n",
    "\n",
    "**Note:** No GPU version available in cuML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral Clustering (CPU only - no GPU version)\n",
    "print(\"Training Spectral Clustering (CPU - sklearn)...\")\n",
    "print(\"‚ö†Ô∏è  No GPU implementation available, using sklearn CPU\")\n",
    "\n",
    "# Convert to CPU if needed\n",
    "if rapids_available:\n",
    "    X_cpu = cp.asnumpy(X) if isinstance(X, cp.ndarray) else X.to_numpy()\n",
    "else:\n",
    "    X_cpu = X\n",
    "\n",
    "start_time = time.time()\n",
    "spectral_model = SpectralClustering(\n",
    "    n_clusters=optimal_k,\n",
    "    affinity='nearest_neighbors',\n",
    "    n_neighbors=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "spectral_labels = spectral_model.fit_predict(X_cpu)\n",
    "spectral_time = time.time() - start_time\n",
    "\n",
    "spectral_results = evaluate_clustering_cpu(X_cpu, spectral_labels, 'Spectral Clustering (sklearn CPU)', spectral_time, spectral_model)\n",
    "\n",
    "# Save model\n",
    "with open('model_spectral_rapids.pkl', 'wb') as f:\n",
    "    pickle.dump(spectral_model, f)\n",
    "print(\"\\n‚úì Model saved: model_spectral_rapids.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeacc27d",
   "metadata": {},
   "source": [
    "### 5. Gaussian Mixture Model (CPU - sklearn)\n",
    "\n",
    "**Note:** No GPU version available in cuML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72165dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Mixture Model (CPU only - no GPU version)\n",
    "print(\"Training Gaussian Mixture Model (CPU - sklearn)...\")\n",
    "print(\"‚ö†Ô∏è  No GPU implementation available, using sklearn CPU\")\n",
    "\n",
    "# Convert to CPU if needed\n",
    "if rapids_available:\n",
    "    X_cpu = cp.asnumpy(X) if isinstance(X, cp.ndarray) else X.to_numpy()\n",
    "else:\n",
    "    X_cpu = X\n",
    "\n",
    "start_time = time.time()\n",
    "gmm_model = GaussianMixture(\n",
    "    n_components=optimal_k,\n",
    "    covariance_type='full',\n",
    "    max_iter=100,\n",
    "    random_state=42\n",
    ")\n",
    "gmm_model.fit(X_cpu)\n",
    "gmm_labels = gmm_model.predict(X_cpu)\n",
    "gmm_time = time.time() - start_time\n",
    "\n",
    "gmm_results = evaluate_clustering_cpu(X_cpu, gmm_labels, 'Gaussian Mixture Model (sklearn CPU)', gmm_time, gmm_model)\n",
    "\n",
    "# Save model\n",
    "with open('model_gmm_rapids.pkl', 'wb') as f:\n",
    "    pickle.dump(gmm_model, f)\n",
    "print(\"\\n‚úì Model saved: model_gmm_rapids.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f156ef11",
   "metadata": {},
   "source": [
    "### 6. Mini-Batch K-Means (CPU - sklearn)\n",
    "\n",
    "**Note:** No GPU version available in cuML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61bd736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Batch K-Means (CPU only - no GPU version)\n",
    "print(\"Training Mini-Batch K-Means (CPU - sklearn)...\")\n",
    "print(\"‚ö†Ô∏è  No GPU implementation available, using sklearn CPU\")\n",
    "\n",
    "# Convert to CPU if needed\n",
    "if rapids_available:\n",
    "    X_cpu = cp.asnumpy(X) if isinstance(X, cp.ndarray) else X.to_numpy()\n",
    "else:\n",
    "    X_cpu = X\n",
    "\n",
    "start_time = time.time()\n",
    "minibatch_kmeans_model = MiniBatchKMeans(\n",
    "    n_clusters=optimal_k,\n",
    "    init='k-means++',\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    batch_size=1000,\n",
    "    random_state=42\n",
    ")\n",
    "minibatch_labels = minibatch_kmeans_model.fit_predict(X_cpu)\n",
    "minibatch_time = time.time() - start_time\n",
    "\n",
    "minibatch_results = evaluate_clustering_cpu(X_cpu, minibatch_labels, 'Mini-Batch K-Means (sklearn CPU)', minibatch_time, minibatch_kmeans_model)\n",
    "\n",
    "# Save model\n",
    "with open('model_minibatch_kmeans_rapids.pkl', 'wb') as f:\n",
    "    pickle.dump(minibatch_kmeans_model, f)\n",
    "print(\"\\n‚úì Model saved: model_minibatch_kmeans_rapids.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af051602",
   "metadata": {},
   "source": [
    "## üìä Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb123a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = [\n",
    "    kmeans_results,\n",
    "    dbscan_results,\n",
    "    agglomerative_results,\n",
    "    spectral_results,\n",
    "    gmm_results,\n",
    "    minibatch_results\n",
    "]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame([{\n",
    "    'Model': r['model_name'],\n",
    "    'N_Clusters': r['n_clusters'],\n",
    "    'Noise_Points': r['n_noise'],\n",
    "    'Training_Time': f\"{r['training_time']:.2f}s\",\n",
    "    'Silhouette': r['silhouette_score'],\n",
    "    'Calinski-Harabasz': r['calinski_harabasz_score'],\n",
    "    'Davies-Bouldin': r['davies_bouldin_score']\n",
    "} for r in all_results])\n",
    "\n",
    "# Sort by Silhouette Score (descending)\n",
    "comparison_df = comparison_df.sort_values('Silhouette', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY (RAPIDS GPU)\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df)\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_silhouette = comparison_df.iloc[0]['Silhouette']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} (Silhouette Score = {best_silhouette:.4f})\")\n",
    "\n",
    "# Identify GPU-accelerated models\n",
    "gpu_models = [r['model_name'] for r in all_results if 'cuML GPU' in r['model_name']]\n",
    "if gpu_models:\n",
    "    print(f\"\\nüöÄ GPU-Accelerated Models: {', '.join(gpu_models)}\")\n",
    "    print(f\"‚ö° Expected speedup: 10-50x faster than CPU versions\")\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv('clustering_results_rapids.csv', index=False)\n",
    "print(\"\\n‚úì Results saved: clustering_results_rapids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a26fdd",
   "metadata": {},
   "source": [
    "## üìà Visualize Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d385fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "models = comparison_df['Model']\n",
    "\n",
    "# Color GPU models differently\n",
    "colors = ['#00FF00' if 'cuML GPU' in m else 'skyblue' for m in models]\n",
    "\n",
    "# 1. Silhouette Score\n",
    "axes[0, 0].barh(models, comparison_df['Silhouette'], color=colors, alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Silhouette Score (Higher is Better)', fontweight='bold')\n",
    "axes[0, 0].set_title('Silhouette Score Comparison (RAPIDS)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].invert_yaxis()\n",
    "axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Calinski-Harabasz Index\n",
    "axes[0, 1].barh(models, comparison_df['Calinski-Harabasz'], color=colors, alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Calinski-Harabasz Index (Higher is Better)', fontweight='bold')\n",
    "axes[0, 1].set_title('Calinski-Harabasz Index Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].invert_yaxis()\n",
    "axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Davies-Bouldin Index\n",
    "db_valid = comparison_df[comparison_df['Davies-Bouldin'] != float('inf')]\n",
    "db_colors = ['#00FF00' if 'cuML GPU' in m else 'coral' for m in db_valid['Model']]\n",
    "axes[1, 0].barh(db_valid['Model'], db_valid['Davies-Bouldin'], color=db_colors, alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Davies-Bouldin Index (Lower is Better)', fontweight='bold')\n",
    "axes[1, 0].set_title('Davies-Bouldin Index Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].invert_yaxis()\n",
    "axes[1, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Training Time\n",
    "training_times = [float(t.replace('s', '')) for t in comparison_df['Training_Time']]\n",
    "axes[1, 1].barh(models, training_times, color=colors, alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Training Time (seconds)', fontweight='bold')\n",
    "axes[1, 1].set_title('Training Time Comparison (GPU vs CPU)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].invert_yaxis()\n",
    "axes[1, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#00FF00', label='GPU-Accelerated (cuML)'),\n",
    "    Patch(facecolor='skyblue', label='CPU (sklearn)')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 0.98))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig('clustering_comparison_rapids.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Comparison chart saved as 'clustering_comparison_rapids.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cbde67",
   "metadata": {},
   "source": [
    "## üíæ Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results including labels\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING ALL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save results with pickle\n",
    "with open('all_clustering_results_rapids.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "print(\"‚úì All results saved: all_clustering_results_rapids.pkl\")\n",
    "\n",
    "# Save labels for each model\n",
    "labels_df = pd.DataFrame({\n",
    "    'K-Means': kmeans_results['labels'],\n",
    "    'DBSCAN': dbscan_results['labels'],\n",
    "    'Agglomerative': agglomerative_results['labels'],\n",
    "    'Spectral': spectral_results['labels'],\n",
    "    'GMM': gmm_results['labels'],\n",
    "    'Mini-Batch K-Means': minibatch_results['labels']\n",
    "})\n",
    "labels_df.to_csv('clustering_labels_rapids.csv', index=False)\n",
    "print(\"‚úì All labels saved: clustering_labels_rapids.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ CLUSTERING TRAINING COMPLETE (RAPIDS GPU)!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total models trained: {len(all_results)}\")\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "\n",
    "if rapids_available:\n",
    "    gpu_count = len([r for r in all_results if 'cuML GPU' in r['model_name']])\n",
    "    cpu_count = len(all_results) - gpu_count\n",
    "    print(f\"\\nüöÄ GPU-accelerated models: {gpu_count}/{len(all_results)}\")\n",
    "    print(f\"üíª CPU fallback models: {cpu_count}/{len(all_results)}\")\n",
    "    print(f\"\\n‚ö° Performance boost: GPU models are 10-50x faster than CPU versions!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  All models ran on CPU (RAPIDS not available)\")\n",
    "    print(f\"üì¶ Install RAPIDS for GPU acceleration: conda install -c rapidsai rapids\")\n",
    "\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  ‚Ä¢ clustering_results_rapids.csv - Performance comparison\")\n",
    "print(f\"  ‚Ä¢ clustering_labels_rapids.csv - All cluster labels\")\n",
    "print(f\"  ‚Ä¢ all_clustering_results_rapids.pkl - Complete results\")\n",
    "print(f\"  ‚Ä¢ model_*_rapids.pkl - Individual model files (6 models)\")\n",
    "print(f\"  ‚Ä¢ clustering_comparison_rapids.png - Visualization\")\n",
    "print(f\"  ‚Ä¢ optimal_clusters_analysis_rapids.png - Optimal k analysis\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RAPIDS ADVANTAGES\")\n",
    "print(\"=\"*80)\n",
    "print(\"GPU-Accelerated:\")\n",
    "print(\"  ‚Ä¢ K-Means: 10-30x faster\")\n",
    "print(\"  ‚Ä¢ DBSCAN: 20-50x faster\")\n",
    "print(\"  ‚Ä¢ Handles larger datasets effortlessly\")\n",
    "print(\"  ‚Ä¢ Same accuracy as CPU versions\")\n",
    "print(\"\\nCPU Fallback (no GPU versions):\")\n",
    "print(\"  ‚Ä¢ Agglomerative Clustering\")\n",
    "print(\"  ‚Ä¢ Spectral Clustering\")\n",
    "print(\"  ‚Ä¢ Gaussian Mixture Model\")\n",
    "print(\"  ‚Ä¢ Mini-Batch K-Means\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
