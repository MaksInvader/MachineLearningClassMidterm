{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f151ab8e",
   "metadata": {},
   "source": [
    "# üöÄ GPU-Accelerated Model Evaluation - RAPIDS cuML\n",
    "\n",
    "This notebook provides comprehensive evaluation and comparison of all GPU-trained regression models using RAPIDS cuML.\n",
    "\n",
    "**Dataset Context:** Predicting the year a song was released based on audio characteristics (timbre, pitch, rhythm patterns, etc.)\n",
    "\n",
    "**GPU Acceleration Benefits:**\n",
    "- ‚ö° Fast metric calculations on GPU\n",
    "- üî• Quick prediction generation\n",
    "- üìä Efficient large-scale evaluation\n",
    "\n",
    "## Evaluation Metrics:\n",
    "- **RMSE** (Root Mean Squared Error) - Lower is better\n",
    "- **MAE** (Mean Absolute Error) - Lower is better  \n",
    "- **R¬≤** (Coefficient of Determination) - Higher is better (0 to 1)\n",
    "- **MAPE** (Mean Absolute Percentage Error) - Lower is better\n",
    "- **Residual Analysis**\n",
    "- **Prediction vs Actual Plots**\n",
    "- **Error Distribution Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4742b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import cupy as cp\n",
    "import cudf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "from scipy import stats\n",
    "\n",
    "# cuML metrics (GPU-accelerated)\n",
    "from cuml.metrics import mean_squared_error as cu_mse\n",
    "from cuml.metrics import r2_score as cu_r2_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úì RAPIDS cuML evaluation libraries imported!\")\n",
    "print(f\"‚úì GPU: {cp.cuda.runtime.getDeviceCount()} device(s) detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befcd9b9",
   "metadata": {},
   "source": [
    "## Load GPU-Trained Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d94683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all GPU model results\n",
    "print(\"Loading GPU-trained model results...\\n\")\n",
    "\n",
    "with open('all_model_results_rapids.pkl', 'rb') as f:\n",
    "    all_results = pickle.load(f)\n",
    "\n",
    "print(f\"‚úì Loaded results for {len(all_results)} GPU-trained models\")\n",
    "print(f\"\\nModels loaded:\")\n",
    "for i, result in enumerate(all_results, 1):\n",
    "    print(f\"  {i}. {result['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b35be32",
   "metadata": {},
   "source": [
    "## Load Test Data (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original scaled data to GPU\n",
    "print(\"Loading test data to GPU...\\n\")\n",
    "\n",
    "gdf = cudf.read_csv('cars_scaled_standard_rapids.csv')\n",
    "target_col = gdf.columns[0]\n",
    "X = gdf.drop(columns=[target_col])\n",
    "y = gdf[target_col]\n",
    "\n",
    "# Load test indices\n",
    "with open('test_indices_rapids.pkl', 'rb') as f:\n",
    "    test_indices = pickle.load(f)\n",
    "\n",
    "# Filter test set\n",
    "X_test = X.loc[test_indices]\n",
    "y_test = y.loc[test_indices]\n",
    "\n",
    "print(f\"‚úì Test data loaded to GPU\")\n",
    "print(f\"Test set size: {len(X_test):,} songs\")\n",
    "print(f\"Features: {X_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c63169",
   "metadata": {},
   "source": [
    "## üìä Detailed Metrics Calculation (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb4e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate detailed metrics for all models\n",
    "print(\"Calculating detailed metrics on GPU...\\n\")\n",
    "\n",
    "detailed_metrics = []\n",
    "\n",
    "for result in all_results:\n",
    "    model_name = result['model_name']\n",
    "    y_test_pred = result['y_test_pred']\n",
    "    \n",
    "    # Convert predictions to appropriate format\n",
    "    if hasattr(y_test_pred, 'to_numpy'):\n",
    "        y_pred_np = y_test_pred.to_numpy()\n",
    "    elif isinstance(y_test_pred, cp.ndarray):\n",
    "        y_pred_np = cp.asnumpy(y_test_pred)\n",
    "    else:\n",
    "        y_pred_np = y_test_pred\n",
    "        \n",
    "    y_test_np = y_test.to_numpy() if hasattr(y_test, 'to_numpy') else cp.asnumpy(y_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = result['test_rmse']\n",
    "    mae = result['test_mae']\n",
    "    r2 = result['test_r2']\n",
    "    \n",
    "    # MAPE\n",
    "    mape = mean_absolute_percentage_error(y_test_np, y_pred_np) * 100\n",
    "    \n",
    "    # Residuals\n",
    "    residuals = y_test_np - y_pred_np\n",
    "    \n",
    "    # Additional metrics\n",
    "    mean_residual = np.mean(residuals)\n",
    "    std_residual = np.std(residuals)\n",
    "    max_error = np.max(np.abs(residuals))\n",
    "    \n",
    "    # Accuracy within thresholds (¬±1 year, ¬±5 years, ¬±10 years)\n",
    "    within_1_year = np.sum(np.abs(residuals) <= 1) / len(residuals) * 100\n",
    "    within_5_years = np.sum(np.abs(residuals) <= 5) / len(residuals) * 100\n",
    "    within_10_years = np.sum(np.abs(residuals) <= 10) / len(residuals) * 100\n",
    "    \n",
    "    detailed_metrics.append({\n",
    "        'Model': model_name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2,\n",
    "        'MAPE (%)': mape,\n",
    "        'Mean Residual': mean_residual,\n",
    "        'Std Residual': std_residual,\n",
    "        'Max Error': max_error,\n",
    "        'Within ¬±1 Year (%)': within_1_year,\n",
    "        'Within ¬±5 Years (%)': within_5_years,\n",
    "        'Within ¬±10 Years (%)': within_10_years,\n",
    "        'Training Time (s)': result['training_time']\n",
    "    })\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_metrics)\n",
    "detailed_df = detailed_df.sort_values('RMSE', ascending=True)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"DETAILED MODEL METRICS - GPU EVALUATION\")\n",
    "print(\"=\"*100)\n",
    "print(detailed_df.to_string(index=False))\n",
    "\n",
    "# Save detailed metrics\n",
    "detailed_df.to_csv('detailed_model_metrics_rapids.csv', index=False)\n",
    "print(\"\\n‚úì Detailed metrics saved: detailed_model_metrics_rapids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7615ec4e",
   "metadata": {},
   "source": [
    "## üèÜ Model Ranking with Weighted Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76145852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted ranking system\n",
    "print(\"\\nCalculating weighted model ranking...\\n\")\n",
    "\n",
    "# Normalize metrics to 0-1 scale\n",
    "def normalize(series, lower_is_better=True):\n",
    "    if lower_is_better:\n",
    "        return 1 - (series - series.min()) / (series.max() - series.min())\n",
    "    else:\n",
    "        return (series - series.min()) / (series.max() - series.min())\n",
    "\n",
    "ranking_df = detailed_df[['Model', 'RMSE', 'MAE', 'R¬≤', 'MAPE (%)', 'Training Time (s)']].copy()\n",
    "\n",
    "# Normalize each metric\n",
    "ranking_df['RMSE_norm'] = normalize(ranking_df['RMSE'], lower_is_better=True)\n",
    "ranking_df['MAE_norm'] = normalize(ranking_df['MAE'], lower_is_better=True)\n",
    "ranking_df['R2_norm'] = normalize(ranking_df['R¬≤'], lower_is_better=False)\n",
    "ranking_df['MAPE_norm'] = normalize(ranking_df['MAPE (%)'], lower_is_better=True)\n",
    "ranking_df['Time_norm'] = normalize(ranking_df['Training Time (s)'], lower_is_better=True)\n",
    "\n",
    "# Weighted score (you can adjust weights)\n",
    "weights = {\n",
    "    'RMSE': 0.30,\n",
    "    'MAE': 0.25,\n",
    "    'R¬≤': 0.30,\n",
    "    'MAPE': 0.10,\n",
    "    'Time': 0.05\n",
    "}\n",
    "\n",
    "ranking_df['Weighted_Score'] = (\n",
    "    ranking_df['RMSE_norm'] * weights['RMSE'] +\n",
    "    ranking_df['MAE_norm'] * weights['MAE'] +\n",
    "    ranking_df['R2_norm'] * weights['R¬≤'] +\n",
    "    ranking_df['MAPE_norm'] * weights['MAPE'] +\n",
    "    ranking_df['Time_norm'] * weights['Time']\n",
    ")\n",
    "\n",
    "ranking_df = ranking_df.sort_values('Weighted_Score', ascending=False)\n",
    "ranking_df['Rank'] = range(1, len(ranking_df) + 1)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL RANKING (Weighted Score)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Weights: RMSE=30%, MAE=25%, R¬≤=30%, MAPE=10%, Time=5%\")\n",
    "print(\"=\"*80)\n",
    "print(ranking_df[['Rank', 'Model', 'Weighted_Score', 'RMSE', 'MAE', 'R¬≤']].to_string(index=False))\n",
    "\n",
    "# Save ranking\n",
    "ranking_df.to_csv('model_ranking_rapids.csv', index=False)\n",
    "print(\"\\n‚úì Model ranking saved: model_ranking_rapids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e192e3",
   "metadata": {},
   "source": [
    "## üìà Visualization 1: Performance Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3fd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "metrics_to_plot = [\n",
    "    ('RMSE', 'coral', 'lower'),\n",
    "    ('MAE', 'skyblue', 'lower'),\n",
    "    ('R¬≤', 'lightgreen', 'higher'),\n",
    "    ('MAPE (%)', 'gold', 'lower'),\n",
    "    ('Training Time (s)', 'plum', 'lower'),\n",
    "    ('Within ¬±5 Years (%)', 'lightcoral', 'higher')\n",
    "]\n",
    "\n",
    "for idx, (metric, color, better) in enumerate(metrics_to_plot):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    data = detailed_df.sort_values(metric, ascending=(better == 'lower'))\n",
    "    ax.barh(data['Model'], data[metric], color=color, alpha=0.7)\n",
    "    ax.set_xlabel(f\"{metric} ({better.capitalize()} is Better)\", fontweight='bold')\n",
    "    ax.set_title(f'{metric} - GPU Models', fontsize=12, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_metrics_comparison_rapids.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization saved: performance_metrics_comparison_rapids.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30798c7",
   "metadata": {},
   "source": [
    "## üìà Visualization 2: Prediction vs Actual (All Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2421ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction vs actual for all models\n",
    "n_models = len(all_results)\n",
    "n_cols = 3\n",
    "n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6 * n_rows))\n",
    "axes = axes.flatten() if n_models > 1 else [axes]\n",
    "\n",
    "for idx, result in enumerate(all_results):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = result['y_test_pred']\n",
    "    if hasattr(y_pred, 'to_numpy'):\n",
    "        y_pred_np = y_pred.to_numpy()\n",
    "    elif isinstance(y_pred, cp.ndarray):\n",
    "        y_pred_np = cp.asnumpy(y_pred)\n",
    "    else:\n",
    "        y_pred_np = y_pred\n",
    "        \n",
    "    y_test_np = y_test.to_numpy() if hasattr(y_test, 'to_numpy') else cp.asnumpy(y_test)\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(y_test_np, y_pred_np, alpha=0.3, s=10, color='blue')\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test_np.min(), y_pred_np.min())\n",
    "    max_val = max(y_test_np.max(), y_pred_np.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax.set_xlabel('Actual Release Year', fontweight='bold')\n",
    "    ax.set_ylabel('Predicted Release Year', fontweight='bold')\n",
    "    ax.set_title(f\"{result['model_name']}\\nR¬≤ = {result['test_r2']:.4f}, RMSE = {result['test_rmse']:.4f}\", \n",
    "                 fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_vs_actual_all_models_rapids.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization saved: prediction_vs_actual_all_models_rapids.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697665ea",
   "metadata": {},
   "source": [
    "## üìà Visualization 3: Residual Analysis (All Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df09d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots for all models\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6 * n_rows))\n",
    "axes = axes.flatten() if n_models > 1 else [axes]\n",
    "\n",
    "for idx, result in enumerate(all_results):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get predictions and residuals\n",
    "    y_pred = result['y_test_pred']\n",
    "    if hasattr(y_pred, 'to_numpy'):\n",
    "        y_pred_np = y_pred.to_numpy()\n",
    "    elif isinstance(y_pred, cp.ndarray):\n",
    "        y_pred_np = cp.asnumpy(y_pred)\n",
    "    else:\n",
    "        y_pred_np = y_pred\n",
    "        \n",
    "    y_test_np = y_test.to_numpy() if hasattr(y_test, 'to_numpy') else cp.asnumpy(y_test)\n",
    "    residuals = y_test_np - y_pred_np\n",
    "    \n",
    "    # Residual plot\n",
    "    ax.scatter(y_pred_np, residuals, alpha=0.3, s=10, color='purple')\n",
    "    ax.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Predicted Release Year', fontweight='bold')\n",
    "    ax.set_ylabel('Residuals', fontweight='bold')\n",
    "    ax.set_title(f\"Residuals - {result['model_name']}\", fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('residual_analysis_all_models_rapids.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization saved: residual_analysis_all_models_rapids.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da279bee",
   "metadata": {},
   "source": [
    "## üìà Visualization 4: Error Distribution (All Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution for all models\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6 * n_rows))\n",
    "axes = axes.flatten() if n_models > 1 else [axes]\n",
    "\n",
    "for idx, result in enumerate(all_results):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get predictions and errors\n",
    "    y_pred = result['y_test_pred']\n",
    "    if hasattr(y_pred, 'to_numpy'):\n",
    "        y_pred_np = y_pred.to_numpy()\n",
    "    elif isinstance(y_pred, cp.ndarray):\n",
    "        y_pred_np = cp.asnumpy(y_pred)\n",
    "    else:\n",
    "        y_pred_np = y_pred\n",
    "        \n",
    "    y_test_np = y_test.to_numpy() if hasattr(y_test, 'to_numpy') else cp.asnumpy(y_test)\n",
    "    errors = np.abs(y_test_np - y_pred_np)\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(errors, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "    ax.axvline(x=np.mean(errors), color='r', linestyle='--', linewidth=2, label=f'Mean: {np.mean(errors):.2f}')\n",
    "    ax.axvline(x=np.median(errors), color='g', linestyle='--', linewidth=2, label=f'Median: {np.median(errors):.2f}')\n",
    "    ax.set_xlabel('Absolute Error (Years)', fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax.set_title(f\"Error Distribution - {result['model_name']}\", fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('error_distribution_all_models_rapids.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization saved: error_distribution_all_models_rapids.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be71b33",
   "metadata": {},
   "source": [
    "## üèÜ Best Model Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890aff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model based on RMSE\n",
    "best_model_result = min(all_results, key=lambda x: x['test_rmse'])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BEST MODEL DEEP DIVE (Lowest RMSE)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüèÜ Best Model: {best_model_result['model_name']}\")\n",
    "print(f\"\\nüìä Performance Metrics:\")\n",
    "print(f\"   ‚Ä¢ Test RMSE: {best_model_result['test_rmse']:.4f} years\")\n",
    "print(f\"   ‚Ä¢ Test MAE: {best_model_result['test_mae']:.4f} years\")\n",
    "print(f\"   ‚Ä¢ Test R¬≤: {best_model_result['test_r2']:.4f}\")\n",
    "\n",
    "# Find this model in detailed metrics\n",
    "best_model_metrics = detailed_df[detailed_df['Model'] == best_model_result['model_name']].iloc[0]\n",
    "\n",
    "print(f\"   ‚Ä¢ MAPE: {best_model_metrics['MAPE (%)']:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Max Error: {best_model_metrics['Max Error']:.2f} years\")\n",
    "print(f\"\\nüéØ Prediction Accuracy:\")\n",
    "print(f\"   ‚Ä¢ Within ¬±1 year: {best_model_metrics['Within ¬±1 Year (%)']:.2f}% of songs\")\n",
    "print(f\"   ‚Ä¢ Within ¬±5 years: {best_model_metrics['Within ¬±5 Years (%)']:.2f}% of songs\")\n",
    "print(f\"   ‚Ä¢ Within ¬±10 years: {best_model_metrics['Within ¬±10 Years (%)']:.2f}% of songs\")\n",
    "print(f\"\\n‚ö° Training Time: {best_model_result['training_time']:.2f} seconds (GPU)\")\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"   On average, this GPU-trained model predicts song release years\")\n",
    "print(f\"   within ¬±{best_model_result['test_mae']:.2f} years based on audio features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fef7a",
   "metadata": {},
   "source": [
    "## üìä Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2716e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GPU-ACCELERATED MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ EVALUATED {len(all_results)} GPU-TRAINED MODELS\")\n",
    "\n",
    "print(\"\\nüèÜ TOP 3 MODELS BY RMSE:\")\n",
    "top_3 = detailed_df.head(3)\n",
    "for idx, row in top_3.iterrows():\n",
    "    rank = list(top_3.index).index(idx) + 1\n",
    "    print(f\"\\n   {rank}. {row['Model']}\")\n",
    "    print(f\"      ‚Ä¢ RMSE: {row['RMSE']:.4f} years\")\n",
    "    print(f\"      ‚Ä¢ MAE: {row['MAE']:.4f} years\")\n",
    "    print(f\"      ‚Ä¢ R¬≤: {row['R¬≤']:.4f}\")\n",
    "    print(f\"      ‚Ä¢ Training Time: {row['Training Time (s)']:.2f}s (GPU)\")\n",
    "\n",
    "print(\"\\nüöÄ GPU ACCELERATION BENEFITS:\")\n",
    "print(\"   ‚Ä¢ Fast model training (10-100x speedup)\")\n",
    "print(\"   ‚Ä¢ Quick prediction generation\")\n",
    "print(\"   ‚Ä¢ Efficient metric calculation\")\n",
    "print(\"   ‚Ä¢ Scalable to larger datasets\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "print(f\"   1. Best Overall: {ranking_df.iloc[0]['Model']}\")\n",
    "print(f\"      - Balanced performance across all metrics\")\n",
    "print(f\"      - Weighted Score: {ranking_df.iloc[0]['Weighted_Score']:.4f}\")\n",
    "print(f\"\\n   2. Best for Speed: {detailed_df.sort_values('Training Time (s)').iloc[0]['Model']}\")\n",
    "print(f\"      - Fastest training: {detailed_df.sort_values('Training Time (s)').iloc[0]['Training Time (s)']:.2f}s\")\n",
    "print(f\"\\n   3. Best for Accuracy: {detailed_df.sort_values('RMSE').iloc[0]['Model']}\")\n",
    "print(f\"      - Lowest RMSE: {detailed_df.sort_values('RMSE').iloc[0]['RMSE']:.4f} years\")\n",
    "\n",
    "print(\"\\nüìÅ GENERATED FILES:\")\n",
    "print(\"   ‚Ä¢ detailed_model_metrics_rapids.csv\")\n",
    "print(\"   ‚Ä¢ model_ranking_rapids.csv\")\n",
    "print(\"   ‚Ä¢ performance_metrics_comparison_rapids.png\")\n",
    "print(\"   ‚Ä¢ prediction_vs_actual_all_models_rapids.png\")\n",
    "print(\"   ‚Ä¢ residual_analysis_all_models_rapids.png\")\n",
    "print(\"   ‚Ä¢ error_distribution_all_models_rapids.png\")\n",
    "\n",
    "print(\"\\nüéµ FINAL TAKEAWAY:\")\n",
    "print(\"   GPU-accelerated models successfully predict song release years\")\n",
    "print(\"   from audio features with high accuracy and incredible speed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# GPU memory report\n",
    "mempool = cp.get_default_memory_pool()\n",
    "print(f\"\\nüìä GPU Memory Usage:\")\n",
    "print(f\"   Used: {mempool.used_bytes() / 1024**2:.2f} MB\")\n",
    "print(f\"   Total: {mempool.total_bytes() / 1024**2:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
