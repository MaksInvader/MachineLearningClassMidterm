{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "997f1c4c",
   "metadata": {},
   "source": [
    "# üìä Song Release Year Prediction - Model Evaluation & Comparison\n",
    "\n",
    "This notebook provides comprehensive evaluation and comparison of all trained regression models for predicting song release years from audio features.\n",
    "\n",
    "**Dataset Context:** Predicting the year a song was released based on audio characteristics (timbre, pitch, rhythm patterns, etc.)\n",
    "\n",
    "## Evaluation Metrics:\n",
    "- **RMSE** (Root Mean Squared Error) - Lower is better\n",
    "- **MAE** (Mean Absolute Error) - Lower is better\n",
    "- **R¬≤** (Coefficient of Determination) - Higher is better (0 to 1)\n",
    "- **MAPE** (Mean Absolute Percentage Error) - Lower is better\n",
    "- **Residual Analysis**\n",
    "- **Prediction vs Actual Plots**\n",
    "- **Error Distribution Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ccfe3",
   "metadata": {},
   "source": [
    "## Load Saved Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cab033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all model results\n",
    "print(\"Loading saved model results...\\n\")\n",
    "\n",
    "with open('all_model_results.pkl', 'rb') as f:\n",
    "    all_results = pickle.load(f)\n",
    "\n",
    "print(f\"‚úì Loaded results for {len(all_results)} models\")\n",
    "print(f\"\\nModels loaded:\")\n",
    "for i, result in enumerate(all_results, 1):\n",
    "    print(f\"  {i}. {result['model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data for evaluation\n",
    "df = pd.read_csv('cars_scaled_standard.csv')\n",
    "target_col = df.columns[0]\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Split to get test data (same split as training)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Test set size: {len(X_test):,} songs\")\n",
    "print(f\"Audio features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cb32a",
   "metadata": {},
   "source": [
    "## Comprehensive Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02318e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional metrics\n",
    "detailed_metrics = []\n",
    "\n",
    "for result in all_results:\n",
    "    y_pred = result['predictions']\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    detailed_metrics.append({\n",
    "        'Model': result['model_name'],\n",
    "        'Test RMSE': result['test_rmse'],\n",
    "        'Test MAE': result['test_mae'],\n",
    "        'Test R¬≤': result['test_r2'],\n",
    "        'Test MAPE (%)': mape,\n",
    "        'Mean Residual': residuals.mean(),\n",
    "        'Std Residual': residuals.std(),\n",
    "        'Training Time (s)': result['training_time']\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(detailed_metrics)\n",
    "metrics_df = metrics_df.sort_values('Test R¬≤', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION METRICS\")\n",
    "print(\"=\"*120)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Save detailed metrics\n",
    "metrics_df.to_csv('detailed_model_metrics.csv', index=False)\n",
    "print(\"\\n‚úì Saved: detailed_model_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba89efce",
   "metadata": {},
   "source": [
    "## Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Comprehensive Model Performance Comparison', fontsize=16, fontweight='bold', y=1.00)\n",
    "\n",
    "# 1. R¬≤ Score Comparison\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(metrics_df)))\n",
    "bars1 = axes[0, 0].barh(metrics_df['Model'], metrics_df['Test R¬≤'], color=colors, alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Test R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_title('R¬≤ Score (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "axes[0, 0].invert_yaxis()\n",
    "for i, bar in enumerate(bars1):\n",
    "    width = bar.get_width()\n",
    "    axes[0, 0].text(width, bar.get_y() + bar.get_height()/2, f'{width:.4f}', \n",
    "                    ha='left', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 2. RMSE Comparison\n",
    "axes[0, 1].barh(metrics_df['Model'], metrics_df['Test RMSE'], color='coral', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Test RMSE', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_title('RMSE (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "axes[0, 1].invert_yaxis()\n",
    "\n",
    "# 3. MAE Comparison\n",
    "axes[0, 2].barh(metrics_df['Model'], metrics_df['Test MAE'], color='lightgreen', alpha=0.8)\n",
    "axes[0, 2].set_xlabel('Test MAE', fontsize=11, fontweight='bold')\n",
    "axes[0, 2].set_title('MAE (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "axes[0, 2].grid(alpha=0.3, axis='x')\n",
    "axes[0, 2].invert_yaxis()\n",
    "\n",
    "# 4. MAPE Comparison\n",
    "axes[1, 0].barh(metrics_df['Model'], metrics_df['Test MAPE (%)'], color='gold', alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Test MAPE (%)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_title('MAPE (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='x')\n",
    "axes[1, 0].invert_yaxis()\n",
    "\n",
    "# 5. Training Time Comparison\n",
    "axes[1, 1].barh(metrics_df['Model'], metrics_df['Training Time (s)'], color='skyblue', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Training Time (seconds)', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_title('Training Time', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3, axis='x')\n",
    "axes[1, 1].invert_yaxis()\n",
    "\n",
    "# 6. Normalized Performance Score (combining R¬≤, RMSE, MAE)\n",
    "# Normalize metrics to 0-1 scale (higher is better for all)\n",
    "r2_norm = (metrics_df['Test R¬≤'] - metrics_df['Test R¬≤'].min()) / (metrics_df['Test R¬≤'].max() - metrics_df['Test R¬≤'].min())\n",
    "rmse_norm = 1 - (metrics_df['Test RMSE'] - metrics_df['Test RMSE'].min()) / (metrics_df['Test RMSE'].max() - metrics_df['Test RMSE'].min())\n",
    "mae_norm = 1 - (metrics_df['Test MAE'] - metrics_df['Test MAE'].min()) / (metrics_df['Test MAE'].max() - metrics_df['Test MAE'].min())\n",
    "overall_score = (r2_norm + rmse_norm + mae_norm) / 3 * 100\n",
    "\n",
    "axes[1, 2].barh(metrics_df['Model'], overall_score, color='purple', alpha=0.8)\n",
    "axes[1, 2].set_xlabel('Overall Performance Score', fontsize=11, fontweight='bold')\n",
    "axes[1, 2].set_title('Combined Score (0-100)', fontsize=12, fontweight='bold')\n",
    "axes[1, 2].grid(alpha=0.3, axis='x')\n",
    "axes[1, 2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comprehensive_model_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Saved: comprehensive_model_evaluation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40045f00",
   "metadata": {},
   "source": [
    "## Prediction vs Actual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction vs actual plots for top 6 models\n",
    "top_models = all_results[:6]  # Top 6 models\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, result in enumerate(top_models):\n",
    "    y_pred = result['predictions']\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[idx].scatter(y_test, y_pred, alpha=0.3, s=10, color='blue')\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test.min(), y_pred.min())\n",
    "    max_val = max(y_test.max(), y_pred.max())\n",
    "    axes[idx].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    # Add R¬≤ score\n",
    "    r2 = result['test_r2']\n",
    "    axes[idx].text(0.05, 0.95, f'R¬≤ = {r2:.4f}', transform=axes[idx].transAxes,\n",
    "                  fontsize=11, verticalalignment='top', \n",
    "                  bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    axes[idx].set_xlabel('Actual Values', fontsize=10)\n",
    "    axes[idx].set_ylabel('Predicted Values', fontsize=10)\n",
    "    axes[idx].set_title(f\"{result['model_name']}\", fontsize=11, fontweight='bold')\n",
    "    axes[idx].legend(loc='lower right', fontsize=8)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Prediction vs Actual Values - Top 6 Models', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_vs_actual.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Saved: prediction_vs_actual.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b71354",
   "metadata": {},
   "source": [
    "## Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b27fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis for top 6 models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, result in enumerate(top_models):\n",
    "    y_pred = result['predictions']\n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    # Residual plot\n",
    "    axes[idx].scatter(y_pred, residuals, alpha=0.3, s=10, color='purple')\n",
    "    axes[idx].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    \n",
    "    # Add standard deviation bands\n",
    "    std_residual = residuals.std()\n",
    "    axes[idx].axhline(y=std_residual, color='orange', linestyle=':', lw=1.5, alpha=0.7, label=f'¬±1 STD')\n",
    "    axes[idx].axhline(y=-std_residual, color='orange', linestyle=':', lw=1.5, alpha=0.7)\n",
    "    axes[idx].axhline(y=2*std_residual, color='green', linestyle=':', lw=1.5, alpha=0.7, label=f'¬±2 STD')\n",
    "    axes[idx].axhline(y=-2*std_residual, color='green', linestyle=':', lw=1.5, alpha=0.7)\n",
    "    \n",
    "    axes[idx].set_xlabel('Predicted Values', fontsize=10)\n",
    "    axes[idx].set_ylabel('Residuals', fontsize=10)\n",
    "    axes[idx].set_title(f\"{result['model_name']} - Residuals\", fontsize=11, fontweight='bold')\n",
    "    axes[idx].legend(loc='upper right', fontsize=8)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Residual Analysis - Top 6 Models', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('residual_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Saved: residual_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214321f",
   "metadata": {},
   "source": [
    "## Error Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9264717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution for top 6 models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, result in enumerate(top_models):\n",
    "    y_pred = result['predictions']\n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    # Histogram with KDE\n",
    "    axes[idx].hist(residuals, bins=50, alpha=0.6, color='skyblue', edgecolor='black', density=True)\n",
    "    \n",
    "    # Fit normal distribution\n",
    "    mu, std = residuals.mean(), residuals.std()\n",
    "    x = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "    axes[idx].plot(x, stats.norm.pdf(x, mu, std), 'r-', lw=2, label=f'Normal(Œº={mu:.2f}, œÉ={std:.2f})')\n",
    "    \n",
    "    axes[idx].axvline(x=0, color='green', linestyle='--', lw=2, label='Zero Error')\n",
    "    axes[idx].set_xlabel('Residuals', fontsize=10)\n",
    "    axes[idx].set_ylabel('Density', fontsize=10)\n",
    "    axes[idx].set_title(f\"{result['model_name']} - Error Distribution\", fontsize=11, fontweight='bold')\n",
    "    axes[idx].legend(loc='upper right', fontsize=8)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Error Distribution Analysis - Top 6 Models', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('error_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Saved: error_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2bf9e6",
   "metadata": {},
   "source": [
    "## Best Model Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1180f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of the best model\n",
    "best_result = all_results[0]  # Assuming sorted by R¬≤\n",
    "best_model_name = best_result['model_name']\n",
    "y_pred_best = best_result['predictions']\n",
    "residuals_best = y_test - y_pred_best\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Performance Metrics:\")\n",
    "print(f\"   Test RMSE: {best_result['test_rmse']:.4f}\")\n",
    "print(f\"   Test MAE:  {best_result['test_mae']:.4f}\")\n",
    "print(f\"   Test R¬≤:   {best_result['test_r2']:.4f}\")\n",
    "print(f\"   Test MAPE: {mean_absolute_percentage_error(y_test, y_pred_best)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüìà Residual Statistics:\")\n",
    "print(f\"   Mean:     {residuals_best.mean():.4f}\")\n",
    "print(f\"   Std Dev:  {residuals_best.std():.4f}\")\n",
    "print(f\"   Min:      {residuals_best.min():.4f}\")\n",
    "print(f\"   Max:      {residuals_best.max():.4f}\")\n",
    "print(f\"   Q1:       {residuals_best.quantile(0.25):.4f}\")\n",
    "print(f\"   Median:   {residuals_best.median():.4f}\")\n",
    "print(f\"   Q3:       {residuals_best.quantile(0.75):.4f}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Training Time: {best_result['training_time']:.2f} seconds\")\n",
    "\n",
    "# Percentage of predictions within various error ranges\n",
    "abs_errors = np.abs(residuals_best)\n",
    "within_1 = (abs_errors <= 1).sum() / len(abs_errors) * 100\n",
    "within_2 = (abs_errors <= 2).sum() / len(abs_errors) * 100\n",
    "within_5 = (abs_errors <= 5).sum() / len(abs_errors) * 100\n",
    "\n",
    "print(f\"\\nüéØ Song Release Year Prediction Accuracy:\")\n",
    "print(f\"   Within ¬±1 year:  {within_1:.2f}% of songs\")\n",
    "print(f\"   Within ¬±2 years: {within_2:.2f}% of songs\")\n",
    "print(f\"   Within ¬±5 years: {within_5:.2f}% of songs\")\n",
    "print(f\"\\nüí° This shows how accurately we can predict when a song was released based on its audio features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaeb1d7",
   "metadata": {},
   "source": [
    "## Model Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ranking system based on multiple criteria\n",
    "ranking_df = metrics_df.copy()\n",
    "\n",
    "# Rank each metric (1 = best)\n",
    "ranking_df['R¬≤ Rank'] = ranking_df['Test R¬≤'].rank(ascending=False)\n",
    "ranking_df['RMSE Rank'] = ranking_df['Test RMSE'].rank(ascending=True)\n",
    "ranking_df['MAE Rank'] = ranking_df['Test MAE'].rank(ascending=True)\n",
    "ranking_df['MAPE Rank'] = ranking_df['Test MAPE (%)'].rank(ascending=True)\n",
    "ranking_df['Time Rank'] = ranking_df['Training Time (s)'].rank(ascending=True)\n",
    "\n",
    "# Calculate overall rank (lower is better)\n",
    "ranking_df['Overall Rank Score'] = (\n",
    "    ranking_df['R¬≤ Rank'] * 0.3 +  # R¬≤ weighted most\n",
    "    ranking_df['RMSE Rank'] * 0.25 +\n",
    "    ranking_df['MAE Rank'] * 0.25 +\n",
    "    ranking_df['MAPE Rank'] * 0.15 +\n",
    "    ranking_df['Time Rank'] * 0.05  # Time weighted least\n",
    ")\n",
    "\n",
    "ranking_df['Final Rank'] = ranking_df['Overall Rank Score'].rank()\n",
    "ranking_df = ranking_df.sort_values('Final Rank')\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL RANKING (Based on Weighted Metrics)\")\n",
    "print(\"=\"*100)\n",
    "print(ranking_df[['Model', 'Final Rank', 'R¬≤ Rank', 'RMSE Rank', 'MAE Rank', 'Overall Rank Score']].to_string(index=False))\n",
    "\n",
    "# Save ranking\n",
    "ranking_df.to_csv('model_ranking.csv', index=False)\n",
    "print(\"\\n‚úì Saved: model_ranking.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed71bf5",
   "metadata": {},
   "source": [
    "## Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ FINAL MODEL RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top_3 = ranking_df.head(3)\n",
    "\n",
    "print(\"\\nüìç Top 3 Models:\")\n",
    "for idx, row in top_3.iterrows():\n",
    "    print(f\"\\n{int(row['Final Rank'])}. {row['Model']}\")\n",
    "    print(f\"   R¬≤: {row['Test R¬≤']:.4f} | RMSE: {row['Test RMSE']:.4f} | MAE: {row['Test MAE']:.4f}\")\n",
    "    print(f\"   Training Time: {row['Training Time (s)']:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° RECOMMENDATIONS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = top_3.iloc[0]\n",
    "print(f\"\\n‚úÖ PRODUCTION MODEL: {best_model['Model']}\")\n",
    "print(f\"   Reason: Highest R¬≤ score ({best_model['Test R¬≤']:.4f}) with excellent generalization\")\n",
    "\n",
    "fastest_model = metrics_df.loc[metrics_df['Training Time (s)'].idxmin()]\n",
    "print(f\"\\n‚ö° FASTEST MODEL: {fastest_model['Model']}\")\n",
    "print(f\"   Training Time: {fastest_model['Training Time (s)']:.2f}s\")\n",
    "print(f\"   R¬≤: {fastest_model['Test R¬≤']:.4f}\")\n",
    "\n",
    "if fastest_model['Test R¬≤'] > 0.85:\n",
    "    print(f\"   Recommendation: Good choice for real-time applications\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìÅ GENERATED FILES:\")\n",
    "print(\"=\"*80)\n",
    "print(\"   ‚Ä¢ detailed_model_metrics.csv - All metrics\")\n",
    "print(\"   ‚Ä¢ model_ranking.csv - Weighted rankings\")\n",
    "print(\"   ‚Ä¢ comprehensive_model_evaluation.png\")\n",
    "print(\"   ‚Ä¢ prediction_vs_actual.png\")\n",
    "print(\"   ‚Ä¢ residual_analysis.png\")\n",
    "print(\"   ‚Ä¢ error_distribution.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ MODEL EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüéµ Summary: These models predict song release years from audio features\")\n",
    "print(\"   Use the best model to predict when a song was released based on its sound!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
