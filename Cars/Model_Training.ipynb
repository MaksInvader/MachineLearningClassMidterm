{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c3d54b",
   "metadata": {},
   "source": [
    "# ðŸŽµ Song Release Year Prediction - Model Training\n",
    "\n",
    "This notebook trains multiple regression models to predict the year a song was released based on its audio features.\n",
    "\n",
    "**Dataset Context:** Audio features of songs (timbre, pitch, rhythm, etc.) used to predict release year.\n",
    "\n",
    "## Models to Train:\n",
    "1. Linear Regression\n",
    "2. Ridge Regression\n",
    "3. Lasso Regression\n",
    "4. ElasticNet Regression\n",
    "5. Decision Tree Regressor\n",
    "6. Random Forest Regressor\n",
    "7. Gradient Boosting Regressor\n",
    "8. XGBoost Regressor (if available)\n",
    "9. Support Vector Regressor (SVR)\n",
    "10. K-Nearest Neighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73446bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ XGBoost is available\n",
      "\n",
      "âœ“ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Try to import XGBoost\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    xgboost_available = True\n",
    "    print(\"âœ“ XGBoost is available\")\n",
    "except ImportError:\n",
    "    xgboost_available = False\n",
    "    print(\"âš  XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "print(\"\\nâœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f08aae8",
   "metadata": {},
   "source": [
    "## Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ceb705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed song features data...\n",
      "\n",
      "Dataset shape: (515130, 100)\n",
      "Columns (audio features): 100\n",
      "Rows (songs): 515,130\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2001</th>\n",
       "      <th>49.94357</th>\n",
       "      <th>21.47114</th>\n",
       "      <th>73.0775</th>\n",
       "      <th>8.74861</th>\n",
       "      <th>-17.40628</th>\n",
       "      <th>-13.09905</th>\n",
       "      <th>-25.01202</th>\n",
       "      <th>-12.23257</th>\n",
       "      <th>7.83089</th>\n",
       "      <th>...</th>\n",
       "      <th>2.26327</th>\n",
       "      <th>feature_sum</th>\n",
       "      <th>feature_mean</th>\n",
       "      <th>feature_std</th>\n",
       "      <th>feature_min</th>\n",
       "      <th>feature_max</th>\n",
       "      <th>feature_range</th>\n",
       "      <th>num_positive</th>\n",
       "      <th>num_negative</th>\n",
       "      <th>positive_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.897119</td>\n",
       "      <td>0.340776</td>\n",
       "      <td>1.832149</td>\n",
       "      <td>0.772068</td>\n",
       "      <td>-0.169637</td>\n",
       "      <td>-1.232206</td>\n",
       "      <td>0.800067</td>\n",
       "      <td>0.115965</td>\n",
       "      <td>1.505517</td>\n",
       "      <td>...</td>\n",
       "      <td>1.303308</td>\n",
       "      <td>-0.714015</td>\n",
       "      <td>-0.714015</td>\n",
       "      <td>-0.729524</td>\n",
       "      <td>0.377471</td>\n",
       "      <td>-0.422735</td>\n",
       "      <td>-0.437059</td>\n",
       "      <td>0.754446</td>\n",
       "      <td>-0.754444</td>\n",
       "      <td>0.754446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.272365</td>\n",
       "      <td>0.609508</td>\n",
       "      <td>1.401274</td>\n",
       "      <td>0.802770</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>-0.724991</td>\n",
       "      <td>-0.064422</td>\n",
       "      <td>-0.072825</td>\n",
       "      <td>1.235229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096608</td>\n",
       "      <td>-1.247273</td>\n",
       "      <td>-1.247273</td>\n",
       "      <td>-1.150659</td>\n",
       "      <td>0.713968</td>\n",
       "      <td>-1.070976</td>\n",
       "      <td>-1.063895</td>\n",
       "      <td>0.295308</td>\n",
       "      <td>-0.295307</td>\n",
       "      <td>0.295308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.815383</td>\n",
       "      <td>-0.066078</td>\n",
       "      <td>0.821536</td>\n",
       "      <td>0.096232</td>\n",
       "      <td>0.346899</td>\n",
       "      <td>-1.343440</td>\n",
       "      <td>0.533412</td>\n",
       "      <td>-1.127676</td>\n",
       "      <td>-0.020300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.893911</td>\n",
       "      <td>-0.842589</td>\n",
       "      <td>-0.842589</td>\n",
       "      <td>-0.708272</td>\n",
       "      <td>1.065736</td>\n",
       "      <td>-0.349783</td>\n",
       "      <td>-0.496472</td>\n",
       "      <td>-0.622966</td>\n",
       "      <td>0.622968</td>\n",
       "      <td>-0.622966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.274568</td>\n",
       "      <td>0.816740</td>\n",
       "      <td>1.736307</td>\n",
       "      <td>0.479876</td>\n",
       "      <td>-0.422435</td>\n",
       "      <td>-0.585022</td>\n",
       "      <td>-0.724961</td>\n",
       "      <td>-1.000306</td>\n",
       "      <td>0.890946</td>\n",
       "      <td>...</td>\n",
       "      <td>1.396102</td>\n",
       "      <td>-1.263565</td>\n",
       "      <td>-1.263565</td>\n",
       "      <td>-1.229833</td>\n",
       "      <td>0.717071</td>\n",
       "      <td>-1.146780</td>\n",
       "      <td>-1.130709</td>\n",
       "      <td>-1.311672</td>\n",
       "      <td>1.311674</td>\n",
       "      <td>-1.311672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.203308</td>\n",
       "      <td>-0.021764</td>\n",
       "      <td>2.486225</td>\n",
       "      <td>1.388011</td>\n",
       "      <td>-0.864429</td>\n",
       "      <td>-0.765311</td>\n",
       "      <td>1.654661</td>\n",
       "      <td>-0.448923</td>\n",
       "      <td>-0.011715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061789</td>\n",
       "      <td>-1.188286</td>\n",
       "      <td>-1.188286</td>\n",
       "      <td>-1.168220</td>\n",
       "      <td>0.749370</td>\n",
       "      <td>-1.077859</td>\n",
       "      <td>-1.076247</td>\n",
       "      <td>-0.622966</td>\n",
       "      <td>0.622968</td>\n",
       "      <td>-0.622966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2001  49.94357  21.47114   73.0775   8.74861  -17.40628  -13.09905  \\\n",
       "0  2001  0.897119  0.340776  1.832149  0.772068  -0.169637  -1.232206   \n",
       "1  2001  1.272365  0.609508  1.401274  0.802770   0.001628  -0.724991   \n",
       "2  2001  0.815383 -0.066078  0.821536  0.096232   0.346899  -1.343440   \n",
       "3  2001  1.274568  0.816740  1.736307  0.479876  -0.422435  -0.585022   \n",
       "4  2001  1.203308 -0.021764  2.486225  1.388011  -0.864429  -0.765311   \n",
       "\n",
       "   -25.01202  -12.23257   7.83089  ...   2.26327  feature_sum  feature_mean  \\\n",
       "0   0.800067   0.115965  1.505517  ...  1.303308    -0.714015     -0.714015   \n",
       "1  -0.064422  -0.072825  1.235229  ... -0.096608    -1.247273     -1.247273   \n",
       "2   0.533412  -1.127676 -0.020300  ...  0.893911    -0.842589     -0.842589   \n",
       "3  -0.724961  -1.000306  0.890946  ...  1.396102    -1.263565     -1.263565   \n",
       "4   1.654661  -0.448923 -0.011715  ... -0.061789    -1.188286     -1.188286   \n",
       "\n",
       "   feature_std  feature_min  feature_max  feature_range  num_positive  \\\n",
       "0    -0.729524     0.377471    -0.422735      -0.437059      0.754446   \n",
       "1    -1.150659     0.713968    -1.070976      -1.063895      0.295308   \n",
       "2    -0.708272     1.065736    -0.349783      -0.496472     -0.622966   \n",
       "3    -1.229833     0.717071    -1.146780      -1.130709     -1.311672   \n",
       "4    -1.168220     0.749370    -1.077859      -1.076247     -0.622966   \n",
       "\n",
       "   num_negative  positive_ratio  \n",
       "0     -0.754444        0.754446  \n",
       "1     -0.295307        0.295308  \n",
       "2      0.622968       -0.622966  \n",
       "3      1.311674       -1.311672  \n",
       "4      0.622968       -0.622966  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the scaled dataset (StandardScaler version recommended)\n",
    "print(\"Loading processed song features data...\\n\")\n",
    "\n",
    "df = pd.read_csv('cars_scaled_standard.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns (audio features): {df.shape[1]}\")\n",
    "print(f\"Rows (songs): {df.shape[0]:,}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54d3a6",
   "metadata": {},
   "source": [
    "## Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2bd2368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable: 2001\n",
      "Number of features: 99\n",
      "Number of samples: 515,130\n",
      "\n",
      "Target statistics:\n",
      "count    515130.000000\n",
      "mean       1998.396300\n",
      "std          10.931639\n",
      "min        1922.000000\n",
      "25%        1994.000000\n",
      "50%        2002.000000\n",
      "75%        2006.000000\n",
      "max        2011.000000\n",
      "Name: 2001, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "target_col = df.columns[0]  # First column is the target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"Target variable: {target_col}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of samples: {X.shape[0]:,}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc19710a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 412,104 samples (80.0%)\n",
      "Test set size: 103,026 samples (20.0%)\n",
      "\n",
      "Features: 99\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nFeatures: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40de955",
   "metadata": {},
   "source": [
    "## Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d00789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate a regression model.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Train the model\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nðŸ“Š Training Metrics:\")\n",
    "    print(f\"   RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"   MAE:  {train_mae:.4f}\")\n",
    "    print(f\"   RÂ²:   {train_r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Test Metrics:\")\n",
    "    print(f\"   RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"   MAE:  {test_mae:.4f}\")\n",
    "    print(f\"   RÂ²:   {test_r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\nâ±ï¸  Training Time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    if train_r2 - test_r2 > 0.1:\n",
    "        print(f\"\\nâš ï¸  Warning: Possible overfitting detected (RÂ² difference: {train_r2 - test_r2:.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'training_time': training_time,\n",
    "        'predictions': y_test_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96dae4c",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "\n",
    "### 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ba695b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training: Linear Regression\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Training Metrics:\n",
      "   RMSE: 9.4608\n",
      "   MAE:  6.7373\n",
      "   RÂ²:   0.2522\n",
      "\n",
      "ðŸ“Š Test Metrics:\n",
      "   RMSE: 9.4113\n",
      "   MAE:  6.7181\n",
      "   RÂ²:   0.2538\n",
      "\n",
      "â±ï¸  Training Time: 4.17 seconds\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_results = evaluate_model(lr_model, X_train, X_test, y_train, y_test, \"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6415366",
   "metadata": {},
   "source": [
    "### 2. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "327b84ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training: Ridge Regression\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Training Metrics:\n",
      "   RMSE: 9.4609\n",
      "   MAE:  6.7373\n",
      "   RÂ²:   0.2522\n",
      "\n",
      "ðŸ“Š Test Metrics:\n",
      "   RMSE: 9.4114\n",
      "   MAE:  6.7181\n",
      "   RÂ²:   0.2538\n",
      "\n",
      "â±ï¸  Training Time: 0.49 seconds\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression (L2 regularization)\n",
    "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_results = evaluate_model(ridge_model, X_train, X_test, y_train, y_test, \"Ridge Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a1845",
   "metadata": {},
   "source": [
    "### 3. Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b16d750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training: Lasso Regression\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Training Metrics:\n",
      "   RMSE: 9.5266\n",
      "   MAE:  6.7727\n",
      "   RÂ²:   0.2418\n",
      "\n",
      "ðŸ“Š Test Metrics:\n",
      "   RMSE: 9.4818\n",
      "   MAE:  6.7564\n",
      "   RÂ²:   0.2426\n",
      "\n",
      "â±ï¸  Training Time: 5.61 seconds\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression (L1 regularization)\n",
    "lasso_model = Lasso(alpha=0.1, random_state=42, max_iter=10000)\n",
    "lasso_results = evaluate_model(lasso_model, X_train, X_test, y_train, y_test, \"Lasso Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d6e55f",
   "metadata": {},
   "source": [
    "### 4. ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e361894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training: ElasticNet Regression\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Training Metrics:\n",
      "   RMSE: 9.5229\n",
      "   MAE:  6.7950\n",
      "   RÂ²:   0.2424\n",
      "\n",
      "ðŸ“Š Test Metrics:\n",
      "   RMSE: 9.4777\n",
      "   MAE:  6.7786\n",
      "   RÂ²:   0.2433\n",
      "\n",
      "â±ï¸  Training Time: 5.13 seconds\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet Regression (L1 + L2 regularization)\n",
    "elasticnet_model = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=10000)\n",
    "elasticnet_results = evaluate_model(elasticnet_model, X_train, X_test, y_train, y_test, \"ElasticNet Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41821b65",
   "metadata": {},
   "source": [
    "### 5. Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1225c437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training: Decision Tree Regressor\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Training Metrics:\n",
      "   RMSE: 9.4046\n",
      "   MAE:  6.6260\n",
      "   RÂ²:   0.2611\n",
      "\n",
      "ðŸ“Š Test Metrics:\n",
      "   RMSE: 9.6912\n",
      "   MAE:  6.8158\n",
      "   RÂ²:   0.2088\n",
      "\n",
      "â±ï¸  Training Time: 57.85 seconds\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regressor\n",
    "dt_model = DecisionTreeRegressor(max_depth=10, min_samples_split=20, random_state=42)\n",
    "dt_results = evaluate_model(dt_model, X_train, X_test, y_train, y_test, \"Decision Tree Regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b97d02",
   "metadata": {},
   "source": [
    "### 6. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0ca3d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training: Random Forest Regressor\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 300building tree 2 of 300\n",
      "building tree 3 of 300\n",
      "\n",
      "building tree 4 of 300\n",
      "building tree 5 of 300\n",
      "building tree 6 of 300\n",
      "building tree 7 of 300\n",
      "building tree 8 of 300\n",
      "building tree 9 of 300\n",
      "building tree 10 of 300\n",
      "building tree 11 of 300\n",
      "building tree 12 of 300\n",
      "building tree 13 of 300\n",
      "building tree 14 of 300\n",
      "building tree 15 of 300\n",
      "building tree 16 of 300\n",
      "building tree 17 of 300\n",
      "building tree 18 of 300\n",
      "building tree 19 of 300\n",
      "building tree 20 of 300\n",
      "building tree 21 of 300\n",
      "building tree 22 of 300\n",
      "building tree 23 of 300\n",
      "building tree 24 of 300\n",
      "building tree 25 of 300\n",
      "building tree 26 of 300\n",
      "building tree 27 of 300\n",
      "building tree 28 of 300\n",
      "building tree 29 of 300\n",
      "building tree 30 of 300\n",
      "building tree 31 of 300\n",
      "building tree 32 of 300\n",
      "building tree 33 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   26.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 34 of 300\n",
      "building tree 35 of 300\n",
      "building tree 36 of 300\n",
      "building tree 37 of 300\n",
      "building tree 38 of 300\n",
      "building tree 39 of 300\n",
      "building tree 40 of 300\n",
      "building tree 41 of 300\n",
      "building tree 42 of 300building tree 43 of 300\n",
      "\n",
      "building tree 44 of 300\n",
      "building tree 45 of 300\n",
      "building tree 46 of 300\n",
      "building tree 47 of 300\n",
      "building tree 48 of 300\n",
      "building tree 49 of 300\n",
      "building tree 50 of 300\n",
      "building tree 51 of 300\n",
      "building tree 52 of 300\n",
      "building tree 53 of 300\n",
      "building tree 54 of 300\n",
      "building tree 55 of 300\n",
      "building tree 56 of 300\n",
      "building tree 57 of 300building tree 58 of 300\n",
      "\n",
      "building tree 59 of 300\n",
      "building tree 60 of 300\n",
      "building tree 61 of 300\n",
      "building tree 62 of 300\n",
      "building tree 63 of 300\n",
      "building tree 64 of 300\n",
      "building tree 65 of 300\n",
      "building tree 66 of 300\n",
      "building tree 67 of 300\n",
      "building tree 68 of 300\n",
      "building tree 69 of 300\n",
      "building tree 70 of 300\n",
      "building tree 71 of 300\n",
      "building tree 72 of 300\n",
      "building tree 73 of 300\n",
      "building tree 74 of 300\n",
      "building tree 75 of 300\n",
      "building tree 76 of 300\n",
      "building tree 77 of 300\n",
      "building tree 78 of 300\n",
      "building tree 79 of 300\n",
      "building tree 80 of 300\n",
      "building tree 81 of 300\n",
      "building tree 82 of 300\n",
      "building tree 83 of 300\n",
      "building tree 84 of 300\n",
      "building tree 85 of 300\n",
      "building tree 86 of 300\n",
      "building tree 87 of 300\n",
      "building tree 88 of 300\n",
      "building tree 89 of 300\n",
      "building tree 90 of 300\n",
      "building tree 91 of 300\n",
      "building tree 92 of 300\n",
      "building tree 93 of 300\n",
      "building tree 94 of 300\n",
      "building tree 95 of 300\n",
      "building tree 96 of 300\n",
      "building tree 97 of 300\n",
      "building tree 98 of 300\n",
      "building tree 99 of 300\n",
      "building tree 100 of 300\n",
      "building tree 101 of 300\n",
      "building tree 102 of 300\n",
      "building tree 103 of 300\n",
      "building tree 104 of 300\n",
      "building tree 105 of 300\n",
      "building tree 106 of 300\n",
      "building tree 107 of 300\n",
      "building tree 108 of 300\n",
      "building tree 109 of 300\n",
      "building tree 110 of 300\n",
      "building tree 111 of 300\n",
      "building tree 112 of 300\n",
      "building tree 113 of 300\n",
      "building tree 114 of 300\n",
      "building tree 115 of 300\n",
      "building tree 116 of 300\n",
      "building tree 117 of 300\n",
      "building tree 118 of 300\n",
      "building tree 119 of 300\n",
      "building tree 120 of 300\n",
      "building tree 121 of 300\n",
      "building tree 122 of 300\n",
      "building tree 123 of 300\n",
      "building tree 124 of 300\n",
      "building tree 125 of 300\n",
      "building tree 126 of 300\n",
      "building tree 127 of 300\n",
      "building tree 128 of 300\n",
      "building tree 129 of 300\n",
      "building tree 130 of 300\n",
      "building tree 131 of 300\n",
      "building tree 132 of 300\n",
      "building tree 133 of 300\n",
      "building tree 134 of 300\n",
      "building tree 135 of 300\n",
      "building tree 136 of 300\n",
      "building tree 137 of 300\n",
      "building tree 138 of 300\n",
      "building tree 139 of 300\n",
      "building tree 140 of 300\n",
      "building tree 141 of 300\n",
      "building tree 142 of 300\n",
      "building tree 143 of 300\n",
      "building tree 144 of 300\n",
      "building tree 145 of 300\n",
      "building tree 146 of 300\n",
      "building tree 147 of 300\n",
      "building tree 148 of 300\n",
      "building tree 149 of 300\n",
      "building tree 150 of 300\n",
      "building tree 151 of 300\n",
      "building tree 152 of 300\n",
      "building tree 153 of 300\n",
      "building tree 154 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 155 of 300\n",
      "building tree 156 of 300\n",
      "building tree 157 of 300\n",
      "building tree 158 of 300\n",
      "building tree 159 of 300\n",
      "building tree 160 of 300\n",
      "building tree 161 of 300\n",
      "building tree 162 of 300\n",
      "building tree 163 of 300\n",
      "building tree 164 of 300\n",
      "building tree 165 of 300\n",
      "building tree 166 of 300\n",
      "building tree 167 of 300\n",
      "building tree 168 of 300\n",
      "building tree 169 of 300\n",
      "building tree 170 of 300\n",
      "building tree 171 of 300\n",
      "building tree 172 of 300\n",
      "building tree 173 of 300\n",
      "building tree 174 of 300\n",
      "building tree 175 of 300\n",
      "building tree 176 of 300\n",
      "building tree 177 of 300\n",
      "building tree 178 of 300\n",
      "building tree 179 of 300\n",
      "building tree 180 of 300\n",
      "building tree 181 of 300\n",
      "building tree 182 of 300\n",
      "building tree 183 of 300\n",
      "building tree 184 of 300\n",
      "building tree 185 of 300\n",
      "building tree 186 of 300\n",
      "building tree 187 of 300\n",
      "building tree 188 of 300\n",
      "building tree 189 of 300\n",
      "building tree 190 of 300\n",
      "building tree 191 of 300\n",
      "building tree 192 of 300\n",
      "building tree 193 of 300\n",
      "building tree 194 of 300\n",
      "building tree 195 of 300\n",
      "building tree 196 of 300\n",
      "building tree 197 of 300\n",
      "building tree 198 of 300\n",
      "building tree 199 of 300\n",
      "building tree 200 of 300\n",
      "building tree 201 of 300\n",
      "building tree 202 of 300\n",
      "building tree 203 of 300\n",
      "building tree 204 of 300\n",
      "building tree 205 of 300\n",
      "building tree 206 of 300\n",
      "building tree 207 of 300\n",
      "building tree 208 of 300\n",
      "building tree 209 of 300\n",
      "building tree 210 of 300\n",
      "building tree 211 of 300\n",
      "building tree 212 of 300\n",
      "building tree 213 of 300\n",
      "building tree 214 of 300\n",
      "building tree 215 of 300\n",
      "building tree 216 of 300\n",
      "building tree 217 of 300\n",
      "building tree 218 of 300\n",
      "building tree 219 of 300\n",
      "building tree 220 of 300\n",
      "building tree 221 of 300\n",
      "building tree 222 of 300\n",
      "building tree 223 of 300\n",
      "building tree 224 of 300\n",
      "building tree 225 of 300\n",
      "building tree 226 of 300\n",
      "building tree 227 of 300\n",
      "building tree 228 of 300\n",
      "building tree 229 of 300\n",
      "building tree 230 of 300\n",
      "building tree 231 of 300\n",
      "building tree 232 of 300\n",
      "building tree 233 of 300\n",
      "building tree 234 of 300\n",
      "building tree 235 of 300\n",
      "building tree 236 of 300\n",
      "building tree 237 of 300\n",
      "building tree 238 of 300\n",
      "building tree 239 of 300\n",
      "building tree 240 of 300\n",
      "building tree 241 of 300\n",
      "building tree 242 of 300\n",
      "building tree 243 of 300\n",
      "building tree 244 of 300\n",
      "building tree 245 of 300\n",
      "building tree 246 of 300\n",
      "building tree 247 of 300\n",
      "building tree 248 of 300\n",
      "building tree 249 of 300\n",
      "building tree 250 of 300\n",
      "building tree 251 of 300\n",
      "building tree 252 of 300\n",
      "building tree 253 of 300\n",
      "building tree 254 of 300\n",
      "building tree 255 of 300\n",
      "building tree 256 of 300\n",
      "building tree 257 of 300\n",
      "building tree 258 of 300\n",
      "building tree 259 of 300\n",
      "building tree 260 of 300\n",
      "building tree 261 of 300\n",
      "building tree 262 of 300\n",
      "building tree 263 of 300\n",
      "building tree 264 of 300\n",
      "building tree 265 of 300\n",
      "building tree 266 of 300\n",
      "building tree 267 of 300\n",
      "building tree 268 of 300\n",
      "building tree 269 of 300\n",
      "building tree 270 of 300\n",
      "building tree 271 of 300\n",
      "building tree 272 of 300\n",
      "building tree 273 of 300\n",
      "building tree 274 of 300\n",
      "building tree 275 of 300\n",
      "building tree 276 of 300\n",
      "building tree 277 of 300\n",
      "building tree 278 of 300\n",
      "building tree 279 of 300\n",
      "building tree 280 of 300\n",
      "building tree 281 of 300\n",
      "building tree 282 of 300\n",
      "building tree 283 of 300\n",
      "building tree 284 of 300\n",
      "building tree 285 of 300\n",
      "building tree 286 of 300\n",
      "building tree 287 of 300\n",
      "building tree 288 of 300\n",
      "building tree 289 of 300\n",
      "building tree 290 of 300\n",
      "building tree 291 of 300\n",
      "building tree 292 of 300\n",
      "building tree 293 of 300\n",
      "building tree 294 of 300\n",
      "building tree 295 of 300\n",
      "building tree 296 of 300\n",
      "building tree 297 of 300\n",
      "building tree 298 of 300\n",
      "building tree 299 of 300\n",
      "building tree 300 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  4.3min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=8)]: Done 300 out of 300 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Training Metrics:\n",
      "   RMSE: 9.5431\n",
      "   MAE:  6.9022\n",
      "   RÂ²:   0.2392\n",
      "\n",
      "ðŸ“Š Test Metrics:\n",
      "   RMSE: 9.6766\n",
      "   MAE:  7.0120\n",
      "   RÂ²:   0.2112\n",
      "\n",
      "â±ï¸  Training Time: 257.08 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 300 out of 300 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=300,          # more trees â†’ smoother model\n",
    "    max_depth=10,              # shallower â†’ less overfitting\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=5,        # THIS helps a lot with variance\n",
    "    max_features=\"sqrt\",       # reduce correlation between trees\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "rf_results = evaluate_model(rf_model, X_train, X_test, y_train, y_test, \"Random Forest Regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94909c6",
   "metadata": {},
   "source": [
    "### 7. Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2457e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training: Gradient Boosting Regressor\n",
      "================================================================================\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         115.9387           52.00m\n",
      "         2         112.7713           54.92m\n",
      "         3         110.1467           53.16m\n",
      "         4         107.8345           52.03m\n",
      "         5         105.8781           51.09m\n",
      "         6         104.1050           50.25m\n",
      "         7         102.5790           49.72m\n",
      "         8         101.1755           49.00m\n",
      "         9          99.9943           48.39m\n",
      "        10          98.8852           47.81m\n",
      "        20          92.0295           44.95m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Gradient Boosting Regressor\u001b[39;00m\n\u001b[0;32m      2\u001b[0m gb_model \u001b[38;5;241m=\u001b[39m GradientBoostingRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[0;32m      3\u001b[0m                                       random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m gb_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgb_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGradient Boosting Regressor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, X_train, X_test, y_train, y_test, model_name)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m training_time \u001b[38;5;241m=\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:787\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:883\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    876\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(\n\u001b[0;32m    877\u001b[0m             y_true\u001b[38;5;241m=\u001b[39my_oob_masked,\n\u001b[0;32m    878\u001b[0m             raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    879\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight_oob_masked,\n\u001b[0;32m    880\u001b[0m         )\n\u001b[0;32m    882\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 883\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:489\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    486\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    488\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 489\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    494\u001b[0m X_for_tree_update \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1404\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Regressor\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, \n",
    "                                      random_state=42, verbose=1)\n",
    "gb_results = evaluate_model(gb_model, X_train, X_test, y_train, y_test, \"Gradient Boosting Regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60244b6",
   "metadata": {},
   "source": [
    "### 8. XGBoost Regressor (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21cab08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training: XGBoost Regressor\n",
      "================================================================================\n",
      "[00:28:28] INFO: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\data\\iterative_dmatrix.cc:56: Finished constructing the `IterativeDMatrix`: (412104, 99, 40798296).\n",
      "[00:28:28] INFO: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\data\\iterative_dmatrix.cc:56: Finished constructing the `IterativeDMatrix`: (412104, 99, 40798296).\n",
      "\n",
      "ðŸ“Š Training Metrics:\n",
      "   RMSE: 8.9885\n",
      "   MAE:  6.3064\n",
      "   RÂ²:   0.3250\n",
      "\n",
      "ðŸ“Š Test Metrics:\n",
      "   RMSE: 9.0538\n",
      "   MAE:  6.3614\n",
      "   RÂ²:   0.3095\n",
      "\n",
      "â±ï¸  Training Time: 10.15 seconds\n",
      "\n",
      "ðŸ“Š Training Metrics:\n",
      "   RMSE: 8.9885\n",
      "   MAE:  6.3064\n",
      "   RÂ²:   0.3250\n",
      "\n",
      "ðŸ“Š Test Metrics:\n",
      "   RMSE: 9.0538\n",
      "   MAE:  6.3614\n",
      "   RÂ²:   0.3095\n",
      "\n",
      "â±ï¸  Training Time: 10.15 seconds\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Regressor\n",
    "if xgboost_available:\n",
    "    xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, \n",
    "                             random_state=42, n_jobs=-1, verbosity=2)\n",
    "    xgb_results = evaluate_model(xgb_model, X_train, X_test, y_train, y_test, \"XGBoost Regressor\")\n",
    "else:\n",
    "    print(\"\\nSkipping XGBoost (not installed)\")\n",
    "    xgb_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d9345",
   "metadata": {},
   "source": [
    "### 9. Support Vector Regressor (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Regressor (using subset for faster training)\n",
    "print(\"\\nâš ï¸  Note: Training SVR on a subset (50,000 samples) due to computational cost...\")\n",
    "\n",
    "# Use subset for SVR\n",
    "subset_size = min(50000, len(X_train))\n",
    "X_train_subset = X_train.iloc[:subset_size]\n",
    "y_train_subset = y_train.iloc[:subset_size]\n",
    "\n",
    "svr_model = SVR(kernel='rbf', C=10, gamma='scale', verbose=True)\n",
    "svr_results = evaluate_model(svr_model, X_train_subset, X_test, y_train_subset, y_test, \n",
    "                             \"Support Vector Regressor (SVR)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7697fbef",
   "metadata": {},
   "source": [
    "### 10. K-Nearest Neighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b2d4475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training: K-Nearest Neighbors Regressor\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# K-Nearest Neighbors Regressor\u001b[39;00m\n\u001b[0;32m      2\u001b[0m knn_model \u001b[38;5;241m=\u001b[39m KNeighborsRegressor(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m knn_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mK-Nearest Neighbors Regressor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, X_train, X_test, y_train, y_test, model_name)\u001b[0m\n\u001b[0;32m     15\u001b[0m training_time \u001b[38;5;241m=\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\neighbors\\_regression.py:243\u001b[0m, in \u001b[0;36mKNeighborsRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict the target for the provided data.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m    Target values.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\neighbors\\_base.py:869\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    862\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[0;32m    865\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[0;32m    866\u001b[0m     )\n\u001b[0;32m    867\u001b[0m )\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[1;32m--> 869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mArgKmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[0;32m    881\u001b[0m ):\n\u001b[0;32m    882\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[0;32m    883\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[0;32m    884\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:281\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[1;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03mreturns.\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArgKmin64\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m    294\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    295\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    301\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[0;32m    302\u001b[0m     )\n",
      "File \u001b[1;32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:59\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\threadpoolctl.py:592\u001b[0m, in \u001b[0;36m_ThreadpoolLimiter.__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m--> 592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_original_limits()\n\u001b[0;32m    595\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m, controller, \u001b[38;5;241m*\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors Regressor\n",
    "knn_model = KNeighborsRegressor(n_neighbors=5, n_jobs=-1)\n",
    "knn_results = evaluate_model(knn_model, X_train, X_test, y_train, y_test, \"K-Nearest Neighbors Regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d83e6",
   "metadata": {},
   "source": [
    "## Collect All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cdbd3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gb_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Collect all results (only those that were actually trained)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m all_results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     lr_results,\n\u001b[0;32m      4\u001b[0m     ridge_results,\n\u001b[0;32m      5\u001b[0m     lasso_results,\n\u001b[0;32m      6\u001b[0m     elasticnet_results,\n\u001b[0;32m      7\u001b[0m     dt_results,\n\u001b[0;32m      8\u001b[0m     rf_results,\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mgb_results\u001b[49m,\n\u001b[0;32m     10\u001b[0m     knn_results,\n\u001b[0;32m     11\u001b[0m     svr_results\n\u001b[0;32m     12\u001b[0m ]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xgb_results:\n\u001b[0;32m     15\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mappend(xgb_results)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gb_results' is not defined"
     ]
    }
   ],
   "source": [
    "# Collect all results (only those that were actually trained)\n",
    "all_results = []\n",
    "\n",
    "# Check each model result and only add if it exists\n",
    "model_results_to_check = [\n",
    "    ('lr_results', 'lr_results'),\n",
    "    ('ridge_results', 'ridge_results'),\n",
    "    ('lasso_results', 'lasso_results'),\n",
    "    ('elasticnet_results', 'elasticnet_results'),\n",
    "    ('dt_results', 'dt_results'),\n",
    "    ('rf_results', 'rf_results'),\n",
    "    ('gb_results', 'gb_results'),\n",
    "    ('xgb_results', 'xgb_results'),\n",
    "    ('svr_results', 'svr_results'),\n",
    "    ('knn_results', 'knn_results')\n",
    "]\n",
    "\n",
    "for var_name, result_name in model_results_to_check:\n",
    "    try:\n",
    "        result = eval(result_name)\n",
    "        if result is not None:\n",
    "            all_results.append(result)\n",
    "            print(f\"âœ“ Added {result['model_name']} to comparison\")\n",
    "    except NameError:\n",
    "        print(f\"âŠ˜ Skipped {var_name} (not trained yet)\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total models to compare: {len(all_results)}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame([{\n",
    "    'Model': r['model_name'],\n",
    "    'Train RMSE': r['train_rmse'],\n",
    "    'Test RMSE': r['test_rmse'],\n",
    "    'Train MAE': r['train_mae'],\n",
    "    'Test MAE': r['test_mae'],\n",
    "    'Train RÂ²': r['train_r2'],\n",
    "    'Test RÂ²': r['test_r2'],\n",
    "    'Training Time (s)': r['training_time']\n",
    "} for r in all_results])\n",
    "\n",
    "# Sort by Test RÂ² (descending)\n",
    "results_df = results_df.sort_values('Test RÂ²', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_test_r2 = results_df.iloc[0]['Test RÂ²']\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name} (Test RÂ² = {best_test_r2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde5d07",
   "metadata": {},
   "source": [
    "## Visualize Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8215b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Test RÂ² Comparison\n",
    "axes[0, 0].barh(results_df['Model'], results_df['Test RÂ²'], color='skyblue', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Test RÂ²', fontsize=12)\n",
    "axes[0, 0].set_title('Model Comparison - Test RÂ² Score', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "axes[0, 0].invert_yaxis()\n",
    "\n",
    "# 2. Test RMSE Comparison\n",
    "axes[0, 1].barh(results_df['Model'], results_df['Test RMSE'], color='coral', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Test RMSE', fontsize=12)\n",
    "axes[0, 1].set_title('Model Comparison - Test RMSE', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "axes[0, 1].invert_yaxis()\n",
    "\n",
    "# 3. Test MAE Comparison\n",
    "axes[1, 0].barh(results_df['Model'], results_df['Test MAE'], color='lightgreen', alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Test MAE', fontsize=12)\n",
    "axes[1, 0].set_title('Model Comparison - Test MAE', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='x')\n",
    "axes[1, 0].invert_yaxis()\n",
    "\n",
    "# 4. Training Time Comparison\n",
    "axes[1, 1].barh(results_df['Model'], results_df['Training Time (s)'], color='gold', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Training Time (seconds)', fontsize=12)\n",
    "axes[1, 1].set_title('Model Comparison - Training Time', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3, axis='x')\n",
    "axes[1, 1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Comparison chart saved as 'model_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a475f0b",
   "metadata": {},
   "source": [
    "## Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained models (only those that were actually trained)\n",
    "print(\"Saving trained models...\\n\")\n",
    "\n",
    "# Build dictionary mapping model names to their results\n",
    "model_mapping = [\n",
    "    ('linear_regression', 'lr_results'),\n",
    "    ('ridge_regression', 'ridge_results'),\n",
    "    ('lasso_regression', 'lasso_results'),\n",
    "    ('elasticnet_regression', 'elasticnet_results'),\n",
    "    ('decision_tree', 'dt_results'),\n",
    "    ('random_forest', 'rf_results'),\n",
    "    ('gradient_boosting', 'gb_results'),\n",
    "    ('xgboost', 'xgb_results'),\n",
    "    ('svr', 'svr_results'),\n",
    "    ('knn', 'knn_results')\n",
    "]\n",
    "\n",
    "models_to_save = {}\n",
    "\n",
    "# Only add models that were actually trained\n",
    "for model_name, result_var in model_mapping:\n",
    "    try:\n",
    "        result = eval(result_var)\n",
    "        if result is not None:\n",
    "            models_to_save[model_name] = result\n",
    "    except NameError:\n",
    "        # Variable doesn't exist, skip it\n",
    "        pass\n",
    "\n",
    "# Save each trained model\n",
    "for model_name, results in models_to_save.items():\n",
    "    with open(f'model_{model_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(results['model'], f)\n",
    "    print(f\"âœ“ Saved: model_{model_name}.pkl\")\n",
    "\n",
    "# Save results DataFrame\n",
    "results_df.to_csv('model_results.csv', index=False)\n",
    "print(f\"\\nâœ“ Saved: model_results.csv\")\n",
    "\n",
    "# Save all results including predictions\n",
    "with open('all_model_results.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "print(f\"âœ“ Saved: all_model_results.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“ Generated Files:\")\n",
    "print(f\"   â€¢ {len(models_to_save)} model files (*.pkl)\")\n",
    "print(f\"   â€¢ model_results.csv - Summary metrics\")\n",
    "print(f\"   â€¢ all_model_results.pkl - Complete results with predictions\")\n",
    "print(f\"   â€¢ model_comparison.png - Visualization\")\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n",
    "print(f\"   Test RÂ²: {best_test_r2:.4f}\")\n",
    "print(f\"\\nðŸ’¡ Note: This model predicts song release year from audio features!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda_env)",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
