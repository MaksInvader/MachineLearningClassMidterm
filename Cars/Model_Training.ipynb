{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c3d54b",
   "metadata": {},
   "source": [
    "# üéµ Song Release Year Prediction - Model Training\n",
    "\n",
    "This notebook trains multiple regression models to predict the year a song was released based on its audio features.\n",
    "\n",
    "**Dataset Context:** Audio features of songs (timbre, pitch, rhythm, etc.) used to predict release year.\n",
    "\n",
    "## Models to Train:\n",
    "1. Linear Regression\n",
    "2. Ridge Regression\n",
    "3. Lasso Regression\n",
    "4. ElasticNet Regression\n",
    "5. Decision Tree Regressor\n",
    "6. Random Forest Regressor\n",
    "7. Gradient Boosting Regressor\n",
    "8. XGBoost Regressor (if available)\n",
    "9. Support Vector Regressor (SVR)\n",
    "10. K-Nearest Neighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73446bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì XGBoost is available\n",
      "\n",
      "‚úì All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Try to import XGBoost\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    xgboost_available = True\n",
    "    print(\"‚úì XGBoost is available\")\n",
    "except ImportError:\n",
    "    xgboost_available = False\n",
    "    print(\"‚ö† XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "print(\"\\n‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f08aae8",
   "metadata": {},
   "source": [
    "## Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ceb705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed song features data...\n",
      "\n",
      "Dataset shape: (515130, 100)\n",
      "Columns (audio features): 100\n",
      "Rows (songs): 515,130\n",
      "\n",
      "First few rows:\n",
      "Dataset shape: (515130, 100)\n",
      "Columns (audio features): 100\n",
      "Rows (songs): 515,130\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2001</th>\n",
       "      <th>49.94357</th>\n",
       "      <th>21.47114</th>\n",
       "      <th>73.0775</th>\n",
       "      <th>8.74861</th>\n",
       "      <th>-17.40628</th>\n",
       "      <th>-13.09905</th>\n",
       "      <th>-25.01202</th>\n",
       "      <th>-12.23257</th>\n",
       "      <th>7.83089</th>\n",
       "      <th>...</th>\n",
       "      <th>2.26327</th>\n",
       "      <th>feature_sum</th>\n",
       "      <th>feature_mean</th>\n",
       "      <th>feature_std</th>\n",
       "      <th>feature_min</th>\n",
       "      <th>feature_max</th>\n",
       "      <th>feature_range</th>\n",
       "      <th>num_positive</th>\n",
       "      <th>num_negative</th>\n",
       "      <th>positive_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.897119</td>\n",
       "      <td>0.340776</td>\n",
       "      <td>1.832149</td>\n",
       "      <td>0.772068</td>\n",
       "      <td>-0.169637</td>\n",
       "      <td>-1.232206</td>\n",
       "      <td>0.800067</td>\n",
       "      <td>0.115965</td>\n",
       "      <td>1.505517</td>\n",
       "      <td>...</td>\n",
       "      <td>1.303308</td>\n",
       "      <td>-0.714015</td>\n",
       "      <td>-0.714015</td>\n",
       "      <td>-0.729524</td>\n",
       "      <td>0.377471</td>\n",
       "      <td>-0.422735</td>\n",
       "      <td>-0.437059</td>\n",
       "      <td>0.754446</td>\n",
       "      <td>-0.754444</td>\n",
       "      <td>0.754446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.272365</td>\n",
       "      <td>0.609508</td>\n",
       "      <td>1.401274</td>\n",
       "      <td>0.802770</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>-0.724991</td>\n",
       "      <td>-0.064422</td>\n",
       "      <td>-0.072825</td>\n",
       "      <td>1.235229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096608</td>\n",
       "      <td>-1.247273</td>\n",
       "      <td>-1.247273</td>\n",
       "      <td>-1.150659</td>\n",
       "      <td>0.713968</td>\n",
       "      <td>-1.070976</td>\n",
       "      <td>-1.063895</td>\n",
       "      <td>0.295308</td>\n",
       "      <td>-0.295307</td>\n",
       "      <td>0.295308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.815383</td>\n",
       "      <td>-0.066078</td>\n",
       "      <td>0.821536</td>\n",
       "      <td>0.096232</td>\n",
       "      <td>0.346899</td>\n",
       "      <td>-1.343440</td>\n",
       "      <td>0.533412</td>\n",
       "      <td>-1.127676</td>\n",
       "      <td>-0.020300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.893911</td>\n",
       "      <td>-0.842589</td>\n",
       "      <td>-0.842589</td>\n",
       "      <td>-0.708272</td>\n",
       "      <td>1.065736</td>\n",
       "      <td>-0.349783</td>\n",
       "      <td>-0.496472</td>\n",
       "      <td>-0.622966</td>\n",
       "      <td>0.622968</td>\n",
       "      <td>-0.622966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.274568</td>\n",
       "      <td>0.816740</td>\n",
       "      <td>1.736307</td>\n",
       "      <td>0.479876</td>\n",
       "      <td>-0.422435</td>\n",
       "      <td>-0.585022</td>\n",
       "      <td>-0.724961</td>\n",
       "      <td>-1.000306</td>\n",
       "      <td>0.890946</td>\n",
       "      <td>...</td>\n",
       "      <td>1.396102</td>\n",
       "      <td>-1.263565</td>\n",
       "      <td>-1.263565</td>\n",
       "      <td>-1.229833</td>\n",
       "      <td>0.717071</td>\n",
       "      <td>-1.146780</td>\n",
       "      <td>-1.130709</td>\n",
       "      <td>-1.311672</td>\n",
       "      <td>1.311674</td>\n",
       "      <td>-1.311672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.203308</td>\n",
       "      <td>-0.021764</td>\n",
       "      <td>2.486225</td>\n",
       "      <td>1.388011</td>\n",
       "      <td>-0.864429</td>\n",
       "      <td>-0.765311</td>\n",
       "      <td>1.654661</td>\n",
       "      <td>-0.448923</td>\n",
       "      <td>-0.011715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061789</td>\n",
       "      <td>-1.188286</td>\n",
       "      <td>-1.188286</td>\n",
       "      <td>-1.168220</td>\n",
       "      <td>0.749370</td>\n",
       "      <td>-1.077859</td>\n",
       "      <td>-1.076247</td>\n",
       "      <td>-0.622966</td>\n",
       "      <td>0.622968</td>\n",
       "      <td>-0.622966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2001  49.94357  21.47114   73.0775   8.74861  -17.40628  -13.09905  \\\n",
       "0  2001  0.897119  0.340776  1.832149  0.772068  -0.169637  -1.232206   \n",
       "1  2001  1.272365  0.609508  1.401274  0.802770   0.001628  -0.724991   \n",
       "2  2001  0.815383 -0.066078  0.821536  0.096232   0.346899  -1.343440   \n",
       "3  2001  1.274568  0.816740  1.736307  0.479876  -0.422435  -0.585022   \n",
       "4  2001  1.203308 -0.021764  2.486225  1.388011  -0.864429  -0.765311   \n",
       "\n",
       "   -25.01202  -12.23257   7.83089  ...   2.26327  feature_sum  feature_mean  \\\n",
       "0   0.800067   0.115965  1.505517  ...  1.303308    -0.714015     -0.714015   \n",
       "1  -0.064422  -0.072825  1.235229  ... -0.096608    -1.247273     -1.247273   \n",
       "2   0.533412  -1.127676 -0.020300  ...  0.893911    -0.842589     -0.842589   \n",
       "3  -0.724961  -1.000306  0.890946  ...  1.396102    -1.263565     -1.263565   \n",
       "4   1.654661  -0.448923 -0.011715  ... -0.061789    -1.188286     -1.188286   \n",
       "\n",
       "   feature_std  feature_min  feature_max  feature_range  num_positive  \\\n",
       "0    -0.729524     0.377471    -0.422735      -0.437059      0.754446   \n",
       "1    -1.150659     0.713968    -1.070976      -1.063895      0.295308   \n",
       "2    -0.708272     1.065736    -0.349783      -0.496472     -0.622966   \n",
       "3    -1.229833     0.717071    -1.146780      -1.130709     -1.311672   \n",
       "4    -1.168220     0.749370    -1.077859      -1.076247     -0.622966   \n",
       "\n",
       "   num_negative  positive_ratio  \n",
       "0     -0.754444        0.754446  \n",
       "1     -0.295307        0.295308  \n",
       "2      0.622968       -0.622966  \n",
       "3      1.311674       -1.311672  \n",
       "4      0.622968       -0.622966  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the scaled dataset (StandardScaler version recommended)\n",
    "print(\"Loading processed song features data...\\n\")\n",
    "\n",
    "df = pd.read_csv('cars_scaled_standard.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns (audio features): {df.shape[1]}\")\n",
    "print(f\"Rows (songs): {df.shape[0]:,}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54d3a6",
   "metadata": {},
   "source": [
    "## Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2bd2368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable: 2001\n",
      "Number of features: 99\n",
      "Number of samples: 515,130\n",
      "\n",
      "Target statistics:\n",
      "count    515130.000000\n",
      "mean       1998.396300\n",
      "std          10.931639\n",
      "min        1922.000000\n",
      "25%        1994.000000\n",
      "50%        2002.000000\n",
      "75%        2006.000000\n",
      "max        2011.000000\n",
      "Name: 2001, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "target_col = df.columns[0]  # First column is the target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"Target variable: {target_col}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of samples: {X.shape[0]:,}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc19710a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 412,104 samples (80.0%)\n",
      "Test set size: 103,026 samples (20.0%)\n",
      "\n",
      "Features: 99\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nFeatures: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40de955",
   "metadata": {},
   "source": [
    "## Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d00789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate a regression model.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Train the model\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüìä Training Metrics:\")\n",
    "    print(f\"   RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"   MAE:  {train_mae:.4f}\")\n",
    "    print(f\"   R¬≤:   {train_r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Test Metrics:\")\n",
    "    print(f\"   RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"   MAE:  {test_mae:.4f}\")\n",
    "    print(f\"   R¬≤:   {test_r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Training Time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    if train_r2 - test_r2 > 0.1:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: Possible overfitting detected (R¬≤ difference: {train_r2 - test_r2:.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'training_time': training_time,\n",
    "        'predictions': y_test_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96dae4c",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "\n",
    "### 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ba695b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training: Linear Regression\n",
      "================================================================================\n",
      "\n",
      "üìä Training Metrics:\n",
      "   RMSE: 9.4608\n",
      "   MAE:  6.7373\n",
      "   R¬≤:   0.2522\n",
      "\n",
      "üìä Test Metrics:\n",
      "   RMSE: 9.4113\n",
      "   MAE:  6.7181\n",
      "   R¬≤:   0.2538\n",
      "\n",
      "‚è±Ô∏è  Training Time: 5.65 seconds\n",
      "\n",
      "üìä Training Metrics:\n",
      "   RMSE: 9.4608\n",
      "   MAE:  6.7373\n",
      "   R¬≤:   0.2522\n",
      "\n",
      "üìä Test Metrics:\n",
      "   RMSE: 9.4113\n",
      "   MAE:  6.7181\n",
      "   R¬≤:   0.2538\n",
      "\n",
      "‚è±Ô∏è  Training Time: 5.65 seconds\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_results = evaluate_model(lr_model, X_train, X_test, y_train, y_test, \"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6415366",
   "metadata": {},
   "source": [
    "### 2. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "327b84ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training: Ridge Regression\n",
      "================================================================================\n",
      "\n",
      "üìä Training Metrics:\n",
      "   RMSE: 9.4609\n",
      "   MAE:  6.7373\n",
      "   R¬≤:   0.2522\n",
      "\n",
      "üìä Test Metrics:\n",
      "   RMSE: 9.4114\n",
      "   MAE:  6.7181\n",
      "   R¬≤:   0.2538\n",
      "\n",
      "‚è±Ô∏è  Training Time: 0.57 seconds\n",
      "\n",
      "üìä Training Metrics:\n",
      "   RMSE: 9.4609\n",
      "   MAE:  6.7373\n",
      "   R¬≤:   0.2522\n",
      "\n",
      "üìä Test Metrics:\n",
      "   RMSE: 9.4114\n",
      "   MAE:  6.7181\n",
      "   R¬≤:   0.2538\n",
      "\n",
      "‚è±Ô∏è  Training Time: 0.57 seconds\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression (L2 regularization)\n",
    "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_results = evaluate_model(ridge_model, X_train, X_test, y_train, y_test, \"Ridge Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a1845",
   "metadata": {},
   "source": [
    "### 3. Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16d750a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Lasso.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Lasso Regression (L1 regularization)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m lasso_model \u001b[38;5;241m=\u001b[39m \u001b[43mLasso\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m lasso_results \u001b[38;5;241m=\u001b[39m evaluate_model(lasso_model, X_train, X_test, y_train, y_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLasso Regression\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Lasso.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "# Lasso Regression (L1 regularization)\n",
    "lasso_model = Lasso(alpha=0.1, random_state=42, max_iter=10000)\n",
    "lasso_results = evaluate_model(lasso_model, X_train, X_test, y_train, y_test, \"Lasso Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d6e55f",
   "metadata": {},
   "source": [
    "### 4. ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e361894",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ElasticNet.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ElasticNet Regression (L1 + L2 regularization)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m elasticnet_model \u001b[38;5;241m=\u001b[39m \u001b[43mElasticNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m elasticnet_results \u001b[38;5;241m=\u001b[39m evaluate_model(elasticnet_model, X_train, X_test, y_train, y_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElasticNet Regression\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: ElasticNet.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "# ElasticNet Regression (L1 + L2 regularization)\n",
    "elasticnet_model = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=10000)\n",
    "elasticnet_results = evaluate_model(elasticnet_model, X_train, X_test, y_train, y_test, \"ElasticNet Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41821b65",
   "metadata": {},
   "source": [
    "### 5. Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1225c437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training: Decision Tree Regressor\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Decision Tree Regressor\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dt_model \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m dt_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDecision Tree Regressor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, X_train, X_test, y_train, y_test, model_name)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m training_time \u001b[38;5;241m=\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1404\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Decision Tree Regressor\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dt_model \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m dt_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDecision Tree Regressor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, X_train, X_test, y_train, y_test, model_name)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m training_time \u001b[38;5;241m=\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1404\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\conda_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Decision Tree Regressor\n",
    "dt_model = DecisionTreeRegressor(max_depth=10, min_samples_split=20, random_state=42)\n",
    "dt_results = evaluate_model(dt_model, X_train, X_test, y_train, y_test, \"Decision Tree Regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b97d02",
   "metadata": {},
   "source": [
    "### 6. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=15, min_samples_split=10, \n",
    "                                  random_state=42, n_jobs=-1, verbose=2)\n",
    "rf_results = evaluate_model(rf_model, X_train, X_test, y_train, y_test, \"Random Forest Regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94909c6",
   "metadata": {},
   "source": [
    "### 7. Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Regressor\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, \n",
    "                                      random_state=42, verbose=1)\n",
    "gb_results = evaluate_model(gb_model, X_train, X_test, y_train, y_test, \"Gradient Boosting Regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60244b6",
   "metadata": {},
   "source": [
    "### 8. XGBoost Regressor (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cab08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Regressor\n",
    "if xgboost_available:\n",
    "    xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, \n",
    "                             random_state=42, n_jobs=-1, verbosity=2)\n",
    "    xgb_results = evaluate_model(xgb_model, X_train, X_test, y_train, y_test, \"XGBoost Regressor\")\n",
    "else:\n",
    "    print(\"\\nSkipping XGBoost (not installed)\")\n",
    "    xgb_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d9345",
   "metadata": {},
   "source": [
    "### 9. Support Vector Regressor (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Regressor (using subset for faster training)\n",
    "print(\"\\n‚ö†Ô∏è  Note: Training SVR on a subset (50,000 samples) due to computational cost...\")\n",
    "\n",
    "# Use subset for SVR\n",
    "subset_size = min(50000, len(X_train))\n",
    "X_train_subset = X_train.iloc[:subset_size]\n",
    "y_train_subset = y_train.iloc[:subset_size]\n",
    "\n",
    "svr_model = SVR(kernel='rbf', C=10, gamma='scale', verbose=True)\n",
    "svr_results = evaluate_model(svr_model, X_train_subset, X_test, y_train_subset, y_test, \n",
    "                             \"Support Vector Regressor (SVR)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7697fbef",
   "metadata": {},
   "source": [
    "### 10. K-Nearest Neighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors Regressor\n",
    "knn_model = KNeighborsRegressor(n_neighbors=5, n_jobs=-1)\n",
    "knn_results = evaluate_model(knn_model, X_train, X_test, y_train, y_test, \"K-Nearest Neighbors Regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d83e6",
   "metadata": {},
   "source": [
    "## Collect All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cdbd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = [\n",
    "    lr_results,\n",
    "    ridge_results,\n",
    "    lasso_results,\n",
    "    elasticnet_results,\n",
    "    dt_results,\n",
    "    rf_results,\n",
    "    gb_results,\n",
    "    knn_results,\n",
    "    svr_results\n",
    "]\n",
    "\n",
    "if xgb_results:\n",
    "    all_results.append(xgb_results)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame([{\n",
    "    'Model': r['model_name'],\n",
    "    'Train RMSE': r['train_rmse'],\n",
    "    'Test RMSE': r['test_rmse'],\n",
    "    'Train MAE': r['train_mae'],\n",
    "    'Test MAE': r['test_mae'],\n",
    "    'Train R¬≤': r['train_r2'],\n",
    "    'Test R¬≤': r['test_r2'],\n",
    "    'Training Time (s)': r['training_time']\n",
    "} for r in all_results])\n",
    "\n",
    "# Sort by Test R¬≤ (descending)\n",
    "results_df = results_df.sort_values('Test R¬≤', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_test_r2 = results_df.iloc[0]['Test R¬≤']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} (Test R¬≤ = {best_test_r2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde5d07",
   "metadata": {},
   "source": [
    "## Visualize Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8215b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Test R¬≤ Comparison\n",
    "axes[0, 0].barh(results_df['Model'], results_df['Test R¬≤'], color='skyblue', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Test R¬≤', fontsize=12)\n",
    "axes[0, 0].set_title('Model Comparison - Test R¬≤ Score', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "axes[0, 0].invert_yaxis()\n",
    "\n",
    "# 2. Test RMSE Comparison\n",
    "axes[0, 1].barh(results_df['Model'], results_df['Test RMSE'], color='coral', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Test RMSE', fontsize=12)\n",
    "axes[0, 1].set_title('Model Comparison - Test RMSE', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "axes[0, 1].invert_yaxis()\n",
    "\n",
    "# 3. Test MAE Comparison\n",
    "axes[1, 0].barh(results_df['Model'], results_df['Test MAE'], color='lightgreen', alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Test MAE', fontsize=12)\n",
    "axes[1, 0].set_title('Model Comparison - Test MAE', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='x')\n",
    "axes[1, 0].invert_yaxis()\n",
    "\n",
    "# 4. Training Time Comparison\n",
    "axes[1, 1].barh(results_df['Model'], results_df['Training Time (s)'], color='gold', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Training Time (seconds)', fontsize=12)\n",
    "axes[1, 1].set_title('Model Comparison - Training Time', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3, axis='x')\n",
    "axes[1, 1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Comparison chart saved as 'model_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a475f0b",
   "metadata": {},
   "source": [
    "## Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained models\n",
    "print(\"Saving trained models...\\n\")\n",
    "\n",
    "models_to_save = {\n",
    "    'linear_regression': lr_results,\n",
    "    'ridge_regression': ridge_results,\n",
    "    'lasso_regression': lasso_results,\n",
    "    'elasticnet_regression': elasticnet_results,\n",
    "    'decision_tree': dt_results,\n",
    "    'random_forest': rf_results,\n",
    "    'gradient_boosting': gb_results,\n",
    "    'knn': knn_results,\n",
    "    'svr': svr_results\n",
    "}\n",
    "\n",
    "if xgb_results:\n",
    "    models_to_save['xgboost'] = xgb_results\n",
    "\n",
    "for model_name, results in models_to_save.items():\n",
    "    with open(f'model_{model_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(results['model'], f)\n",
    "    print(f\"‚úì Saved: model_{model_name}.pkl\")\n",
    "\n",
    "# Save results DataFrame\n",
    "results_df.to_csv('model_results.csv', index=False)\n",
    "print(f\"\\n‚úì Saved: model_results.csv\")\n",
    "\n",
    "# Save all results including predictions\n",
    "with open('all_model_results.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "print(f\"‚úì Saved: all_model_results.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "print(f\"   ‚Ä¢ {len(models_to_save)} model files (*.pkl)\")\n",
    "print(f\"   ‚Ä¢ model_results.csv - Summary metrics\")\n",
    "print(f\"   ‚Ä¢ all_model_results.pkl - Complete results with predictions\")\n",
    "print(f\"   ‚Ä¢ model_comparison.png - Visualization\")\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   Test R¬≤: {best_test_r2:.4f}\")\n",
    "print(f\"\\nüí° Note: This model predicts song release year from audio features!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda_env)",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
